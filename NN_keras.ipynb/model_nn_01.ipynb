{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_selection, metrics, model_selection, decomposition\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import time\n",
    "import random\n",
    "from itertools import product\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = Path(r\"..\\\\..\\\\Data\\\\Input\")\n",
    "\n",
    "feature_directory = Path(r\"..\\\\..\\\\Data\\\\Feature\")\n",
    "\n",
    "model_name = \"model_nn_01\"\n",
    "model_directory = Path()/model_name\n",
    "model_directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dic, save_path):\n",
    "    with open(save_path, 'wb') as f:\n",
    "    # with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(dic, f)\n",
    "\n",
    "def load_pickle(load_path):\n",
    "    with open(load_path, 'rb') as f:\n",
    "    # with gzip.open(load_path, 'rb') as f:\n",
    "        message_dict = pickle.load(f)\n",
    "    return message_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 3141410 entries, (0, 1) to (1219, 3773)\n",
      "Columns: 302 entries, row_id to f_299\n",
      "dtypes: float32(301), object(1)\n",
      "memory usage: 3.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_parquet(input_directory/'train_low_mem.parquet', engine='pyarrow').set_index(['time_id','investment_id'])\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>-0.300875</td>\n",
       "      <td>0.932573</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>-0.402206</td>\n",
       "      <td>0.378386</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>-0.413469</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366028</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>0.200075</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.086764</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-1.044826</td>\n",
       "      <td>-0.287605</td>\n",
       "      <td>0.321566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.231040</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154193</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>0.615937</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>-0.607963</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>-1.083155</td>\n",
       "      <td>0.979656</td>\n",
       "      <td>-1.125681</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.551904</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.219097</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.612428</td>\n",
       "      <td>-0.113944</td>\n",
       "      <td>0.243608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.064780</td>\n",
       "      <td>-2.343535</td>\n",
       "      <td>-0.011870</td>\n",
       "      <td>1.874606</td>\n",
       "      <td>-0.606346</td>\n",
       "      <td>-0.586827</td>\n",
       "      <td>-0.815737</td>\n",
       "      <td>0.778096</td>\n",
       "      <td>0.298990</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382201</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.266359</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.609113</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>-0.783423</td>\n",
       "      <td>1.151730</td>\n",
       "      <td>-0.773309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.531940</td>\n",
       "      <td>0.842057</td>\n",
       "      <td>-0.262993</td>\n",
       "      <td>2.330030</td>\n",
       "      <td>-0.583422</td>\n",
       "      <td>-0.618392</td>\n",
       "      <td>-0.742814</td>\n",
       "      <td>-0.946789</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170365</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.741355</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.588445</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>0.753279</td>\n",
       "      <td>1.345611</td>\n",
       "      <td>-0.737624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         target       f_0       f_1       f_2       f_3  \\\n",
       "time_id investment_id                                                     \n",
       "0       1             -0.300875  0.932573  0.113691 -0.402206  0.378386   \n",
       "        2             -0.231040  0.810802 -0.514115  0.742368 -0.616673   \n",
       "        6              0.568807  0.393974  0.615937  0.567806 -0.607963   \n",
       "        7             -1.064780 -2.343535 -0.011870  1.874606 -0.606346   \n",
       "        8             -0.531940  0.842057 -0.262993  2.330030 -0.583422   \n",
       "\n",
       "                            f_4       f_5       f_6       f_7       f_8  ...  \\\n",
       "time_id investment_id                                                    ...   \n",
       "0       1             -0.203938 -0.413469  0.965623  1.230508  0.114809  ...   \n",
       "        2             -0.194255  1.771210  1.428127  1.134144  0.114809  ...   \n",
       "        6              0.068883 -1.083155  0.979656 -1.125681  0.114809  ...   \n",
       "        7             -0.586827 -0.815737  0.778096  0.298990  0.114809  ...   \n",
       "        8             -0.618392 -0.742814 -0.946789  1.230508  0.114809  ...   \n",
       "\n",
       "                          f_290     f_291     f_292     f_293     f_294  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.366028 -1.095620  0.200075  0.819155  0.941183   \n",
       "        2             -0.154193  0.912726 -0.734579  0.819155  0.941183   \n",
       "        6             -0.138020  0.912726 -0.551904 -1.220772 -1.060166   \n",
       "        7              0.382201  0.912726 -0.266359 -1.220772  0.941183   \n",
       "        8             -0.170365  0.912726 -0.741355 -1.220772  0.941183   \n",
       "\n",
       "                          f_295     f_296     f_297     f_298     f_299  \n",
       "time_id investment_id                                                    \n",
       "0       1             -0.086764 -1.087009 -1.044826 -0.287605  0.321566  \n",
       "        2             -0.387617 -1.087009 -0.929529 -0.974060 -0.343624  \n",
       "        6             -0.219097 -1.087009 -0.612428 -0.113944  0.243608  \n",
       "        7             -0.609113  0.104928 -0.783423  1.151730 -0.773309  \n",
       "        8             -0.588445  0.104928  0.753279  1.345611 -0.737624  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_data = df_data.drop('row_id', axis=1)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_norm = df_data.copy()\n",
    "\n",
    "# for i in tqdm(range(300)):\n",
    "#     feature = f'f_{i}'\n",
    "\n",
    "#     df_data_norm[feature] = df_data[feature].groupby(level='time_id').apply(\n",
    "#         lambda x: pd.DataFrame(preprocessing.RobustScaler(quantile_range=(1., 99.), with_scaling=True, with_centering=True).fit_transform(x.values.reshape(-1, 1)), index=x.index, columns=[f'f_{i}']))\n",
    "\n",
    "\n",
    "# df_data_norm.to_parquet(input_directory/'train_norm2.parquet')\n",
    "df_data_norm = pd.read_parquet(input_directory/'train_norm2.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'histgram of normalized feature'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(18, 4))\n",
    "# ax = plt.subplot(121)\n",
    "# df_data['f_1'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of raw feature\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "df_data_norm['f_293'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of normalized feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Feature & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_id  investment_id\n",
       "0        1               -0.300875\n",
       "         2               -0.231040\n",
       "         6                0.568807\n",
       "         7               -1.064780\n",
       "         8               -0.531940\n",
       "                            ...   \n",
       "1219     3768             0.033600\n",
       "         3769            -0.223264\n",
       "         3770            -0.559415\n",
       "         3772             0.009599\n",
       "         3773             1.212112\n",
       "Name: target, Length: 3141410, dtype: float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = df_data_norm.filter(like='f_')\n",
    "target = df_data_norm['target']\n",
    "# target = target.reset_index(drop=True)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.133483</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>-0.107040</td>\n",
       "      <td>0.232203</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>-0.084325</td>\n",
       "      <td>0.295687</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.565489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.097949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049540</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.239130</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>0.119334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108289</td>\n",
       "      <td>-0.102564</td>\n",
       "      <td>0.160293</td>\n",
       "      <td>-0.011403</td>\n",
       "      <td>0.029314</td>\n",
       "      <td>0.390865</td>\n",
       "      <td>0.426703</td>\n",
       "      <td>0.295676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.076149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.097370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.209301</td>\n",
       "      <td>-0.285416</td>\n",
       "      <td>-0.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.119521</td>\n",
       "      <td>-0.009270</td>\n",
       "      <td>0.091237</td>\n",
       "      <td>-0.229989</td>\n",
       "      <td>0.299662</td>\n",
       "      <td>-0.347619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.298825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.059196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.127263</td>\n",
       "      <td>-0.063854</td>\n",
       "      <td>0.104602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.544325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424744</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.063067</td>\n",
       "      <td>-0.171823</td>\n",
       "      <td>0.242565</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.337304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.080431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.171502</td>\n",
       "      <td>0.262178</td>\n",
       "      <td>-0.087566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.114755</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>0.531115</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.070495</td>\n",
       "      <td>-0.155961</td>\n",
       "      <td>-0.246051</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.098786</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.075289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226062</td>\n",
       "      <td>0.312121</td>\n",
       "      <td>-0.080823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1219</th>\n",
       "      <th>3768</th>\n",
       "      <td>-0.047359</td>\n",
       "      <td>-0.148677</td>\n",
       "      <td>-0.083344</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.017038</td>\n",
       "      <td>-0.092924</td>\n",
       "      <td>0.378928</td>\n",
       "      <td>0.318737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.387626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080894</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.105754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063816</td>\n",
       "      <td>-0.096651</td>\n",
       "      <td>-0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>-0.374504</td>\n",
       "      <td>-0.029735</td>\n",
       "      <td>-0.035861</td>\n",
       "      <td>-0.006363</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.029534</td>\n",
       "      <td>-0.402315</td>\n",
       "      <td>0.339488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.329040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.107742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.107359</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.039689</td>\n",
       "      <td>-0.294157</td>\n",
       "      <td>-0.070620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>0.154132</td>\n",
       "      <td>-0.237883</td>\n",
       "      <td>0.186720</td>\n",
       "      <td>-0.009407</td>\n",
       "      <td>-0.012827</td>\n",
       "      <td>0.291013</td>\n",
       "      <td>0.030878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.191191</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015572</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.022208</td>\n",
       "      <td>-0.489768</td>\n",
       "      <td>-0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>-0.652056</td>\n",
       "      <td>0.089206</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.435851</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>-0.146850</td>\n",
       "      <td>0.091365</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.302704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.053337</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018566</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.147317</td>\n",
       "      <td>0.130867</td>\n",
       "      <td>-0.020435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>-0.088998</td>\n",
       "      <td>0.059471</td>\n",
       "      <td>-0.123831</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.122948</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>0.257889</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.524376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032652</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.069543</td>\n",
       "      <td>0.148146</td>\n",
       "      <td>0.170569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141410 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            f_0       f_1       f_2       f_3       f_4  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.133483  0.025641 -0.107040  0.232203  0.027035   \n",
       "        2              0.108289 -0.102564  0.160293 -0.011403  0.029314   \n",
       "        6              0.022050  0.128205  0.119521 -0.009270  0.091237   \n",
       "        7             -0.544325  0.000000  0.424744 -0.008875 -0.063067   \n",
       "        8              0.114755 -0.051282  0.531115 -0.003263 -0.070495   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "1219    3768          -0.047359 -0.148677 -0.083344 -0.002532 -0.017038   \n",
       "        3769          -0.374504 -0.029735 -0.035861 -0.006363 -0.048209   \n",
       "        3770           0.154132 -0.237883  0.186720 -0.009407 -0.012827   \n",
       "        3772          -0.652056  0.089206  0.000941  0.435851 -0.010454   \n",
       "        3773          -0.088998  0.059471 -0.123831  0.139659  0.122948   \n",
       "\n",
       "                            f_5       f_6       f_7  f_8       f_9  ...  \\\n",
       "time_id investment_id                                               ...   \n",
       "0       1             -0.084325  0.295687  0.323107  0.0 -0.565489  ...   \n",
       "        2              0.390865  0.426703  0.295676  0.0 -0.076149  ...   \n",
       "        6             -0.229989  0.299662 -0.347619  0.0 -0.298825  ...   \n",
       "        7             -0.171823  0.242565  0.057936  0.0 -0.337304  ...   \n",
       "        8             -0.155961 -0.246051  0.323107  0.0 -0.017943  ...   \n",
       "...                         ...       ...       ...  ...       ...  ...   \n",
       "1219    3768          -0.092924  0.378928  0.318737  0.0 -0.387626  ...   \n",
       "        3769          -0.029534 -0.402315  0.339488  0.0 -0.329040  ...   \n",
       "        3770           0.291013  0.030878  0.000000  0.0  0.253357  ...   \n",
       "        3772          -0.146850  0.091365 -0.286822  0.0 -0.302704  ...   \n",
       "        3773           0.096628  0.257889 -0.286822  0.0 -0.524376  ...   \n",
       "\n",
       "                          f_290  f_291     f_292  f_293  f_294     f_295  \\\n",
       "time_id investment_id                                                      \n",
       "0       1              0.102298   -1.0  0.097949    0.0    0.0  0.049540   \n",
       "        2             -0.003282    0.0 -0.097370    0.0    0.0 -0.025318   \n",
       "        6              0.000000    0.0 -0.059196   -1.0   -1.0  0.016613   \n",
       "        7              0.105580    0.0  0.000476   -1.0    0.0 -0.080431   \n",
       "        8             -0.006564    0.0 -0.098786   -1.0    0.0 -0.075289   \n",
       "...                         ...    ...       ...    ...    ...       ...   \n",
       "1219    3768          -0.080894   -1.0 -0.105754    0.0    0.0 -0.031788   \n",
       "        3769           0.008933   -1.0 -0.107742    0.0    0.0 -0.107359   \n",
       "        3770          -0.170720   -1.0  0.191191   -1.0    0.0 -0.015572   \n",
       "        3772          -0.170720   -1.0  0.053337   -1.0    0.0 -0.018566   \n",
       "        3773          -0.086849    0.0  0.682466    0.0    0.0  0.032652   \n",
       "\n",
       "                       f_296     f_297     f_298     f_299  \n",
       "time_id investment_id                                       \n",
       "0       1               -0.5 -0.239130 -0.108588  0.119334  \n",
       "        2               -0.5 -0.209301 -0.285416 -0.006368  \n",
       "        6               -0.5 -0.127263 -0.063854  0.104602  \n",
       "        7                0.0 -0.171502  0.262178 -0.087566  \n",
       "        8                0.0  0.226062  0.312121 -0.080823  \n",
       "...                      ...       ...       ...       ...  \n",
       "1219    3768             0.0 -0.063816 -0.096651 -0.068704  \n",
       "        3769            -0.5  0.039689 -0.294157 -0.070620  \n",
       "        3770             0.5  0.022208 -0.489768 -0.003278  \n",
       "        3772            -0.5 -0.147317  0.130867 -0.020435  \n",
       "        3773             0.5 -0.069543  0.148146  0.170569  \n",
       "\n",
       "[3141410 rows x 300 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature = feature.reset_index(drop=True)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 0, train_end: 1018, test_start: 1019, test_end: 1219\n"
     ]
    }
   ],
   "source": [
    "train_test_ratio = 5\n",
    "\n",
    "uniquedate = target.index.get_level_values(level='time_id').unique().tolist()\n",
    "train_start = 0\n",
    "train_end = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))]\n",
    "test_start = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))+1]\n",
    "test_end = 1219\n",
    "\n",
    "\n",
    "# dates of train, dates of test\n",
    "print(\n",
    "    f'train_start: {train_start}, train_end: {train_end}, test_start: {test_start}, test_end: {test_end}')\n",
    "\n",
    "feature_train = feature.loc[idx[train_start:train_end, :], :]\n",
    "feature_test = feature.loc[idx[test_start:test_end, :], :]\n",
    "\n",
    "target_train = target.loc[idx[train_start:train_end, :]]\n",
    "target_test = target.loc[idx[test_start:test_end, :]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UbiquantDataset(Dataset):\n",
    "    def __init__(self, feature, target):\n",
    "        self.feature = torch.tensor(feature.values)\n",
    "        self.target = torch.tensor(target.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # feature = torch.tensor(self.feature.iloc[idx, :])\n",
    "        # target = torch.tensor(self.target.iloc[idx])\n",
    "        return self.feature[idx, :], self.target[idx]\n",
    "\n",
    "def get_train_valid_data(X, y, train_idx, valid_idx):\n",
    "    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    x_val, y_val = X.iloc[valid_idx, :], y.iloc[valid_idx]\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_val = scaler.transform(x_val)xx\n",
    "    return (x_train, y_train,\n",
    "            x_val, y_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (cls): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.n_in = param['n_in']\n",
    "        self.cls = nn.Sequential(\n",
    "            #layer1\n",
    "            nn.Linear(param['n_in'], param['n_hidden1']),\n",
    "            param['act_f'](),\n",
    "            nn.BatchNorm1d(num_features=param['n_hidden1']),\n",
    "            #layer2\n",
    "            nn.Linear(param['n_hidden1'], param['n_hidden2']),\n",
    "            param['act_f'](),\n",
    "            nn.BatchNorm1d(num_features=param['n_hidden2']),\n",
    "            #layer3\n",
    "            nn.Linear(param['n_hidden2'], param['n_hidden3']),\n",
    "            param['act_f'](),\n",
    "            nn.BatchNorm1d(num_features=param['n_hidden3']),\n",
    "\n",
    "            nn.Linear(param['n_hidden3'], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.n_in)\n",
    "        x = self.cls(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork({'n_in': 300, 'n_hidden1': 256, 'n_hidden2': 128,\n",
    "                       'n_hidden3': 64, 'act_f': nn.ReLU})\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_corr(y_true, y_pred):\n",
    "    return np.corrcoef(y_true.numpy().reshape(1, -1), y_pred.numpy().reshape(1, -1))[0][1]\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, params):\n",
    "\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss, test_corr = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(test_dataloader):\n",
    "            pred = model(X)\n",
    "            loss = params['criterion'](pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            corr = params['custom_corr'](pred, y)\n",
    "            test_corr += corr\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_corr /= num_batches\n",
    "    return test_loss, test_corr\n",
    "\n",
    "\n",
    "def train(NN_model, train_dataloader, val_dataloader, params):\n",
    "    # model, optimizer\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = NN_model(params).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=params['T_max'], eta_min=4e-08)\n",
    "    # train\n",
    "    for epoch in range(1, params['epoch']+1):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        train_corr = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = params['criterion'](output, target)\n",
    "            loss.backward()\n",
    "            if params['clip']:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), params['clip'])\n",
    "            train_loss += loss.item()\n",
    "            train_corr += params['custom_corr'](model(data).detach(), target.detach())\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if batch_idx % params['log_interval'] == 0:\n",
    "                print('Train : [{}/{} ({:.0f}%)]\\tLoss: {:.5f}'.format(\n",
    "                    batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                    100. * batch_idx / len(train_dataloader), loss.item()))\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_corr /= len(train_dataloader)\n",
    "        val_loss, val_corr = test(model, val_dataloader, params)\n",
    "        print(\"| Epoch {} | running {:.2f} seconds | train loss {:.5f} | train corr {:.5f} | val loss {:.5f} | val corr {:.5f} |\".format(\n",
    "            epoch, time.time()-start_time, train_loss, train_corr, val_loss, val_corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lr\": 1e-3, 'batch_size': 256, 'n_splits': 2,\n",
    "          'n_in': 300,\n",
    "          'n_hidden1': 525, 'n_hidden2': 210, 'n_hidden3': 128, 'n_hidden4': 64,\n",
    "          'epoch': 2, 'clip': None, 'log_interval': 500,\n",
    "          'T_max': 10,\n",
    "          'act_f': nn.SiLU, 'criterion': nn.HuberLoss(),\n",
    "          'custom_corr': custom_corr\n",
    "          }\n",
    "\n",
    "\n",
    "cv = model_selection.KFold(\n",
    "    n_splits=params['n_splits'], random_state=2021, shuffle=True)\n",
    "\n",
    "\n",
    "# loss_fn = nn.HuberLoss() nn.MSELoss(reduction='mean')\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "fold: 1\n",
      "Train : [0/1245446 (0%)]\tLoss: 0.42351\n",
      "Train : [128000/1245446 (10%)]\tLoss: 0.33832\n",
      "Train : [256000/1245446 (21%)]\tLoss: 0.34287\n",
      "Train : [384000/1245446 (31%)]\tLoss: 0.27049\n",
      "Train : [512000/1245446 (41%)]\tLoss: 0.31819\n",
      "Train : [640000/1245446 (51%)]\tLoss: 0.26539\n",
      "Train : [768000/1245446 (62%)]\tLoss: 0.30403\n",
      "Train : [896000/1245446 (72%)]\tLoss: 0.31218\n",
      "Train : [1024000/1245446 (82%)]\tLoss: 0.32030\n",
      "Train : [1152000/1245446 (92%)]\tLoss: 0.30423\n",
      "| Epoch 1 | running 78.11 seconds | train loss 0.31696 | train corr -0.00041 | val loss 0.33370 | val corr 0.00872 |\n",
      "Train : [0/1245446 (0%)]\tLoss: 0.31626\n",
      "Train : [128000/1245446 (10%)]\tLoss: 0.30436\n",
      "Train : [256000/1245446 (21%)]\tLoss: 0.31671\n",
      "Train : [384000/1245446 (31%)]\tLoss: 0.34581\n",
      "Train : [512000/1245446 (41%)]\tLoss: 0.30083\n",
      "Train : [640000/1245446 (51%)]\tLoss: 0.39847\n",
      "Train : [768000/1245446 (62%)]\tLoss: 0.29428\n",
      "Train : [896000/1245446 (72%)]\tLoss: 0.33990\n",
      "Train : [1024000/1245446 (82%)]\tLoss: 0.38961\n",
      "Train : [1152000/1245446 (92%)]\tLoss: 0.32437\n",
      "| Epoch 2 | running 75.11 seconds | train loss 0.31606 | train corr 0.00026 | val loss 0.34456 | val corr -0.02157 |\n",
      "##############################################################\n",
      "fold: 2\n",
      "Train : [0/1245447 (0%)]\tLoss: 0.40905\n",
      "Train : [128000/1245447 (10%)]\tLoss: 0.23899\n",
      "Train : [256000/1245447 (21%)]\tLoss: 0.30245\n",
      "Train : [384000/1245447 (31%)]\tLoss: 0.28296\n",
      "Train : [512000/1245447 (41%)]\tLoss: 0.31833\n",
      "Train : [640000/1245447 (51%)]\tLoss: 0.32686\n",
      "Train : [768000/1245447 (62%)]\tLoss: 0.26005\n",
      "Train : [896000/1245447 (72%)]\tLoss: 0.27818\n",
      "Train : [1024000/1245447 (82%)]\tLoss: 0.30925\n",
      "Train : [1152000/1245447 (92%)]\tLoss: 0.30705\n",
      "| Epoch 1 | running 76.68 seconds | train loss 0.31714 | train corr 0.00138 | val loss 0.34376 | val corr 0.01554 |\n",
      "Train : [0/1245447 (0%)]\tLoss: 0.35410\n",
      "Train : [128000/1245447 (10%)]\tLoss: 0.28611\n",
      "Train : [256000/1245447 (21%)]\tLoss: 0.31768\n",
      "Train : [384000/1245447 (31%)]\tLoss: 0.35766\n",
      "Train : [512000/1245447 (41%)]\tLoss: 0.28139\n",
      "Train : [640000/1245447 (51%)]\tLoss: 0.29896\n",
      "Train : [768000/1245447 (62%)]\tLoss: 0.30840\n",
      "Train : [896000/1245447 (72%)]\tLoss: 0.25762\n",
      "Train : [1024000/1245447 (82%)]\tLoss: 0.29252\n",
      "Train : [1152000/1245447 (92%)]\tLoss: 0.37597\n",
      "| Epoch 2 | running 78.57 seconds | train loss 0.31617 | train corr 0.00112 | val loss 0.32183 | val corr -0.02222 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(cv.split(X=feature_train)):\n",
    "    print(\"##############################################################\")\n",
    "    print(f\"fold: {i+1}\")\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = get_train_valid_data(\n",
    "        feature_train, target_train, train_idx, valid_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(UbiquantDataset(\n",
    "        X_train, y_train), batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(UbiquantDataset(\n",
    "        X_valid, y_valid), batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    train(NeuralNetwork, train_dataloader, valid_dataloader, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06e2c26d81da9559881f4d926ec79778e2b8c97ac068d329245981963e3d233b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
