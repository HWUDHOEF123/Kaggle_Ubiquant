{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_selection, metrics, model_selection, decomposition\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random\n",
    "from itertools import product\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = Path(r\"..\\\\..\\\\Data\\\\Input\")\n",
    "\n",
    "feature_directory = Path(r\"..\\\\..\\\\Data\\\\Feature\")\n",
    "\n",
    "model_name = \"model_nn_01\"\n",
    "model_directory = Path()/model_name\n",
    "model_directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dic, save_path):\n",
    "    with open(save_path, 'wb') as f:\n",
    "    # with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(dic, f)\n",
    "\n",
    "def load_pickle(load_path):\n",
    "    with open(load_path, 'rb') as f:\n",
    "    # with gzip.open(load_path, 'rb') as f:\n",
    "        message_dict = pickle.load(f)\n",
    "    return message_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 3141410 entries, (0, 1) to (1219, 3773)\n",
      "Columns: 302 entries, row_id to f_299\n",
      "dtypes: float32(301), object(1)\n",
      "memory usage: 3.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_parquet(input_directory/'train_low_mem.parquet', engine='pyarrow').set_index(['time_id','investment_id'])\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>-0.300875</td>\n",
       "      <td>0.932573</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>-0.402206</td>\n",
       "      <td>0.378386</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>-0.413469</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366028</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>0.200075</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.086764</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-1.044826</td>\n",
       "      <td>-0.287605</td>\n",
       "      <td>0.321566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.231040</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154193</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>0.615937</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>-0.607963</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>-1.083155</td>\n",
       "      <td>0.979656</td>\n",
       "      <td>-1.125681</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.551904</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.219097</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.612428</td>\n",
       "      <td>-0.113944</td>\n",
       "      <td>0.243608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.064780</td>\n",
       "      <td>-2.343535</td>\n",
       "      <td>-0.011870</td>\n",
       "      <td>1.874606</td>\n",
       "      <td>-0.606346</td>\n",
       "      <td>-0.586827</td>\n",
       "      <td>-0.815737</td>\n",
       "      <td>0.778096</td>\n",
       "      <td>0.298990</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382201</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.266359</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.609113</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>-0.783423</td>\n",
       "      <td>1.151730</td>\n",
       "      <td>-0.773309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.531940</td>\n",
       "      <td>0.842057</td>\n",
       "      <td>-0.262993</td>\n",
       "      <td>2.330030</td>\n",
       "      <td>-0.583422</td>\n",
       "      <td>-0.618392</td>\n",
       "      <td>-0.742814</td>\n",
       "      <td>-0.946789</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170365</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.741355</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.588445</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>0.753279</td>\n",
       "      <td>1.345611</td>\n",
       "      <td>-0.737624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         target       f_0       f_1       f_2       f_3  \\\n",
       "time_id investment_id                                                     \n",
       "0       1             -0.300875  0.932573  0.113691 -0.402206  0.378386   \n",
       "        2             -0.231040  0.810802 -0.514115  0.742368 -0.616673   \n",
       "        6              0.568807  0.393974  0.615937  0.567806 -0.607963   \n",
       "        7             -1.064780 -2.343535 -0.011870  1.874606 -0.606346   \n",
       "        8             -0.531940  0.842057 -0.262993  2.330030 -0.583422   \n",
       "\n",
       "                            f_4       f_5       f_6       f_7       f_8  ...  \\\n",
       "time_id investment_id                                                    ...   \n",
       "0       1             -0.203938 -0.413469  0.965623  1.230508  0.114809  ...   \n",
       "        2             -0.194255  1.771210  1.428127  1.134144  0.114809  ...   \n",
       "        6              0.068883 -1.083155  0.979656 -1.125681  0.114809  ...   \n",
       "        7             -0.586827 -0.815737  0.778096  0.298990  0.114809  ...   \n",
       "        8             -0.618392 -0.742814 -0.946789  1.230508  0.114809  ...   \n",
       "\n",
       "                          f_290     f_291     f_292     f_293     f_294  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.366028 -1.095620  0.200075  0.819155  0.941183   \n",
       "        2             -0.154193  0.912726 -0.734579  0.819155  0.941183   \n",
       "        6             -0.138020  0.912726 -0.551904 -1.220772 -1.060166   \n",
       "        7              0.382201  0.912726 -0.266359 -1.220772  0.941183   \n",
       "        8             -0.170365  0.912726 -0.741355 -1.220772  0.941183   \n",
       "\n",
       "                          f_295     f_296     f_297     f_298     f_299  \n",
       "time_id investment_id                                                    \n",
       "0       1             -0.086764 -1.087009 -1.044826 -0.287605  0.321566  \n",
       "        2             -0.387617 -1.087009 -0.929529 -0.974060 -0.343624  \n",
       "        6             -0.219097 -1.087009 -0.612428 -0.113944  0.243608  \n",
       "        7             -0.609113  0.104928 -0.783423  1.151730 -0.773309  \n",
       "        8             -0.588445  0.104928  0.753279  1.345611 -0.737624  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_data = df_data.drop('row_id', axis=1)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_norm = df_data.copy()\n",
    "\n",
    "# for i in tqdm(range(300)):\n",
    "#     feature = f'f_{i}'\n",
    "\n",
    "#     df_data_norm[feature] = df_data[feature].groupby(level='time_id').apply(\n",
    "#         lambda x: pd.DataFrame(preprocessing.RobustScaler(quantile_range=(1., 99.), with_scaling=True, with_centering=True).fit_transform(x.values.reshape(-1, 1)), index=x.index, columns=[f'f_{i}']))\n",
    "\n",
    "\n",
    "# df_data_norm.to_parquet(input_directory/'train_norm2.parquet')\n",
    "df_data_norm = pd.read_parquet(input_directory/'train_norm2.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'histgram of normalized feature'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(18, 4))\n",
    "# ax = plt.subplot(121)\n",
    "# df_data['f_1'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of raw feature\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "df_data_norm['f_1'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of normalized feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Feature & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_id  investment_id\n",
       "0        1               -0.300875\n",
       "         2               -0.231040\n",
       "         6                0.568807\n",
       "         7               -1.064780\n",
       "         8               -0.531940\n",
       "                            ...   \n",
       "1219     3768             0.033600\n",
       "         3769            -0.223264\n",
       "         3770            -0.559415\n",
       "         3772             0.009599\n",
       "         3773             1.212112\n",
       "Name: target, Length: 3141410, dtype: float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = df_data_norm.filter(like='f_')\n",
    "target = df_data_norm['target']\n",
    "# target = target.reset_index(drop=True)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.133483</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>-0.107040</td>\n",
       "      <td>0.232203</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>-0.084325</td>\n",
       "      <td>0.295687</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.565489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.097949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049540</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.239130</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>0.119334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108289</td>\n",
       "      <td>-0.102564</td>\n",
       "      <td>0.160293</td>\n",
       "      <td>-0.011403</td>\n",
       "      <td>0.029314</td>\n",
       "      <td>0.390865</td>\n",
       "      <td>0.426703</td>\n",
       "      <td>0.295676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.076149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.097370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.209301</td>\n",
       "      <td>-0.285416</td>\n",
       "      <td>-0.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.119521</td>\n",
       "      <td>-0.009270</td>\n",
       "      <td>0.091237</td>\n",
       "      <td>-0.229989</td>\n",
       "      <td>0.299662</td>\n",
       "      <td>-0.347619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.298825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.059196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.127263</td>\n",
       "      <td>-0.063854</td>\n",
       "      <td>0.104602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.544325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424744</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.063067</td>\n",
       "      <td>-0.171823</td>\n",
       "      <td>0.242565</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.337304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.080431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.171502</td>\n",
       "      <td>0.262178</td>\n",
       "      <td>-0.087566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.114755</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>0.531115</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.070495</td>\n",
       "      <td>-0.155961</td>\n",
       "      <td>-0.246051</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.098786</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.075289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226062</td>\n",
       "      <td>0.312121</td>\n",
       "      <td>-0.080823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1219</th>\n",
       "      <th>3768</th>\n",
       "      <td>-0.047359</td>\n",
       "      <td>-0.148677</td>\n",
       "      <td>-0.083344</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.017038</td>\n",
       "      <td>-0.092924</td>\n",
       "      <td>0.378928</td>\n",
       "      <td>0.318737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.387626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080894</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.105754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063816</td>\n",
       "      <td>-0.096651</td>\n",
       "      <td>-0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>-0.374504</td>\n",
       "      <td>-0.029735</td>\n",
       "      <td>-0.035861</td>\n",
       "      <td>-0.006363</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.029534</td>\n",
       "      <td>-0.402315</td>\n",
       "      <td>0.339488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.329040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.107742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.107359</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.039689</td>\n",
       "      <td>-0.294157</td>\n",
       "      <td>-0.070620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>0.154132</td>\n",
       "      <td>-0.237883</td>\n",
       "      <td>0.186720</td>\n",
       "      <td>-0.009407</td>\n",
       "      <td>-0.012827</td>\n",
       "      <td>0.291013</td>\n",
       "      <td>0.030878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.191191</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015572</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.022208</td>\n",
       "      <td>-0.489768</td>\n",
       "      <td>-0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>-0.652056</td>\n",
       "      <td>0.089206</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.435851</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>-0.146850</td>\n",
       "      <td>0.091365</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.302704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.053337</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018566</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.147317</td>\n",
       "      <td>0.130867</td>\n",
       "      <td>-0.020435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>-0.088998</td>\n",
       "      <td>0.059471</td>\n",
       "      <td>-0.123831</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.122948</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>0.257889</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.524376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032652</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.069543</td>\n",
       "      <td>0.148146</td>\n",
       "      <td>0.170569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141410 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            f_0       f_1       f_2       f_3       f_4  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.133483  0.025641 -0.107040  0.232203  0.027035   \n",
       "        2              0.108289 -0.102564  0.160293 -0.011403  0.029314   \n",
       "        6              0.022050  0.128205  0.119521 -0.009270  0.091237   \n",
       "        7             -0.544325  0.000000  0.424744 -0.008875 -0.063067   \n",
       "        8              0.114755 -0.051282  0.531115 -0.003263 -0.070495   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "1219    3768          -0.047359 -0.148677 -0.083344 -0.002532 -0.017038   \n",
       "        3769          -0.374504 -0.029735 -0.035861 -0.006363 -0.048209   \n",
       "        3770           0.154132 -0.237883  0.186720 -0.009407 -0.012827   \n",
       "        3772          -0.652056  0.089206  0.000941  0.435851 -0.010454   \n",
       "        3773          -0.088998  0.059471 -0.123831  0.139659  0.122948   \n",
       "\n",
       "                            f_5       f_6       f_7  f_8       f_9  ...  \\\n",
       "time_id investment_id                                               ...   \n",
       "0       1             -0.084325  0.295687  0.323107  0.0 -0.565489  ...   \n",
       "        2              0.390865  0.426703  0.295676  0.0 -0.076149  ...   \n",
       "        6             -0.229989  0.299662 -0.347619  0.0 -0.298825  ...   \n",
       "        7             -0.171823  0.242565  0.057936  0.0 -0.337304  ...   \n",
       "        8             -0.155961 -0.246051  0.323107  0.0 -0.017943  ...   \n",
       "...                         ...       ...       ...  ...       ...  ...   \n",
       "1219    3768          -0.092924  0.378928  0.318737  0.0 -0.387626  ...   \n",
       "        3769          -0.029534 -0.402315  0.339488  0.0 -0.329040  ...   \n",
       "        3770           0.291013  0.030878  0.000000  0.0  0.253357  ...   \n",
       "        3772          -0.146850  0.091365 -0.286822  0.0 -0.302704  ...   \n",
       "        3773           0.096628  0.257889 -0.286822  0.0 -0.524376  ...   \n",
       "\n",
       "                          f_290  f_291     f_292  f_293  f_294     f_295  \\\n",
       "time_id investment_id                                                      \n",
       "0       1              0.102298   -1.0  0.097949    0.0    0.0  0.049540   \n",
       "        2             -0.003282    0.0 -0.097370    0.0    0.0 -0.025318   \n",
       "        6              0.000000    0.0 -0.059196   -1.0   -1.0  0.016613   \n",
       "        7              0.105580    0.0  0.000476   -1.0    0.0 -0.080431   \n",
       "        8             -0.006564    0.0 -0.098786   -1.0    0.0 -0.075289   \n",
       "...                         ...    ...       ...    ...    ...       ...   \n",
       "1219    3768          -0.080894   -1.0 -0.105754    0.0    0.0 -0.031788   \n",
       "        3769           0.008933   -1.0 -0.107742    0.0    0.0 -0.107359   \n",
       "        3770          -0.170720   -1.0  0.191191   -1.0    0.0 -0.015572   \n",
       "        3772          -0.170720   -1.0  0.053337   -1.0    0.0 -0.018566   \n",
       "        3773          -0.086849    0.0  0.682466    0.0    0.0  0.032652   \n",
       "\n",
       "                       f_296     f_297     f_298     f_299  \n",
       "time_id investment_id                                       \n",
       "0       1               -0.5 -0.239130 -0.108588  0.119334  \n",
       "        2               -0.5 -0.209301 -0.285416 -0.006368  \n",
       "        6               -0.5 -0.127263 -0.063854  0.104602  \n",
       "        7                0.0 -0.171502  0.262178 -0.087566  \n",
       "        8                0.0  0.226062  0.312121 -0.080823  \n",
       "...                      ...       ...       ...       ...  \n",
       "1219    3768             0.0 -0.063816 -0.096651 -0.068704  \n",
       "        3769            -0.5  0.039689 -0.294157 -0.070620  \n",
       "        3770             0.5  0.022208 -0.489768 -0.003278  \n",
       "        3772            -0.5 -0.147317  0.130867 -0.020435  \n",
       "        3773             0.5 -0.069543  0.148146  0.170569  \n",
       "\n",
       "[3141410 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature = feature.reset_index(drop=True)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 0, train_end: 1018, test_start: 1019, test_end: 1219\n"
     ]
    }
   ],
   "source": [
    "train_test_ratio = 5\n",
    "\n",
    "uniquedate = target.index.get_level_values(level='time_id').unique().tolist()\n",
    "train_start = 0\n",
    "train_end = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))]\n",
    "test_start = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))+1]\n",
    "test_end = 1219\n",
    "\n",
    "\n",
    "# dates of train, dates of test\n",
    "print(\n",
    "    f'train_start: {train_start}, train_end: {train_end}, test_start: {test_start}, test_end: {test_end}')\n",
    "\n",
    "feature_train = feature.loc[idx[train_start:train_end, :], :]\n",
    "feature_test = feature.loc[idx[test_start:test_end, :], :]\n",
    "\n",
    "target_train = target.loc[idx[train_start:train_end, :]]\n",
    "target_test = target.loc[idx[test_start:test_end, :]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_data(X, y, train_idx, valid_idx):\n",
    "    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    x_val, y_val = X.iloc[valid_idx, :], y.iloc[valid_idx]\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_val = scaler.transform(x_val)\n",
    "    return (x_train, y_train,\n",
    "            x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.HuberLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 2\n",
    "batch_size = 64\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UbiquantDataset(Dataset):\n",
    "    def __init__(self, feature, target):\n",
    "        self.feature = torch.tensor(feature.values)\n",
    "        self.target = torch.tensor(target.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # feature = torch.tensor(self.feature.iloc[idx, :])\n",
    "        # target = torch.tensor(self.target.iloc[idx])\n",
    "        return self.feature[idx, :], self.target[idx]\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "# dataloader = DataLoader(UbiquantDataset(feature_train, target_train), batch_size=64, shuffle=True)\n",
    "# dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "cv: 1\n",
      "-------------------------------------------------\n",
      "Epoch: 1\n",
      "loss: 0.418208  [    0/1245446]\n",
      "loss: 0.262168  [ 6400/1245446]\n",
      "loss: 0.237383  [12800/1245446]\n",
      "loss: 0.243403  [19200/1245446]\n",
      "loss: 3.286797  [25600/1245446]\n",
      "loss: 2.648721  [32000/1245446]\n",
      "loss: 0.275080  [38400/1245446]\n",
      "loss: 0.420341  [44800/1245446]\n",
      "loss: 0.334350  [51200/1245446]\n",
      "loss: 8.527649  [57600/1245446]\n",
      "loss: 1883.900269  [64000/1245446]\n",
      "loss: 1.000248  [70400/1245446]\n",
      "loss: 0.511916  [76800/1245446]\n",
      "loss: 0.327506  [83200/1245446]\n",
      "loss: 0.352686  [89600/1245446]\n",
      "loss: 0.295097  [96000/1245446]\n",
      "loss: 0.412521  [102400/1245446]\n",
      "loss: 0.409077  [108800/1245446]\n",
      "loss: 0.213738  [115200/1245446]\n",
      "loss: 0.377543  [121600/1245446]\n",
      "loss: 0.479487  [128000/1245446]\n",
      "loss: 0.381072  [134400/1245446]\n",
      "loss: 0.241106  [140800/1245446]\n",
      "loss: 0.249520  [147200/1245446]\n",
      "loss: 0.378443  [153600/1245446]\n",
      "loss: 0.302384  [160000/1245446]\n",
      "loss: 0.274072  [166400/1245446]\n",
      "loss: 0.303843  [172800/1245446]\n",
      "loss: 0.322530  [179200/1245446]\n",
      "loss: 0.299693  [185600/1245446]\n",
      "loss: 0.295373  [192000/1245446]\n",
      "loss: 0.231267  [198400/1245446]\n",
      "loss: 0.292846  [204800/1245446]\n",
      "loss: 0.373203  [211200/1245446]\n",
      "loss: 0.238671  [217600/1245446]\n",
      "loss: 0.444558  [224000/1245446]\n",
      "loss: 0.257177  [230400/1245446]\n",
      "loss: 0.339869  [236800/1245446]\n",
      "loss: 0.277428  [243200/1245446]\n",
      "loss: 0.236228  [249600/1245446]\n",
      "loss: 0.279753  [256000/1245446]\n",
      "loss: 0.267599  [262400/1245446]\n",
      "loss: 0.319572  [268800/1245446]\n",
      "loss: 0.194655  [275200/1245446]\n",
      "loss: 0.287951  [281600/1245446]\n",
      "loss: 0.377998  [288000/1245446]\n",
      "loss: 0.358153  [294400/1245446]\n",
      "loss: 0.360732  [300800/1245446]\n",
      "loss: 0.211596  [307200/1245446]\n",
      "loss: 0.276718  [313600/1245446]\n",
      "loss: 0.257403  [320000/1245446]\n",
      "loss: 0.330184  [326400/1245446]\n",
      "loss: 0.467380  [332800/1245446]\n",
      "loss: 0.351892  [339200/1245446]\n",
      "loss: 0.473606  [345600/1245446]\n",
      "loss: 0.337098  [352000/1245446]\n",
      "loss: 0.289841  [358400/1245446]\n",
      "loss: 0.247625  [364800/1245446]\n",
      "loss: 0.453947  [371200/1245446]\n",
      "loss: 0.401194  [377600/1245446]\n",
      "loss: 0.280752  [384000/1245446]\n",
      "loss: 0.248156  [390400/1245446]\n",
      "loss: 0.236668  [396800/1245446]\n",
      "loss: 0.298016  [403200/1245446]\n",
      "loss: 0.351238  [409600/1245446]\n",
      "loss: 0.303354  [416000/1245446]\n",
      "loss: 0.362746  [422400/1245446]\n",
      "loss: 0.350360  [428800/1245446]\n",
      "loss: 0.267382  [435200/1245446]\n",
      "loss: 0.341038  [441600/1245446]\n",
      "loss: 0.335340  [448000/1245446]\n",
      "loss: 0.330104  [454400/1245446]\n",
      "loss: 0.308655  [460800/1245446]\n",
      "loss: 0.278965  [467200/1245446]\n",
      "loss: 0.275441  [473600/1245446]\n",
      "loss: 0.242455  [480000/1245446]\n",
      "loss: 0.205827  [486400/1245446]\n",
      "loss: 0.235659  [492800/1245446]\n",
      "loss: 0.199138  [499200/1245446]\n",
      "loss: 0.278297  [505600/1245446]\n",
      "loss: 0.335982  [512000/1245446]\n",
      "loss: 0.270624  [518400/1245446]\n",
      "loss: 0.364457  [524800/1245446]\n",
      "loss: 0.304335  [531200/1245446]\n",
      "loss: 0.280305  [537600/1245446]\n",
      "loss: 0.472167  [544000/1245446]\n",
      "loss: 0.454177  [550400/1245446]\n",
      "loss: 0.386079  [556800/1245446]\n",
      "loss: 0.298774  [563200/1245446]\n",
      "loss: 0.334558  [569600/1245446]\n",
      "loss: 0.437596  [576000/1245446]\n",
      "loss: 0.205359  [582400/1245446]\n",
      "loss: 0.233600  [588800/1245446]\n",
      "loss: 0.298015  [595200/1245446]\n",
      "loss: 0.371640  [601600/1245446]\n",
      "loss: 0.381962  [608000/1245446]\n",
      "loss: 0.220371  [614400/1245446]\n",
      "loss: 0.339431  [620800/1245446]\n",
      "loss: 0.289974  [627200/1245446]\n",
      "loss: 0.284940  [633600/1245446]\n",
      "loss: 0.251683  [640000/1245446]\n",
      "loss: 0.226821  [646400/1245446]\n",
      "loss: 0.306822  [652800/1245446]\n",
      "loss: 0.353287  [659200/1245446]\n",
      "loss: 0.307062  [665600/1245446]\n",
      "loss: 0.251200  [672000/1245446]\n",
      "loss: 0.270542  [678400/1245446]\n",
      "loss: 0.192177  [684800/1245446]\n",
      "loss: 0.328725  [691200/1245446]\n",
      "loss: 0.265458  [697600/1245446]\n",
      "loss: 0.217129  [704000/1245446]\n",
      "loss: 0.404884  [710400/1245446]\n",
      "loss: 0.270117  [716800/1245446]\n",
      "loss: 0.294116  [723200/1245446]\n",
      "loss: 0.428719  [729600/1245446]\n",
      "loss: 0.209475  [736000/1245446]\n",
      "loss: 0.495751  [742400/1245446]\n",
      "loss: 0.273354  [748800/1245446]\n",
      "loss: 0.316108  [755200/1245446]\n",
      "loss: 0.180989  [761600/1245446]\n",
      "loss: 0.239649  [768000/1245446]\n",
      "loss: 0.200688  [774400/1245446]\n",
      "loss: 0.317025  [780800/1245446]\n",
      "loss: 0.350839  [787200/1245446]\n",
      "loss: 0.360415  [793600/1245446]\n",
      "loss: 0.288191  [800000/1245446]\n",
      "loss: 0.262013  [806400/1245446]\n",
      "loss: 0.306168  [812800/1245446]\n",
      "loss: 0.264588  [819200/1245446]\n",
      "loss: 0.580060  [825600/1245446]\n",
      "loss: 0.347439  [832000/1245446]\n",
      "loss: 0.249210  [838400/1245446]\n",
      "loss: 0.233603  [844800/1245446]\n",
      "loss: 0.356885  [851200/1245446]\n",
      "loss: 0.236793  [857600/1245446]\n",
      "loss: 0.254824  [864000/1245446]\n",
      "loss: 0.258859  [870400/1245446]\n",
      "loss: 0.336163  [876800/1245446]\n",
      "loss: 0.419266  [883200/1245446]\n",
      "loss: 0.413980  [889600/1245446]\n",
      "loss: 0.305544  [896000/1245446]\n",
      "loss: 0.292466  [902400/1245446]\n",
      "loss: 0.310313  [908800/1245446]\n",
      "loss: 0.311705  [915200/1245446]\n",
      "loss: 0.350981  [921600/1245446]\n",
      "loss: 0.266756  [928000/1245446]\n",
      "loss: 0.248123  [934400/1245446]\n",
      "loss: 0.303574  [940800/1245446]\n",
      "loss: 0.219103  [947200/1245446]\n",
      "loss: 0.282744  [953600/1245446]\n",
      "loss: 0.280253  [960000/1245446]\n",
      "loss: 0.189575  [966400/1245446]\n",
      "loss: 0.350477  [972800/1245446]\n",
      "loss: 0.351428  [979200/1245446]\n",
      "loss: 0.269054  [985600/1245446]\n",
      "loss: 0.311177  [992000/1245446]\n",
      "loss: 0.251137  [998400/1245446]\n",
      "loss: 0.242461  [1004800/1245446]\n",
      "loss: 0.255434  [1011200/1245446]\n",
      "loss: 0.205645  [1017600/1245446]\n",
      "loss: 0.362062  [1024000/1245446]\n",
      "loss: 0.331505  [1030400/1245446]\n",
      "loss: 0.354365  [1036800/1245446]\n",
      "loss: 0.227453  [1043200/1245446]\n",
      "loss: 0.234338  [1049600/1245446]\n",
      "loss: 0.553669  [1056000/1245446]\n",
      "loss: 0.297203  [1062400/1245446]\n",
      "loss: 0.357450  [1068800/1245446]\n",
      "loss: 0.220062  [1075200/1245446]\n",
      "loss: 0.238224  [1081600/1245446]\n",
      "loss: 0.248342  [1088000/1245446]\n",
      "loss: 0.314695  [1094400/1245446]\n",
      "loss: 0.294578  [1100800/1245446]\n",
      "loss: 0.456826  [1107200/1245446]\n",
      "loss: 0.249924  [1113600/1245446]\n",
      "loss: 0.279602  [1120000/1245446]\n",
      "loss: 0.271121  [1126400/1245446]\n",
      "loss: 0.317687  [1132800/1245446]\n",
      "loss: 0.350974  [1139200/1245446]\n",
      "loss: 0.359108  [1145600/1245446]\n",
      "loss: 0.337127  [1152000/1245446]\n",
      "loss: 0.532381  [1158400/1245446]\n",
      "loss: 0.349152  [1164800/1245446]\n",
      "loss: 0.277979  [1171200/1245446]\n",
      "loss: 0.225752  [1177600/1245446]\n",
      "loss: 0.485139  [1184000/1245446]\n",
      "loss: 0.353269  [1190400/1245446]\n",
      "loss: 0.249008  [1196800/1245446]\n",
      "loss: 0.307240  [1203200/1245446]\n",
      "loss: 0.348715  [1209600/1245446]\n",
      "loss: 0.331196  [1216000/1245446]\n",
      "loss: 0.261228  [1222400/1245446]\n",
      "loss: 0.315295  [1228800/1245446]\n",
      "loss: 0.186795  [1235200/1245446]\n",
      "loss: 0.381292  [1241600/1245446]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 0.316246 \n",
      "\n",
      "-------------------------------------------------\n",
      "Epoch: 2\n",
      "loss: 0.374003  [    0/1245446]\n",
      "loss: 0.347822  [ 6400/1245446]\n",
      "loss: 0.325924  [12800/1245446]\n",
      "loss: 0.261003  [19200/1245446]\n",
      "loss: 0.283407  [25600/1245446]\n",
      "loss: 0.293085  [32000/1245446]\n",
      "loss: 0.298559  [38400/1245446]\n",
      "loss: 0.237744  [44800/1245446]\n",
      "loss: 0.260018  [51200/1245446]\n",
      "loss: 0.329773  [57600/1245446]\n",
      "loss: 0.410107  [64000/1245446]\n",
      "loss: 0.330165  [70400/1245446]\n",
      "loss: 0.549265  [76800/1245446]\n",
      "loss: 0.391932  [83200/1245446]\n",
      "loss: 0.511987  [89600/1245446]\n",
      "loss: 0.306238  [96000/1245446]\n",
      "loss: 0.176442  [102400/1245446]\n",
      "loss: 0.392248  [108800/1245446]\n",
      "loss: 0.313682  [115200/1245446]\n",
      "loss: 0.430715  [121600/1245446]\n",
      "loss: 0.259209  [128000/1245446]\n",
      "loss: 0.267107  [134400/1245446]\n",
      "loss: 0.244389  [140800/1245446]\n",
      "loss: 0.354642  [147200/1245446]\n",
      "loss: 0.229513  [153600/1245446]\n",
      "loss: 0.314724  [160000/1245446]\n",
      "loss: 0.299247  [166400/1245446]\n",
      "loss: 0.335832  [172800/1245446]\n",
      "loss: 0.397156  [179200/1245446]\n",
      "loss: 0.197201  [185600/1245446]\n",
      "loss: 0.257057  [192000/1245446]\n",
      "loss: 0.337225  [198400/1245446]\n",
      "loss: 0.432531  [204800/1245446]\n",
      "loss: 0.258439  [211200/1245446]\n",
      "loss: 0.278038  [217600/1245446]\n",
      "loss: 0.319296  [224000/1245446]\n",
      "loss: 0.285818  [230400/1245446]\n",
      "loss: 0.238416  [236800/1245446]\n",
      "loss: 0.307955  [243200/1245446]\n",
      "loss: 0.240178  [249600/1245446]\n",
      "loss: 0.280903  [256000/1245446]\n",
      "loss: 0.285400  [262400/1245446]\n",
      "loss: 0.421908  [268800/1245446]\n",
      "loss: 0.328062  [275200/1245446]\n",
      "loss: 0.213857  [281600/1245446]\n",
      "loss: 0.277375  [288000/1245446]\n",
      "loss: 0.267743  [294400/1245446]\n",
      "loss: 0.317612  [300800/1245446]\n",
      "loss: 0.395402  [307200/1245446]\n",
      "loss: 0.253181  [313600/1245446]\n",
      "loss: 0.532715  [320000/1245446]\n",
      "loss: 0.463416  [326400/1245446]\n",
      "loss: 0.305037  [332800/1245446]\n",
      "loss: 0.328890  [339200/1245446]\n",
      "loss: 0.152292  [345600/1245446]\n",
      "loss: 0.408181  [352000/1245446]\n",
      "loss: 0.328746  [358400/1245446]\n",
      "loss: 0.273632  [364800/1245446]\n",
      "loss: 0.375019  [371200/1245446]\n",
      "loss: 0.191909  [377600/1245446]\n",
      "loss: 0.258212  [384000/1245446]\n",
      "loss: 0.370117  [390400/1245446]\n",
      "loss: 0.241946  [396800/1245446]\n",
      "loss: 0.393334  [403200/1245446]\n",
      "loss: 0.346884  [409600/1245446]\n",
      "loss: 0.362474  [416000/1245446]\n",
      "loss: 0.345673  [422400/1245446]\n",
      "loss: 0.401042  [428800/1245446]\n",
      "loss: 0.391340  [435200/1245446]\n",
      "loss: 0.365211  [441600/1245446]\n",
      "loss: 0.290616  [448000/1245446]\n",
      "loss: 0.262305  [454400/1245446]\n",
      "loss: 0.205396  [460800/1245446]\n",
      "loss: 0.221908  [467200/1245446]\n",
      "loss: 0.176391  [473600/1245446]\n",
      "loss: 0.321068  [480000/1245446]\n",
      "loss: 0.409016  [486400/1245446]\n",
      "loss: 0.202224  [492800/1245446]\n",
      "loss: 0.537609  [499200/1245446]\n",
      "loss: 0.188544  [505600/1245446]\n",
      "loss: 0.344798  [512000/1245446]\n",
      "loss: 0.304800  [518400/1245446]\n",
      "loss: 0.358019  [524800/1245446]\n",
      "loss: 0.274406  [531200/1245446]\n",
      "loss: 0.305061  [537600/1245446]\n",
      "loss: 0.294879  [544000/1245446]\n",
      "loss: 0.307207  [550400/1245446]\n",
      "loss: 0.359544  [556800/1245446]\n",
      "loss: 0.339440  [563200/1245446]\n",
      "loss: 0.194629  [569600/1245446]\n",
      "loss: 0.433646  [576000/1245446]\n",
      "loss: 0.326353  [582400/1245446]\n",
      "loss: 0.279074  [588800/1245446]\n",
      "loss: 0.194063  [595200/1245446]\n",
      "loss: 0.331781  [601600/1245446]\n",
      "loss: 0.347752  [608000/1245446]\n",
      "loss: 0.541295  [614400/1245446]\n",
      "loss: 0.354602  [620800/1245446]\n",
      "loss: 0.347334  [627200/1245446]\n",
      "loss: 0.211800  [633600/1245446]\n",
      "loss: 0.490774  [640000/1245446]\n",
      "loss: 0.359587  [646400/1245446]\n",
      "loss: 0.361844  [652800/1245446]\n",
      "loss: 0.344596  [659200/1245446]\n",
      "loss: 0.260730  [665600/1245446]\n",
      "loss: 0.293282  [672000/1245446]\n",
      "loss: 0.317276  [678400/1245446]\n",
      "loss: 0.304525  [684800/1245446]\n",
      "loss: 0.337339  [691200/1245446]\n",
      "loss: 0.300958  [697600/1245446]\n",
      "loss: 0.342878  [704000/1245446]\n",
      "loss: 0.219513  [710400/1245446]\n",
      "loss: 0.315904  [716800/1245446]\n",
      "loss: 0.332792  [723200/1245446]\n",
      "loss: 0.336335  [729600/1245446]\n",
      "loss: 0.317852  [736000/1245446]\n",
      "loss: 0.394738  [742400/1245446]\n",
      "loss: 0.279774  [748800/1245446]\n",
      "loss: 0.421321  [755200/1245446]\n",
      "loss: 0.248858  [761600/1245446]\n",
      "loss: 0.341366  [768000/1245446]\n",
      "loss: 0.337793  [774400/1245446]\n",
      "loss: 0.276729  [780800/1245446]\n",
      "loss: 0.267539  [787200/1245446]\n",
      "loss: 0.325255  [793600/1245446]\n",
      "loss: 0.199978  [800000/1245446]\n",
      "loss: 0.375178  [806400/1245446]\n",
      "loss: 0.293232  [812800/1245446]\n",
      "loss: 0.410696  [819200/1245446]\n",
      "loss: 0.342755  [825600/1245446]\n",
      "loss: 0.305163  [832000/1245446]\n",
      "loss: 0.399742  [838400/1245446]\n",
      "loss: 0.296042  [844800/1245446]\n",
      "loss: 0.342313  [851200/1245446]\n",
      "loss: 0.348909  [857600/1245446]\n",
      "loss: 0.397645  [864000/1245446]\n",
      "loss: 0.329921  [870400/1245446]\n",
      "loss: 0.358947  [876800/1245446]\n",
      "loss: 0.242165  [883200/1245446]\n",
      "loss: 0.387329  [889600/1245446]\n",
      "loss: 0.274893  [896000/1245446]\n",
      "loss: 0.236477  [902400/1245446]\n",
      "loss: 0.252338  [908800/1245446]\n",
      "loss: 0.358815  [915200/1245446]\n",
      "loss: 0.354008  [921600/1245446]\n",
      "loss: 0.364044  [928000/1245446]\n",
      "loss: 0.302758  [934400/1245446]\n",
      "loss: 0.252859  [940800/1245446]\n",
      "loss: 0.419175  [947200/1245446]\n",
      "loss: 0.312380  [953600/1245446]\n",
      "loss: 0.406950  [960000/1245446]\n",
      "loss: 0.358317  [966400/1245446]\n",
      "loss: 0.330165  [972800/1245446]\n",
      "loss: 0.426137  [979200/1245446]\n",
      "loss: 0.255704  [985600/1245446]\n",
      "loss: 0.254000  [992000/1245446]\n",
      "loss: 0.293433  [998400/1245446]\n",
      "loss: 0.274352  [1004800/1245446]\n",
      "loss: 0.366359  [1011200/1245446]\n",
      "loss: 0.275425  [1017600/1245446]\n",
      "loss: 0.287339  [1024000/1245446]\n",
      "loss: 0.225667  [1030400/1245446]\n",
      "loss: 0.304127  [1036800/1245446]\n",
      "loss: 0.361813  [1043200/1245446]\n",
      "loss: 0.218876  [1049600/1245446]\n",
      "loss: 0.388750  [1056000/1245446]\n",
      "loss: 0.306056  [1062400/1245446]\n",
      "loss: 0.240414  [1068800/1245446]\n",
      "loss: 0.219658  [1075200/1245446]\n",
      "loss: 0.300092  [1081600/1245446]\n",
      "loss: 0.374925  [1088000/1245446]\n",
      "loss: 0.277410  [1094400/1245446]\n",
      "loss: 0.220752  [1100800/1245446]\n",
      "loss: 0.410223  [1107200/1245446]\n",
      "loss: 0.393642  [1113600/1245446]\n",
      "loss: 0.351481  [1120000/1245446]\n",
      "loss: 0.203148  [1126400/1245446]\n",
      "loss: 0.319682  [1132800/1245446]\n",
      "loss: 0.281933  [1139200/1245446]\n",
      "loss: 0.252522  [1145600/1245446]\n",
      "loss: 0.303968  [1152000/1245446]\n",
      "loss: 0.263218  [1158400/1245446]\n",
      "loss: 0.299184  [1164800/1245446]\n",
      "loss: 0.196998  [1171200/1245446]\n",
      "loss: 0.343798  [1177600/1245446]\n",
      "loss: 0.252438  [1184000/1245446]\n",
      "loss: 0.324132  [1190400/1245446]\n",
      "loss: 0.229651  [1196800/1245446]\n",
      "loss: 0.309309  [1203200/1245446]\n",
      "loss: 0.292943  [1209600/1245446]\n",
      "loss: 0.325892  [1216000/1245446]\n",
      "loss: 0.454407  [1222400/1245446]\n",
      "loss: 0.264789  [1228800/1245446]\n",
      "loss: 0.294477  [1235200/1245446]\n",
      "loss: 0.244276  [1241600/1245446]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 0.316188 \n",
      "\n",
      "-------------------------------------------------\n",
      "Epoch: 3\n",
      "loss: 0.286283  [    0/1245446]\n",
      "loss: 0.247115  [ 6400/1245446]\n",
      "loss: 0.326380  [12800/1245446]\n",
      "loss: 0.340989  [19200/1245446]\n",
      "loss: 0.305334  [25600/1245446]\n",
      "loss: 0.338783  [32000/1245446]\n",
      "loss: 0.263002  [38400/1245446]\n",
      "loss: 0.400002  [44800/1245446]\n",
      "loss: 0.306813  [51200/1245446]\n",
      "loss: 0.228797  [57600/1245446]\n",
      "loss: 0.331521  [64000/1245446]\n",
      "loss: 0.457744  [70400/1245446]\n",
      "loss: 0.340596  [76800/1245446]\n",
      "loss: 0.289807  [83200/1245446]\n",
      "loss: 0.290124  [89600/1245446]\n",
      "loss: 0.350511  [96000/1245446]\n",
      "loss: 0.393119  [102400/1245446]\n",
      "loss: 0.374506  [108800/1245446]\n",
      "loss: 0.405423  [115200/1245446]\n",
      "loss: 0.349271  [121600/1245446]\n",
      "loss: 0.291515  [128000/1245446]\n",
      "loss: 0.315785  [134400/1245446]\n",
      "loss: 0.399495  [140800/1245446]\n",
      "loss: 0.265910  [147200/1245446]\n",
      "loss: 0.327071  [153600/1245446]\n",
      "loss: 0.274060  [160000/1245446]\n",
      "loss: 0.332345  [166400/1245446]\n",
      "loss: 0.348344  [172800/1245446]\n",
      "loss: 0.226803  [179200/1245446]\n",
      "loss: 0.395655  [185600/1245446]\n",
      "loss: 0.306969  [192000/1245446]\n",
      "loss: 0.461745  [198400/1245446]\n",
      "loss: 0.193757  [204800/1245446]\n",
      "loss: 0.216822  [211200/1245446]\n",
      "loss: 0.328227  [217600/1245446]\n",
      "loss: 0.269746  [224000/1245446]\n",
      "loss: 0.325105  [230400/1245446]\n",
      "loss: 0.351859  [236800/1245446]\n",
      "loss: 0.252506  [243200/1245446]\n",
      "loss: 0.303653  [249600/1245446]\n",
      "loss: 0.343834  [256000/1245446]\n",
      "loss: 0.382700  [262400/1245446]\n",
      "loss: 0.264747  [268800/1245446]\n",
      "loss: 0.242106  [275200/1245446]\n",
      "loss: 0.208046  [281600/1245446]\n",
      "loss: 0.294892  [288000/1245446]\n",
      "loss: 0.390840  [294400/1245446]\n",
      "loss: 0.356138  [300800/1245446]\n",
      "loss: 0.395456  [307200/1245446]\n",
      "loss: 0.294180  [313600/1245446]\n",
      "loss: 0.370520  [320000/1245446]\n",
      "loss: 0.398175  [326400/1245446]\n",
      "loss: 0.316687  [332800/1245446]\n",
      "loss: 0.230837  [339200/1245446]\n",
      "loss: 0.279884  [345600/1245446]\n",
      "loss: 0.268105  [352000/1245446]\n",
      "loss: 0.314393  [358400/1245446]\n",
      "loss: 0.242157  [364800/1245446]\n",
      "loss: 0.304367  [371200/1245446]\n",
      "loss: 0.307508  [377600/1245446]\n",
      "loss: 0.312106  [384000/1245446]\n",
      "loss: 0.190243  [390400/1245446]\n",
      "loss: 0.417482  [396800/1245446]\n",
      "loss: 0.311008  [403200/1245446]\n",
      "loss: 0.238712  [409600/1245446]\n",
      "loss: 0.571415  [416000/1245446]\n",
      "loss: 0.349357  [422400/1245446]\n",
      "loss: 0.348188  [428800/1245446]\n",
      "loss: 0.225438  [435200/1245446]\n",
      "loss: 0.264338  [441600/1245446]\n",
      "loss: 0.157609  [448000/1245446]\n",
      "loss: 0.325360  [454400/1245446]\n",
      "loss: 0.288473  [460800/1245446]\n",
      "loss: 0.392819  [467200/1245446]\n",
      "loss: 0.358903  [473600/1245446]\n",
      "loss: 0.307134  [480000/1245446]\n",
      "loss: 0.370071  [486400/1245446]\n",
      "loss: 0.372677  [492800/1245446]\n",
      "loss: 0.401168  [499200/1245446]\n",
      "loss: 0.250988  [505600/1245446]\n",
      "loss: 0.258884  [512000/1245446]\n",
      "loss: 0.394054  [518400/1245446]\n",
      "loss: 0.234583  [524800/1245446]\n",
      "loss: 0.435359  [531200/1245446]\n",
      "loss: 0.325525  [537600/1245446]\n",
      "loss: 0.416958  [544000/1245446]\n",
      "loss: 0.338139  [550400/1245446]\n",
      "loss: 0.211229  [556800/1245446]\n",
      "loss: 0.169536  [563200/1245446]\n",
      "loss: 0.300158  [569600/1245446]\n",
      "loss: 0.277016  [576000/1245446]\n",
      "loss: 0.306845  [582400/1245446]\n",
      "loss: 0.191645  [588800/1245446]\n",
      "loss: 0.405593  [595200/1245446]\n",
      "loss: 0.420042  [601600/1245446]\n",
      "loss: 0.328041  [608000/1245446]\n",
      "loss: 0.470428  [614400/1245446]\n",
      "loss: 0.321696  [620800/1245446]\n",
      "loss: 0.271220  [627200/1245446]\n",
      "loss: 0.344436  [633600/1245446]\n",
      "loss: 0.322082  [640000/1245446]\n",
      "loss: 0.311465  [646400/1245446]\n",
      "loss: 0.322764  [652800/1245446]\n",
      "loss: 0.302350  [659200/1245446]\n",
      "loss: 0.302052  [665600/1245446]\n",
      "loss: 0.381181  [672000/1245446]\n",
      "loss: 0.382894  [678400/1245446]\n",
      "loss: 0.175196  [684800/1245446]\n",
      "loss: 0.262031  [691200/1245446]\n",
      "loss: 0.235862  [697600/1245446]\n",
      "loss: 0.193318  [704000/1245446]\n",
      "loss: 0.327040  [710400/1245446]\n",
      "loss: 0.339238  [716800/1245446]\n",
      "loss: 0.246601  [723200/1245446]\n",
      "loss: 0.227214  [729600/1245446]\n",
      "loss: 0.352644  [736000/1245446]\n",
      "loss: 0.296332  [742400/1245446]\n",
      "loss: 0.418682  [748800/1245446]\n",
      "loss: 0.291975  [755200/1245446]\n",
      "loss: 0.356513  [761600/1245446]\n",
      "loss: 0.264782  [768000/1245446]\n",
      "loss: 0.433662  [774400/1245446]\n",
      "loss: 0.427286  [780800/1245446]\n",
      "loss: 0.295478  [787200/1245446]\n",
      "loss: 0.397631  [793600/1245446]\n",
      "loss: 0.417260  [800000/1245446]\n",
      "loss: 0.286598  [806400/1245446]\n",
      "loss: 0.364540  [812800/1245446]\n",
      "loss: 0.218971  [819200/1245446]\n",
      "loss: 0.390909  [825600/1245446]\n",
      "loss: 0.318923  [832000/1245446]\n",
      "loss: 0.302732  [838400/1245446]\n",
      "loss: 0.356176  [844800/1245446]\n",
      "loss: 0.248870  [851200/1245446]\n",
      "loss: 0.344561  [857600/1245446]\n",
      "loss: 0.302992  [864000/1245446]\n",
      "loss: 0.264248  [870400/1245446]\n",
      "loss: 0.351244  [876800/1245446]\n",
      "loss: 0.334578  [883200/1245446]\n",
      "loss: 0.387585  [889600/1245446]\n",
      "loss: 0.190687  [896000/1245446]\n",
      "loss: 0.334001  [902400/1245446]\n",
      "loss: 0.273203  [908800/1245446]\n",
      "loss: 0.326176  [915200/1245446]\n",
      "loss: 0.366690  [921600/1245446]\n",
      "loss: 0.271833  [928000/1245446]\n",
      "loss: 0.251860  [934400/1245446]\n",
      "loss: 0.273438  [940800/1245446]\n",
      "loss: 0.304550  [947200/1245446]\n",
      "loss: 0.284018  [953600/1245446]\n",
      "loss: 0.176900  [960000/1245446]\n",
      "loss: 0.240403  [966400/1245446]\n",
      "loss: 0.447096  [972800/1245446]\n",
      "loss: 0.435748  [979200/1245446]\n",
      "loss: 0.238910  [985600/1245446]\n",
      "loss: 0.297414  [992000/1245446]\n",
      "loss: 0.339391  [998400/1245446]\n",
      "loss: 0.346478  [1004800/1245446]\n",
      "loss: 0.439123  [1011200/1245446]\n",
      "loss: 0.232916  [1017600/1245446]\n",
      "loss: 0.344900  [1024000/1245446]\n",
      "loss: 0.337741  [1030400/1245446]\n",
      "loss: 0.284042  [1036800/1245446]\n",
      "loss: 0.341080  [1043200/1245446]\n",
      "loss: 0.292473  [1049600/1245446]\n",
      "loss: 0.426152  [1056000/1245446]\n",
      "loss: 0.308865  [1062400/1245446]\n",
      "loss: 0.466359  [1068800/1245446]\n",
      "loss: 0.273085  [1075200/1245446]\n",
      "loss: 0.310890  [1081600/1245446]\n",
      "loss: 0.177392  [1088000/1245446]\n",
      "loss: 0.341525  [1094400/1245446]\n",
      "loss: 0.359085  [1100800/1245446]\n",
      "loss: 0.284075  [1107200/1245446]\n",
      "loss: 0.237631  [1113600/1245446]\n",
      "loss: 0.232551  [1120000/1245446]\n",
      "loss: 0.287134  [1126400/1245446]\n",
      "loss: 0.240163  [1132800/1245446]\n",
      "loss: 0.381567  [1139200/1245446]\n",
      "loss: 0.277625  [1145600/1245446]\n",
      "loss: 0.286861  [1152000/1245446]\n",
      "loss: 0.177390  [1158400/1245446]\n",
      "loss: 0.319499  [1164800/1245446]\n",
      "loss: 0.216958  [1171200/1245446]\n",
      "loss: 0.439544  [1177600/1245446]\n",
      "loss: 0.336761  [1184000/1245446]\n",
      "loss: 0.508336  [1190400/1245446]\n",
      "loss: 0.378468  [1196800/1245446]\n",
      "loss: 0.248378  [1203200/1245446]\n",
      "loss: 0.286711  [1209600/1245446]\n",
      "loss: 0.271688  [1216000/1245446]\n",
      "loss: 0.234883  [1222400/1245446]\n",
      "loss: 0.280982  [1228800/1245446]\n",
      "loss: 0.260447  [1235200/1245446]\n",
      "loss: 0.411464  [1241600/1245446]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 0.316152 \n",
      "\n",
      "-------------------------------------------------\n",
      "Epoch: 4\n",
      "loss: 0.263585  [    0/1245446]\n",
      "loss: 0.364971  [ 6400/1245446]\n",
      "loss: 0.371484  [12800/1245446]\n",
      "loss: 0.296936  [19200/1245446]\n",
      "loss: 0.212478  [25600/1245446]\n",
      "loss: 0.313766  [32000/1245446]\n",
      "loss: 0.370349  [38400/1245446]\n",
      "loss: 0.246368  [44800/1245446]\n",
      "loss: 0.366678  [51200/1245446]\n",
      "loss: 0.329796  [57600/1245446]\n",
      "loss: 0.344587  [64000/1245446]\n",
      "loss: 0.370479  [70400/1245446]\n",
      "loss: 0.287113  [76800/1245446]\n",
      "loss: 0.270513  [83200/1245446]\n",
      "loss: 0.282834  [89600/1245446]\n",
      "loss: 0.216992  [96000/1245446]\n",
      "loss: 0.395770  [102400/1245446]\n",
      "loss: 0.376134  [108800/1245446]\n",
      "loss: 0.208539  [115200/1245446]\n",
      "loss: 0.393557  [121600/1245446]\n",
      "loss: 0.309044  [128000/1245446]\n",
      "loss: 0.348148  [134400/1245446]\n",
      "loss: 0.365845  [140800/1245446]\n",
      "loss: 0.357774  [147200/1245446]\n",
      "loss: 0.332992  [153600/1245446]\n",
      "loss: 0.227667  [160000/1245446]\n",
      "loss: 0.288483  [166400/1245446]\n",
      "loss: 0.427944  [172800/1245446]\n",
      "loss: 0.255197  [179200/1245446]\n",
      "loss: 0.305223  [185600/1245446]\n",
      "loss: 0.413011  [192000/1245446]\n",
      "loss: 0.289419  [198400/1245446]\n",
      "loss: 0.304023  [204800/1245446]\n",
      "loss: 0.441041  [211200/1245446]\n",
      "loss: 0.235700  [217600/1245446]\n",
      "loss: 0.307013  [224000/1245446]\n",
      "loss: 0.364606  [230400/1245446]\n",
      "loss: 0.330878  [236800/1245446]\n",
      "loss: 0.371948  [243200/1245446]\n",
      "loss: 0.219260  [249600/1245446]\n",
      "loss: 0.378130  [256000/1245446]\n",
      "loss: 0.354177  [262400/1245446]\n",
      "loss: 0.389591  [268800/1245446]\n",
      "loss: 0.269608  [275200/1245446]\n",
      "loss: 0.406888  [281600/1245446]\n",
      "loss: 0.275179  [288000/1245446]\n",
      "loss: 0.326163  [294400/1245446]\n",
      "loss: 0.358337  [300800/1245446]\n",
      "loss: 0.246574  [307200/1245446]\n",
      "loss: 0.369060  [313600/1245446]\n",
      "loss: 0.435982  [320000/1245446]\n",
      "loss: 0.303483  [326400/1245446]\n",
      "loss: 0.353877  [332800/1245446]\n",
      "loss: 0.386353  [339200/1245446]\n",
      "loss: 0.352791  [345600/1245446]\n",
      "loss: 0.306786  [352000/1245446]\n",
      "loss: 0.138872  [358400/1245446]\n",
      "loss: 0.394694  [364800/1245446]\n",
      "loss: 0.261796  [371200/1245446]\n",
      "loss: 0.383656  [377600/1245446]\n",
      "loss: 0.326525  [384000/1245446]\n",
      "loss: 0.260419  [390400/1245446]\n",
      "loss: 0.326095  [396800/1245446]\n",
      "loss: 0.267429  [403200/1245446]\n",
      "loss: 0.419999  [409600/1245446]\n",
      "loss: 0.329591  [416000/1245446]\n",
      "loss: 0.247495  [422400/1245446]\n",
      "loss: 0.194389  [428800/1245446]\n",
      "loss: 0.318371  [435200/1245446]\n",
      "loss: 0.453001  [441600/1245446]\n",
      "loss: 0.290826  [448000/1245446]\n",
      "loss: 0.294866  [454400/1245446]\n",
      "loss: 0.260800  [460800/1245446]\n",
      "loss: 0.383587  [467200/1245446]\n",
      "loss: 0.229090  [473600/1245446]\n",
      "loss: 0.303260  [480000/1245446]\n",
      "loss: 0.298041  [486400/1245446]\n",
      "loss: 0.314206  [492800/1245446]\n",
      "loss: 0.225701  [499200/1245446]\n",
      "loss: 0.284544  [505600/1245446]\n",
      "loss: 0.278185  [512000/1245446]\n",
      "loss: 0.242098  [518400/1245446]\n",
      "loss: 0.297612  [524800/1245446]\n",
      "loss: 0.286613  [531200/1245446]\n",
      "loss: 0.440558  [537600/1245446]\n",
      "loss: 0.314723  [544000/1245446]\n",
      "loss: 0.210156  [550400/1245446]\n",
      "loss: 0.280794  [556800/1245446]\n",
      "loss: 0.403603  [563200/1245446]\n",
      "loss: 0.292113  [569600/1245446]\n",
      "loss: 0.408676  [576000/1245446]\n",
      "loss: 0.221463  [582400/1245446]\n",
      "loss: 0.334877  [588800/1245446]\n",
      "loss: 0.487197  [595200/1245446]\n",
      "loss: 0.209366  [601600/1245446]\n",
      "loss: 0.188943  [608000/1245446]\n",
      "loss: 0.373096  [614400/1245446]\n",
      "loss: 0.251898  [620800/1245446]\n",
      "loss: 0.378486  [627200/1245446]\n",
      "loss: 0.346063  [633600/1245446]\n",
      "loss: 0.277519  [640000/1245446]\n",
      "loss: 0.319092  [646400/1245446]\n",
      "loss: 0.263845  [652800/1245446]\n",
      "loss: 0.251324  [659200/1245446]\n",
      "loss: 0.308056  [665600/1245446]\n",
      "loss: 0.300554  [672000/1245446]\n",
      "loss: 0.334962  [678400/1245446]\n",
      "loss: 0.276025  [684800/1245446]\n",
      "loss: 0.238951  [691200/1245446]\n",
      "loss: 0.263048  [697600/1245446]\n",
      "loss: 0.206547  [704000/1245446]\n",
      "loss: 0.344545  [710400/1245446]\n",
      "loss: 0.349931  [716800/1245446]\n",
      "loss: 0.414058  [723200/1245446]\n",
      "loss: 0.191105  [729600/1245446]\n",
      "loss: 0.326110  [736000/1245446]\n",
      "loss: 0.251167  [742400/1245446]\n",
      "loss: 0.286148  [748800/1245446]\n",
      "loss: 0.261613  [755200/1245446]\n",
      "loss: 0.220323  [761600/1245446]\n",
      "loss: 0.319851  [768000/1245446]\n",
      "loss: 0.290404  [774400/1245446]\n",
      "loss: 0.303753  [780800/1245446]\n",
      "loss: 0.344840  [787200/1245446]\n",
      "loss: 0.354135  [793600/1245446]\n",
      "loss: 0.410816  [800000/1245446]\n",
      "loss: 0.320177  [806400/1245446]\n",
      "loss: 0.303703  [812800/1245446]\n",
      "loss: 0.313591  [819200/1245446]\n",
      "loss: 0.398179  [825600/1245446]\n",
      "loss: 0.381828  [832000/1245446]\n",
      "loss: 0.310870  [838400/1245446]\n",
      "loss: 0.459700  [844800/1245446]\n",
      "loss: 0.447397  [851200/1245446]\n",
      "loss: 0.513161  [857600/1245446]\n",
      "loss: 0.324637  [864000/1245446]\n",
      "loss: 0.334925  [870400/1245446]\n",
      "loss: 0.273760  [876800/1245446]\n",
      "loss: 0.344270  [883200/1245446]\n",
      "loss: 0.314307  [889600/1245446]\n",
      "loss: 0.327301  [896000/1245446]\n",
      "loss: 0.447539  [902400/1245446]\n",
      "loss: 0.226097  [908800/1245446]\n",
      "loss: 0.148948  [915200/1245446]\n",
      "loss: 0.218182  [921600/1245446]\n",
      "loss: 0.332538  [928000/1245446]\n",
      "loss: 0.335820  [934400/1245446]\n",
      "loss: 0.346133  [940800/1245446]\n",
      "loss: 0.256987  [947200/1245446]\n",
      "loss: 0.339451  [953600/1245446]\n",
      "loss: 0.363248  [960000/1245446]\n",
      "loss: 0.324529  [966400/1245446]\n",
      "loss: 0.233355  [972800/1245446]\n",
      "loss: 0.334097  [979200/1245446]\n",
      "loss: 0.477645  [985600/1245446]\n",
      "loss: 0.275781  [992000/1245446]\n",
      "loss: 0.247371  [998400/1245446]\n",
      "loss: 0.235744  [1004800/1245446]\n",
      "loss: 0.253797  [1011200/1245446]\n",
      "loss: 0.198869  [1017600/1245446]\n",
      "loss: 0.407568  [1024000/1245446]\n",
      "loss: 0.366533  [1030400/1245446]\n",
      "loss: 0.338237  [1036800/1245446]\n",
      "loss: 0.168833  [1043200/1245446]\n",
      "loss: 0.276411  [1049600/1245446]\n",
      "loss: 0.288932  [1056000/1245446]\n",
      "loss: 0.260645  [1062400/1245446]\n",
      "loss: 0.206465  [1068800/1245446]\n",
      "loss: 0.407077  [1075200/1245446]\n",
      "loss: 0.259579  [1081600/1245446]\n",
      "loss: 0.353604  [1088000/1245446]\n",
      "loss: 0.207092  [1094400/1245446]\n",
      "loss: 0.313345  [1100800/1245446]\n",
      "loss: 0.270713  [1107200/1245446]\n",
      "loss: 0.305360  [1113600/1245446]\n",
      "loss: 0.217442  [1120000/1245446]\n",
      "loss: 0.275596  [1126400/1245446]\n",
      "loss: 0.281620  [1132800/1245446]\n",
      "loss: 0.429124  [1139200/1245446]\n",
      "loss: 0.211950  [1145600/1245446]\n",
      "loss: 0.329336  [1152000/1245446]\n",
      "loss: 0.472476  [1158400/1245446]\n",
      "loss: 0.256221  [1164800/1245446]\n",
      "loss: 0.353403  [1171200/1245446]\n",
      "loss: 0.318072  [1177600/1245446]\n",
      "loss: 0.127486  [1184000/1245446]\n",
      "loss: 0.231029  [1190400/1245446]\n",
      "loss: 0.239226  [1196800/1245446]\n",
      "loss: 0.336286  [1203200/1245446]\n",
      "loss: 0.330842  [1209600/1245446]\n",
      "loss: 0.310334  [1216000/1245446]\n",
      "loss: 0.288764  [1222400/1245446]\n",
      "loss: 0.345363  [1228800/1245446]\n",
      "loss: 0.336280  [1235200/1245446]\n",
      "loss: 0.376190  [1241600/1245446]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 0.316122 \n",
      "\n",
      "-------------------------------------------------\n",
      "Epoch: 5\n",
      "loss: 0.306092  [    0/1245446]\n",
      "loss: 0.217478  [ 6400/1245446]\n",
      "loss: 0.336077  [12800/1245446]\n",
      "loss: 0.381985  [19200/1245446]\n",
      "loss: 0.285013  [25600/1245446]\n",
      "loss: 0.283559  [32000/1245446]\n",
      "loss: 0.348996  [38400/1245446]\n",
      "loss: 0.394757  [44800/1245446]\n",
      "loss: 0.387665  [51200/1245446]\n",
      "loss: 0.445273  [57600/1245446]\n",
      "loss: 0.239481  [64000/1245446]\n",
      "loss: 0.337381  [70400/1245446]\n",
      "loss: 0.315065  [76800/1245446]\n",
      "loss: 0.406032  [83200/1245446]\n",
      "loss: 0.210086  [89600/1245446]\n",
      "loss: 0.335369  [96000/1245446]\n",
      "loss: 0.371353  [102400/1245446]\n",
      "loss: 0.340292  [108800/1245446]\n",
      "loss: 0.251846  [115200/1245446]\n",
      "loss: 0.347280  [121600/1245446]\n",
      "loss: 0.348399  [128000/1245446]\n",
      "loss: 0.352570  [134400/1245446]\n",
      "loss: 0.293845  [140800/1245446]\n",
      "loss: 0.316211  [147200/1245446]\n",
      "loss: 0.281466  [153600/1245446]\n",
      "loss: 0.317208  [160000/1245446]\n",
      "loss: 0.422939  [166400/1245446]\n",
      "loss: 0.334560  [172800/1245446]\n",
      "loss: 0.361135  [179200/1245446]\n",
      "loss: 0.366610  [185600/1245446]\n",
      "loss: 0.418447  [192000/1245446]\n",
      "loss: 0.194774  [198400/1245446]\n",
      "loss: 0.335143  [204800/1245446]\n",
      "loss: 0.306214  [211200/1245446]\n",
      "loss: 0.433587  [217600/1245446]\n",
      "loss: 0.255941  [224000/1245446]\n",
      "loss: 0.380754  [230400/1245446]\n",
      "loss: 0.289461  [236800/1245446]\n",
      "loss: 0.297711  [243200/1245446]\n",
      "loss: 0.188426  [249600/1245446]\n",
      "loss: 0.317096  [256000/1245446]\n",
      "loss: 0.259317  [262400/1245446]\n",
      "loss: 0.258858  [268800/1245446]\n",
      "loss: 0.274727  [275200/1245446]\n",
      "loss: 0.369078  [281600/1245446]\n",
      "loss: 0.300598  [288000/1245446]\n",
      "loss: 0.439079  [294400/1245446]\n",
      "loss: 0.304767  [300800/1245446]\n",
      "loss: 0.356301  [307200/1245446]\n",
      "loss: 0.286735  [313600/1245446]\n",
      "loss: 0.255488  [320000/1245446]\n",
      "loss: 0.341279  [326400/1245446]\n",
      "loss: 0.309140  [332800/1245446]\n",
      "loss: 0.346518  [339200/1245446]\n",
      "loss: 0.290962  [345600/1245446]\n",
      "loss: 0.403897  [352000/1245446]\n",
      "loss: 0.448152  [358400/1245446]\n",
      "loss: 0.226815  [364800/1245446]\n",
      "loss: 0.273823  [371200/1245446]\n",
      "loss: 0.350687  [377600/1245446]\n",
      "loss: 0.406234  [384000/1245446]\n",
      "loss: 0.248359  [390400/1245446]\n",
      "loss: 0.249241  [396800/1245446]\n",
      "loss: 0.431877  [403200/1245446]\n",
      "loss: 0.381740  [409600/1245446]\n",
      "loss: 0.364001  [416000/1245446]\n",
      "loss: 0.319328  [422400/1245446]\n",
      "loss: 0.267439  [428800/1245446]\n",
      "loss: 0.344898  [435200/1245446]\n",
      "loss: 0.298068  [441600/1245446]\n",
      "loss: 0.231067  [448000/1245446]\n",
      "loss: 0.354775  [454400/1245446]\n",
      "loss: 0.423481  [460800/1245446]\n",
      "loss: 0.325552  [467200/1245446]\n",
      "loss: 0.214122  [473600/1245446]\n",
      "loss: 0.205976  [480000/1245446]\n",
      "loss: 0.235280  [486400/1245446]\n",
      "loss: 0.348615  [492800/1245446]\n",
      "loss: 0.190427  [499200/1245446]\n",
      "loss: 0.484795  [505600/1245446]\n",
      "loss: 0.357692  [512000/1245446]\n",
      "loss: 0.332007  [518400/1245446]\n",
      "loss: 0.234468  [524800/1245446]\n",
      "loss: 0.298152  [531200/1245446]\n",
      "loss: 0.315785  [537600/1245446]\n",
      "loss: 0.274868  [544000/1245446]\n",
      "loss: 0.326178  [550400/1245446]\n",
      "loss: 0.294532  [556800/1245446]\n",
      "loss: 0.301782  [563200/1245446]\n",
      "loss: 0.549031  [569600/1245446]\n",
      "loss: 0.265222  [576000/1245446]\n",
      "loss: 0.296780  [582400/1245446]\n",
      "loss: 0.196027  [588800/1245446]\n",
      "loss: 0.322055  [595200/1245446]\n",
      "loss: 0.341037  [601600/1245446]\n",
      "loss: 0.306795  [608000/1245446]\n",
      "loss: 0.382495  [614400/1245446]\n",
      "loss: 0.246483  [620800/1245446]\n",
      "loss: 0.298355  [627200/1245446]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41776/437397195.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch: {t+1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41776/1527172916.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# Compute prediction error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = model_selection.KFold(\n",
    "    n_splits=n_splits, random_state=2021, shuffle=True)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(cv.split(X=feature_train)):\n",
    "    print(\"##############################################################\")\n",
    "    print(f\"cv: {i+1}\")\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = get_train_valid_data(\n",
    "        feature_train, target_train, train_idx, valid_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(UbiquantDataset(\n",
    "        X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(UbiquantDataset(\n",
    "        X_valid, y_valid), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(f\"Epoch: {t+1}\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(valid_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06e2c26d81da9559881f4d926ec79778e2b8c97ac068d329245981963e3d233b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
