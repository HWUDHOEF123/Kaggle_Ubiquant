{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-31T00:33:54.783076Z","iopub.status.busy":"2022-03-31T00:33:54.782661Z","iopub.status.idle":"2022-03-31T00:33:55.546284Z","shell.execute_reply":"2022-03-31T00:33:55.545599Z","shell.execute_reply.started":"2022-03-31T00:33:54.783038Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","import os\n","import time\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        pass#print(os.path.join(dirname, filename))\n","\n","from pathlib import Path\n","import sys\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset \n","from torch.utils.data import DataLoader\n","from torch.nn import functional as F \n","import torchvision\n","import torch.nn.utils.prune as prune\n","\n","from tqdm import tqdm\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T23:10:11.338588Z","iopub.status.busy":"2022-03-30T23:10:11.336582Z","iopub.status.idle":"2022-03-30T23:10:11.348513Z","shell.execute_reply":"2022-03-30T23:10:11.347955Z","shell.execute_reply.started":"2022-03-30T23:10:11.338545Z"},"trusted":true},"outputs":[],"source":["# input_path = Path('../input/ubiquant-market-prediction')\n","input_path = Path('../input/ubiquant-parquet')\n","\n","\n","output_path = Path('../output')\n","\n","model_name = 'my_model'\n","result_path = Path('../working')\n","\n","checkpoint_path = result_path / model_name\n","if not checkpoint_path.exists():\n","    checkpoint_path.mkdir(parents=True, exist_ok=True)\n","\n","os.listdir('../')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T23:10:24.212255Z","iopub.status.busy":"2022-03-30T23:10:24.211611Z","iopub.status.idle":"2022-03-30T23:11:30.289349Z","shell.execute_reply":"2022-03-30T23:11:30.288568Z","shell.execute_reply.started":"2022-03-30T23:10:24.212225Z"},"trusted":true},"outputs":[],"source":["# data_type = {f'f_{i}': np.float16 for i in range(300)}\n","# data_type['investment_id'] = np.uint16\n","# data_type['time_id'] = np.uint16\n","\n","# features = pd.read_csv(input_path/'train.csv').set_index(['investment_id','time_id'])\n","features = pd.read_parquet(input_path/'train.parquet', engine='pyarrow').set_index(['investment_id','time_id']).astype(np.float16)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T23:11:30.291064Z","iopub.status.busy":"2022-03-30T23:11:30.290799Z","iopub.status.idle":"2022-03-30T23:11:30.331879Z","shell.execute_reply":"2022-03-30T23:11:30.331031Z","shell.execute_reply.started":"2022-03-30T23:11:30.291032Z"},"trusted":true},"outputs":[],"source":["target = features['target']\n","del features['row_id']\n","del features['target']\n","\n","features.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T23:36:42.644116Z","iopub.status.busy":"2022-03-30T23:36:42.643703Z","iopub.status.idle":"2022-03-30T23:36:42.650955Z","shell.execute_reply":"2022-03-30T23:36:42.650432Z","shell.execute_reply.started":"2022-03-30T23:36:42.644072Z"},"trusted":true},"outputs":[],"source":["print(features.shape, target.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:07:43.85463Z","iopub.status.busy":"2022-03-31T00:07:43.854367Z","iopub.status.idle":"2022-03-31T00:07:43.866781Z","shell.execute_reply":"2022-03-31T00:07:43.865877Z","shell.execute_reply.started":"2022-03-31T00:07:43.854603Z"},"trusted":true},"outputs":[],"source":["class MultipleTimeSeriesCV:\n","    \"\"\"Generates tuples of train_idx, test_idx pairs\n","    Assumes the MultiIndex contains levels 'symbol' and 'date'\n","    purges overlapping outcomes\"\"\"\n","\n","    def __init__(self,\n","                 n_splits=3,\n","                 train_period_length=126,\n","                 test_period_length=21,\n","                 lookahead=None,\n","                 date_idx='date',\n","                 shuffle=False):\n","        self.n_splits = n_splits\n","        self.lookahead = lookahead\n","        self.test_length = test_period_length\n","        self.train_length = train_period_length\n","        self.shuffle = shuffle\n","        self.date_idx = date_idx\n","\n","    def split(self, X, y=None, groups=None):\n","        unique_dates = X.index.get_level_values(self.date_idx).unique()\n","        days = sorted(unique_dates, reverse=True)\n","        split_idx = []\n","        for i in range(self.n_splits):\n","            test_end_idx = i * self.test_length\n","            test_start_idx = test_end_idx + self.test_length\n","            train_end_idx = test_start_idx + self.lookahead - 1\n","            train_start_idx = train_end_idx + self.train_length + self.lookahead - 1\n","            split_idx.append([train_start_idx, train_end_idx,\n","                              test_start_idx, test_end_idx])\n","\n","        dates = X.reset_index()[[self.date_idx]]\n","        for train_start, train_end, test_start, test_end in split_idx:\n","\n","            train_idx = dates[(dates[self.date_idx] > days[train_start])\n","                              & (dates[self.date_idx] <= days[train_end])].index\n","            test_idx = dates[(dates[self.date_idx] > days[test_start])\n","                             & (dates[self.date_idx] <= days[test_end])].index\n","            if self.shuffle:\n","                np.random.shuffle(list(train_idx))\n","            yield train_idx.to_numpy(), test_idx.to_numpy()\n","\n","    def get_n_splits(self, X, y, groups=None):\n","        return self.n_splits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:34:26.703201Z","iopub.status.busy":"2022-03-31T00:34:26.702723Z","iopub.status.idle":"2022-03-31T00:34:26.732847Z","shell.execute_reply":"2022-03-31T00:34:26.731999Z","shell.execute_reply.started":"2022-03-31T00:34:26.703158Z"},"trusted":true},"outputs":[],"source":["class UbiquantDataset(Dataset):\n","    def __init__(self, X, y, transform=None):\n","        self._feature = X\n","        self._k = X.shape[1]\n","        self._n = X.shape[0]\n","        self._target = y\n","        self._transform = transform\n","        \n","    def __len__(self):\n","        return self._n\n","    \n","    def __getitem__(self, idx):\n","        X = self._feature[idx,:].reshape(self._k, 1)\n","        y = self._target[[idx]]\n","        if self._transform:\n","            sample = self._transform(X)\n","        return torch.from_numpy(X).float(), torch.from_numpy(y).float()\n","\n","def get_train_valid_data(X, y, train_idx, test_idx):\n","    x_train, y_train = X.iloc[train_idx, :].to_numpy(), y.iloc[train_idx].to_numpy()\n","    x_val, y_val = X.iloc[test_idx, :].to_numpy(), y.iloc[test_idx].to_numpy()\n","#     scaler = MinMaxScaler(feature_range=(-1, 1))\n","#     x_train = scaler.fit_transform(x_train)\n","#     x_val = scaler.transform(x_val)\n","    return UbiquantDataset(x_train, y_train), UbiquantDataset(x_val, y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:34:27.86148Z","iopub.status.busy":"2022-03-31T00:34:27.861243Z","iopub.status.idle":"2022-03-31T00:34:27.867729Z","shell.execute_reply":"2022-03-31T00:34:27.866933Z","shell.execute_reply.started":"2022-03-31T00:34:27.861455Z"},"trusted":true},"outputs":[],"source":["class torch_model(nn.Module):\n","    def load_model(self, model_path, cuda=False):\n","        pretrained_model = torch.load(f=model_path, map_location=\"cuda\" if cuda else \"cpu\") # Load pre-trained weights in current model\n","        with torch.no_grad():\n","            self.load_state_dict(pretrained_model, strict=True)\n","        \n","        # Debug loading\n","        #print('Parameters found in pretrained model:')\n","        pretrained_layers = pretrained_model.keys()\n","        #for l in pretrained_layers:\n","        #    print(l, \" \")\n","        for name, module in self.state_dict().items(): \n","            if name in pretrained_layers:\n","                assert torch.equal(pretrained_model[name].cpu(), module.cpu())\n","                #print('{} have been loaded correctly in current model.'.format(name))\n","            else:\n","                raise ValueError(\"state_dict() keys do not match\")\n","        print('model weights have been loaded correctly in current model.')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:34:28.447901Z","iopub.status.busy":"2022-03-31T00:34:28.447189Z","iopub.status.idle":"2022-03-31T00:34:28.458839Z","shell.execute_reply":"2022-03-31T00:34:28.458117Z","shell.execute_reply.started":"2022-03-31T00:34:28.447804Z"},"trusted":true},"outputs":[],"source":["class nn_regressor(torch_model): \n","    def __init__(self, param):\n","        super(nn_regressor, self).__init__()\n","        self.n_in = param['n_in']\n","        self.cls = nn.Sequential(\n","            nn.Linear(param['n_in'], param['n_hidden1']),\n","            param['act_f'](),\n","            nn.BatchNorm1d(num_features=param['n_hidden1']),\n","            nn.Linear(param['n_hidden1'], param['n_hidden2']),\n","            param['act_f'](),\n","            nn.BatchNorm1d(num_features=param['n_hidden2']),\n","            nn.Linear(param['n_hidden2'], param['n_hidden3']),\n","            param['act_f'](),\n","            nn.BatchNorm1d(num_features=param['n_hidden3']),\n","            # nn.Linear(n_hidden3, n_hidden4),\n","            # nn.ReLU(),\n","            # nn.BatchNorm1d(num_features=n_hidden4),\n","            nn.Linear(param['n_hidden3'], 1)\n","        )\n","        \n","    def forward(self, x):\n","        x = x.view(-1, self.n_in) \n","        x = self.cls(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:47:22.279089Z","iopub.status.busy":"2022-03-31T00:47:22.278759Z","iopub.status.idle":"2022-03-31T00:47:22.294386Z","shell.execute_reply":"2022-03-31T00:47:22.293578Z","shell.execute_reply.started":"2022-03-31T00:47:22.279053Z"},"trusted":true},"outputs":[],"source":["def test(model, test_dataloader, params):\n","    device = torch.device(\"cpu\")\n","    test_loss = 0.0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_dataloader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = params['criterion'](output, target)\n","            test_loss += loss.item()\n","    return test_loss / len(test_dataloader)\n","    \n","\n","def train(NN_model, train_dataloader, val_dataloader, params):\n","    # model, optimizer\n","    device = torch.device(\"cpu\")\n","    model = NN_model(params).to(device)\n","    optimizer = optim.Adam(model.parameters(),lr=params['lr'])\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['T_max'], eta_min=4e-08)\n","    # train\n","    for epoch in range(1, params['epoch']+1):\n","        start_time = time.time()\n","        train_loss = 0.0\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(train_dataloader):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = params['criterion'](output, target)\n","            loss.backward()\n","            if params['clip']:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(),params['clip'])\n","            train_loss += loss.item()\n","            optimizer.step()\n","            scheduler.step()\n","            \n","            if batch_idx % params['log_interval'] == 0:\n","                print('Train : [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                    batch_idx * len(data), len(train_dataloader.dataset),\n","                    100. * batch_idx / len(train_dataloader), loss.item()))\n","        \n","        train_loss /= len(train_dataloader)\n","        val_loss = test(model, val_dataloader, params)\n","        print(\"| Epoch {} | running {:.2f} seconds | train Loss {:.6f} | val loss {:.6f} |\".format(\n","            epoch, time.time()-start_time, train_loss, val_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:47:23.636576Z","iopub.status.busy":"2022-03-31T00:47:23.636325Z","iopub.status.idle":"2022-03-31T00:47:23.64199Z","shell.execute_reply":"2022-03-31T00:47:23.641318Z","shell.execute_reply.started":"2022-03-31T00:47:23.636548Z"},"trusted":true},"outputs":[],"source":["train_period_length = 5 * 12 * 5\n","test_period_length = 5 * 5\n","# train_period_length = 120\n","# test_period_length = 24\n","n_splits = 1\n","lookahead = 1\n","\n","params = {\"lr\": 5e-4, 'n_in': 300, 'batch_size': 256, 'n_hidden1': 525, 'n_hidden2': 210, \n","          'n_hidden3': 128, 'n_hidden4': 64, 'epoch': 2, 'clip': None, 'log_interval': 1000, \n","          'T_max': 10, 'act_f': nn.ReLU, 'criterion': nn.MSELoss(reduction='mean')}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:47:24.88309Z","iopub.status.busy":"2022-03-31T00:47:24.882519Z","iopub.status.idle":"2022-03-31T00:47:24.88777Z","shell.execute_reply":"2022-03-31T00:47:24.886675Z","shell.execute_reply.started":"2022-03-31T00:47:24.883054Z"},"trusted":true},"outputs":[],"source":["cv = MultipleTimeSeriesCV(n_splits=n_splits,\n","                          train_period_length=train_period_length,\n","                          test_period_length=test_period_length,\n","                          lookahead=lookahead,\n","                          date_idx='time_id')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T00:47:26.152545Z","iopub.status.busy":"2022-03-31T00:47:26.151862Z","iopub.status.idle":"2022-03-31T00:50:13.12098Z","shell.execute_reply":"2022-03-31T00:50:13.119898Z","shell.execute_reply.started":"2022-03-31T00:47:26.152511Z"},"trusted":true},"outputs":[],"source":["models = []\n","\n","for fold, (train_idx, test_idx) in tqdm(enumerate(cv.split(features))):\n","    print(f'fold: {fold}')\n","    # train-val split\n","    train_set, val_set = get_train_valid_data(features, target, train_idx, test_idx)\n","    train_loader = DataLoader(train_set, batch_size=params['batch_size'], shuffle=True, drop_last=True, num_workers=4)\n","    val_loader = DataLoader(val_set, batch_size=params['batch_size'], shuffle=True, drop_last=False, num_workers=4)\n","    # train\n","    train(nn_regressor, train_loader, val_loader, params)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T05:12:32.018064Z","iopub.status.busy":"2022-03-02T05:12:32.017762Z","iopub.status.idle":"2022-03-02T05:12:32.048957Z","shell.execute_reply":"2022-03-02T05:12:32.048159Z","shell.execute_reply.started":"2022-03-02T05:12:32.018034Z"},"trusted":true},"outputs":[],"source":["import ubiquant\n","env = ubiquant.make_env()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T05:12:55.309247Z","iopub.status.busy":"2022-03-02T05:12:55.308859Z","iopub.status.idle":"2022-03-02T05:12:56.544602Z","shell.execute_reply":"2022-03-02T05:12:56.543487Z","shell.execute_reply.started":"2022-03-02T05:12:55.309186Z"},"trusted":true},"outputs":[],"source":["iter_test = env.iter_test() \n","# features_col = features.columns\n","for (test_df, sample_prediction_df) in iter_test:\n","    features = test_df.filter(like='f')\n","    \n","    y_preds = []\n","    for model in models:\n","        y_pred = model.predict((features.iloc[:,selected_features], features))\n","        y_preds.append(y_pred)\n","    sample_prediction_df['target'] = np.mean(y_preds, axis=0)\n","    env.predict(sample_prediction_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
