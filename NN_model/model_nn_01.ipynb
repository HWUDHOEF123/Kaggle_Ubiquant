{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_selection, metrics, model_selection, decomposition\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import time\n",
    "import random\n",
    "from itertools import product\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = Path(r\"..\\\\..\\\\Data\\\\Input\")\n",
    "\n",
    "feature_directory = Path(r\"..\\\\..\\\\Data\\\\Feature\")\n",
    "\n",
    "model_name = \"model_nn_01\"\n",
    "model_directory = Path()/model_name\n",
    "model_directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dic, save_path):\n",
    "    with open(save_path, 'wb') as f:\n",
    "    # with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(dic, f)\n",
    "\n",
    "def load_pickle(load_path):\n",
    "    with open(load_path, 'rb') as f:\n",
    "    # with gzip.open(load_path, 'rb') as f:\n",
    "        message_dict = pickle.load(f)\n",
    "    return message_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = pd.read_parquet(input_directory/'train_low_mem.parquet', engine='pyarrow').set_index(['time_id','investment_id'])\n",
    "# df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = df_data.drop('row_id', axis=1)\n",
    "# df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_norm = df_data.copy()\n",
    "\n",
    "# for i in tqdm(range(300)):\n",
    "#     feature = f'f_{i}'\n",
    "\n",
    "#     df_data_norm[feature] = df_data[feature].groupby(level='time_id').apply(\n",
    "#         lambda x: pd.DataFrame(preprocessing.RobustScaler(quantile_range=(1., 99.), with_scaling=True, with_centering=True).fit_transform(x.values.reshape(-1, 1)), index=x.index, columns=[f'f_{i}']))\n",
    "\n",
    "\n",
    "# df_data_norm.to_parquet(input_directory/'train_norm2.parquet')\n",
    "df_data_norm = pd.read_parquet(input_directory/'train_norm2.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'histgram of normalized feature'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(18, 4))\n",
    "# ax = plt.subplot(121)\n",
    "# df_data['f_1'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of raw feature\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "df_data_norm['f_1'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of normalized feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Feature & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_id  investment_id\n",
       "0        1               -0.300875\n",
       "         2               -0.231040\n",
       "         6                0.568807\n",
       "         7               -1.064780\n",
       "         8               -0.531940\n",
       "                            ...   \n",
       "1219     3768             0.033600\n",
       "         3769            -0.223264\n",
       "         3770            -0.559415\n",
       "         3772             0.009599\n",
       "         3773             1.212112\n",
       "Name: target, Length: 3141410, dtype: float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df_data_norm.pop('target')\n",
    "feature = df_data_norm\n",
    "# target = target.reset_index(drop=True)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.133483</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>-0.107040</td>\n",
       "      <td>0.232203</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>-0.084325</td>\n",
       "      <td>0.295687</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.565489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.097949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049540</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.239130</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>0.119334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108289</td>\n",
       "      <td>-0.102564</td>\n",
       "      <td>0.160293</td>\n",
       "      <td>-0.011403</td>\n",
       "      <td>0.029314</td>\n",
       "      <td>0.390865</td>\n",
       "      <td>0.426703</td>\n",
       "      <td>0.295676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.076149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.097370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.209301</td>\n",
       "      <td>-0.285416</td>\n",
       "      <td>-0.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.119521</td>\n",
       "      <td>-0.009270</td>\n",
       "      <td>0.091237</td>\n",
       "      <td>-0.229989</td>\n",
       "      <td>0.299662</td>\n",
       "      <td>-0.347619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.298825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.059196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.127263</td>\n",
       "      <td>-0.063854</td>\n",
       "      <td>0.104602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.544325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424744</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.063067</td>\n",
       "      <td>-0.171823</td>\n",
       "      <td>0.242565</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.337304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.080431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.171502</td>\n",
       "      <td>0.262178</td>\n",
       "      <td>-0.087566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.114755</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>0.531115</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.070495</td>\n",
       "      <td>-0.155961</td>\n",
       "      <td>-0.246051</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.098786</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.075289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226062</td>\n",
       "      <td>0.312121</td>\n",
       "      <td>-0.080823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1219</th>\n",
       "      <th>3768</th>\n",
       "      <td>-0.047359</td>\n",
       "      <td>-0.148677</td>\n",
       "      <td>-0.083344</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.017038</td>\n",
       "      <td>-0.092924</td>\n",
       "      <td>0.378928</td>\n",
       "      <td>0.318737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.387626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080894</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.105754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063816</td>\n",
       "      <td>-0.096651</td>\n",
       "      <td>-0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>-0.374504</td>\n",
       "      <td>-0.029735</td>\n",
       "      <td>-0.035861</td>\n",
       "      <td>-0.006363</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.029534</td>\n",
       "      <td>-0.402315</td>\n",
       "      <td>0.339488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.329040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.107742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.107359</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.039689</td>\n",
       "      <td>-0.294157</td>\n",
       "      <td>-0.070620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>0.154132</td>\n",
       "      <td>-0.237883</td>\n",
       "      <td>0.186720</td>\n",
       "      <td>-0.009407</td>\n",
       "      <td>-0.012827</td>\n",
       "      <td>0.291013</td>\n",
       "      <td>0.030878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.191191</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015572</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.022208</td>\n",
       "      <td>-0.489768</td>\n",
       "      <td>-0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>-0.652056</td>\n",
       "      <td>0.089206</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.435851</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>-0.146850</td>\n",
       "      <td>0.091365</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.302704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.053337</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018566</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.147317</td>\n",
       "      <td>0.130867</td>\n",
       "      <td>-0.020435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>-0.088998</td>\n",
       "      <td>0.059471</td>\n",
       "      <td>-0.123831</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.122948</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>0.257889</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.524376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032652</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.069543</td>\n",
       "      <td>0.148146</td>\n",
       "      <td>0.170569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141410 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            f_0       f_1       f_2       f_3       f_4  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.133483  0.025641 -0.107040  0.232203  0.027035   \n",
       "        2              0.108289 -0.102564  0.160293 -0.011403  0.029314   \n",
       "        6              0.022050  0.128205  0.119521 -0.009270  0.091237   \n",
       "        7             -0.544325  0.000000  0.424744 -0.008875 -0.063067   \n",
       "        8              0.114755 -0.051282  0.531115 -0.003263 -0.070495   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "1219    3768          -0.047359 -0.148677 -0.083344 -0.002532 -0.017038   \n",
       "        3769          -0.374504 -0.029735 -0.035861 -0.006363 -0.048209   \n",
       "        3770           0.154132 -0.237883  0.186720 -0.009407 -0.012827   \n",
       "        3772          -0.652056  0.089206  0.000941  0.435851 -0.010454   \n",
       "        3773          -0.088998  0.059471 -0.123831  0.139659  0.122948   \n",
       "\n",
       "                            f_5       f_6       f_7  f_8       f_9  ...  \\\n",
       "time_id investment_id                                               ...   \n",
       "0       1             -0.084325  0.295687  0.323107  0.0 -0.565489  ...   \n",
       "        2              0.390865  0.426703  0.295676  0.0 -0.076149  ...   \n",
       "        6             -0.229989  0.299662 -0.347619  0.0 -0.298825  ...   \n",
       "        7             -0.171823  0.242565  0.057936  0.0 -0.337304  ...   \n",
       "        8             -0.155961 -0.246051  0.323107  0.0 -0.017943  ...   \n",
       "...                         ...       ...       ...  ...       ...  ...   \n",
       "1219    3768          -0.092924  0.378928  0.318737  0.0 -0.387626  ...   \n",
       "        3769          -0.029534 -0.402315  0.339488  0.0 -0.329040  ...   \n",
       "        3770           0.291013  0.030878  0.000000  0.0  0.253357  ...   \n",
       "        3772          -0.146850  0.091365 -0.286822  0.0 -0.302704  ...   \n",
       "        3773           0.096628  0.257889 -0.286822  0.0 -0.524376  ...   \n",
       "\n",
       "                          f_290  f_291     f_292  f_293  f_294     f_295  \\\n",
       "time_id investment_id                                                      \n",
       "0       1              0.102298   -1.0  0.097949    0.0    0.0  0.049540   \n",
       "        2             -0.003282    0.0 -0.097370    0.0    0.0 -0.025318   \n",
       "        6              0.000000    0.0 -0.059196   -1.0   -1.0  0.016613   \n",
       "        7              0.105580    0.0  0.000476   -1.0    0.0 -0.080431   \n",
       "        8             -0.006564    0.0 -0.098786   -1.0    0.0 -0.075289   \n",
       "...                         ...    ...       ...    ...    ...       ...   \n",
       "1219    3768          -0.080894   -1.0 -0.105754    0.0    0.0 -0.031788   \n",
       "        3769           0.008933   -1.0 -0.107742    0.0    0.0 -0.107359   \n",
       "        3770          -0.170720   -1.0  0.191191   -1.0    0.0 -0.015572   \n",
       "        3772          -0.170720   -1.0  0.053337   -1.0    0.0 -0.018566   \n",
       "        3773          -0.086849    0.0  0.682466    0.0    0.0  0.032652   \n",
       "\n",
       "                       f_296     f_297     f_298     f_299  \n",
       "time_id investment_id                                       \n",
       "0       1               -0.5 -0.239130 -0.108588  0.119334  \n",
       "        2               -0.5 -0.209301 -0.285416 -0.006368  \n",
       "        6               -0.5 -0.127263 -0.063854  0.104602  \n",
       "        7                0.0 -0.171502  0.262178 -0.087566  \n",
       "        8                0.0  0.226062  0.312121 -0.080823  \n",
       "...                      ...       ...       ...       ...  \n",
       "1219    3768             0.0 -0.063816 -0.096651 -0.068704  \n",
       "        3769            -0.5  0.039689 -0.294157 -0.070620  \n",
       "        3770             0.5  0.022208 -0.489768 -0.003278  \n",
       "        3772            -0.5 -0.147317  0.130867 -0.020435  \n",
       "        3773             0.5 -0.069543  0.148146  0.170569  \n",
       "\n",
       "[3141410 rows x 300 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature = feature.reset_index(drop=True)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 0, train_end: 1018, test_start: 1019, test_end: 1219\n"
     ]
    }
   ],
   "source": [
    "train_test_ratio = 5\n",
    "\n",
    "uniquedate = target.index.get_level_values(level='time_id').unique().tolist()\n",
    "train_start = 0\n",
    "train_end = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))]\n",
    "test_start = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))+1]\n",
    "test_end = 1219\n",
    "\n",
    "\n",
    "# dates of train, dates of test\n",
    "print(\n",
    "    f'train_start: {train_start}, train_end: {train_end}, test_start: {test_start}, test_end: {test_end}')\n",
    "\n",
    "feature_train = feature.loc[idx[train_start:train_end, :], :]\n",
    "feature_test = feature.loc[idx[test_start:test_end, :], :]\n",
    "\n",
    "target_train = target.loc[idx[train_start:train_end, :]]\n",
    "target_test = target.loc[idx[test_start:test_end, :]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UbiquantDataset(Dataset):\n",
    "    def __init__(self, feature, target):\n",
    "        self.feature = torch.tensor(feature.values)\n",
    "        self.target = torch.tensor(target.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # feature = torch.tensor(self.feature.iloc[idx, :])\n",
    "        # target = torch.tensor(self.target.iloc[idx])\n",
    "        return self.feature[idx, :], self.target[idx]\n",
    "\n",
    "def get_train_valid_data(X, y, train_idx, valid_idx):\n",
    "    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    x_val, y_val = X.iloc[valid_idx, :], y.iloc[valid_idx]\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_val = scaler.transform(x_val)xx\n",
    "    return (x_train, y_train,\n",
    "            x_val, y_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (dense_block): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.n_in = param['n_in']\n",
    "        self.dense_block = self.build_dense_block(param) \n",
    "\n",
    "    def build_dense_block(self, param):\n",
    "        dense_block = []\n",
    "\n",
    "        #layer1\n",
    "        dense_block += [nn.Linear(param['n_in'], param['n_hidden1']), \n",
    "                        param['act_f'](),\n",
    "                        nn.BatchNorm1d(num_features=param['n_hidden1'])\n",
    "                        ]\n",
    "        #layer2\n",
    "        dense_block += [nn.Linear(param['n_hidden1'], param['n_hidden2']), \n",
    "                        param['act_f'](),\n",
    "                        nn.BatchNorm1d(num_features=param['n_hidden2'])\n",
    "                        ]\n",
    "        \n",
    "        #layer3\n",
    "        dense_block += [nn.Linear(param['n_hidden2'], param['n_hidden3']), \n",
    "                        param['act_f'](),\n",
    "                        nn.BatchNorm1d(num_features=param['n_hidden3'])\n",
    "                        ]\n",
    "        \n",
    "        dense_block += [nn.Linear(param['n_hidden3'], 1)]\n",
    "\n",
    "        return nn.Sequential(*dense_block)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1, self.n_in)\n",
    "        out = self.dense_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNetwork({'n_in': 300, 'n_hidden1': 256, 'n_hidden2': 128,\n",
    "                       'n_hidden3': 64, 'act_f': nn.ReLU})\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_corr(y_true, y_pred):\n",
    "    y_pred = y_pred.unsqueeze(1)\n",
    "    return torch.corrcoef(torch.cat((y_true, y_pred), 1).T)[0][1]\n",
    "    # return np.corrcoef(y_true.numpy().reshape(1, -1), y_pred.numpy().reshape(1, -1))[0][1]\n",
    "\n",
    "\n",
    "def corr(y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0][1]\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, params):\n",
    "    device = torch.device(params[\"device\"])\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss, test_corr = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(test_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = params['criterion'](pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            corr = params['custom_corr'](pred, y)\n",
    "            test_corr += corr\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_corr /= num_batches\n",
    "    return test_loss, test_corr\n",
    "\n",
    "\n",
    "def train(NN_model, train_dataloader, val_dataloader, params):\n",
    "    # model, optimizer\n",
    "    device = torch.device(params[\"device\"])\n",
    "    model = NN_model(params).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=params['T_max'], eta_min=4e-08)\n",
    "    # train\n",
    "    for epoch in range(1, params['epoch']+1):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        train_corr = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = params['criterion'](output, target)\n",
    "            loss.backward()\n",
    "            if params['clip']:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), params['clip'])\n",
    "            train_loss += loss.item()\n",
    "            train_corr += params['custom_corr'](\n",
    "                model(data).detach(), target.detach())\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # if batch_idx % params['log_interval'] == 0:\n",
    "            #     print('Train : [{}/{} ({:.0f}%)]\\tLoss: {:.5f}'.format(\n",
    "            #         batch_idx * len(data), len(train_dataloader.dataset),\n",
    "            #         100. * batch_idx / len(train_dataloader), loss.item()))\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_corr /= len(train_dataloader)\n",
    "        val_loss, val_corr = test(model, val_dataloader, params)\n",
    "        print(\"| Epoch {}/{} | running {:.2f} seconds | train loss {:.5f} | train corr {:.5f} | val loss {:.5f} | val corr {:.5f} |\".format(\n",
    "            epoch, params['epoch'], time.time()-start_time, train_loss, train_corr, val_loss, val_corr))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lr\": 1e-3, 'batch_size': 128, 'n_splits': 2,\n",
    "          'n_in': 300,\n",
    "          'n_hidden1': 128, 'n_hidden2': 64, 'n_hidden3': 32, 'n_hidden4': 16,\n",
    "          'epoch': 4, 'clip': False, 'log_interval': 500,\n",
    "          'T_max': 10,\n",
    "          'act_f': nn.SiLU, \n",
    "          'criterion': nn.HuberLoss(),\n",
    "          'custom_corr': custom_corr,\n",
    "          \"device\": 'cuda'\n",
    "          }\n",
    "\n",
    "\n",
    "cv = model_selection.KFold(\n",
    "    n_splits=params['n_splits'], random_state=2021, shuffle=True)\n",
    "\n",
    "\n",
    "# loss_fn = nn.HuberLoss() nn.MSELoss(reduction='mean')\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "fold: 0, memory: 86.10%\n",
      "| Epoch 1/4 | running 81.08 seconds | train loss 0.31624 | train corr -0.00127 | val loss 0.32623 | val corr -0.02126 |\n",
      "| Epoch 2/4 | running 78.16 seconds | train loss 0.31601 | train corr -0.00063 | val loss 0.31667 | val corr -0.01781 |\n",
      "| Epoch 3/4 | running 82.56 seconds | train loss 0.31600 | train corr 0.00164 | val loss 0.31633 | val corr -0.03627 |\n",
      "| Epoch 4/4 | running 80.70 seconds | train loss 0.31600 | train corr -0.00037 | val loss 0.31739 | val corr -0.04262 |\n",
      "##############################################################\n",
      "fold: 1, memory: 79.20%\n",
      "| Epoch 1/4 | running 80.96 seconds | train loss 0.31648 | train corr 0.00021 | val loss 0.31807 | val corr -0.02196 |\n",
      "| Epoch 2/4 | running 83.74 seconds | train loss 0.31614 | train corr -0.00094 | val loss 0.31614 | val corr 0.02266 |\n",
      "| Epoch 3/4 | running 81.93 seconds | train loss 0.31613 | train corr -0.00101 | val loss 0.31856 | val corr 0.03638 |\n",
      "| Epoch 4/4 | running 82.83 seconds | train loss 0.31613 | train corr -0.00104 | val loss 0.31605 | val corr 0.01215 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X=feature_train)):\n",
    "    print(\"##############################################################\")\n",
    "    print(f'fold: {fold}, memory: {psutil.virtual_memory().percent/100:.2%}')\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = get_train_valid_data(\n",
    "        feature_train, target_train, train_idx, valid_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(UbiquantDataset(\n",
    "        X_train, y_train), batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(UbiquantDataset(\n",
    "        X_valid, y_valid), batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "    model = train(NeuralNetwork, train_dataloader, valid_dataloader, params)\n",
    "    torch.save(model.state_dict(), model_directory/f\"model_{fold}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_list = []\n",
    "\n",
    "\n",
    "for fold in range(params['n_splits']):\n",
    "    model = NeuralNetwork(params)\n",
    "    model.load_state_dict(torch.load(model_directory/f\"model_{fold}.pth\"))\n",
    "    model.eval()\n",
    "    best_models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test | mse_test: 0.86153 | corr_test: 0.0013953\n"
     ]
    }
   ],
   "source": [
    "y_test_fit_list = []\n",
    "\n",
    "test_dataloader = DataLoader(UbiquantDataset(\n",
    "    feature_test, target_test), batch_size=len(feature_test), shuffle=False, drop_last=False)\n",
    "X, y = next(iter(test_dataloader))\n",
    "\n",
    "for (i, model) in enumerate(best_models_list):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_fit = model(X)\n",
    "        y_test_fit_list.append(y_test_fit.numpy())\n",
    "\n",
    "y_test_fit = pd.DataFrame(index=feature_test.index)\n",
    "y_test_fit['actual'] = target_test\n",
    "y_test_fit['predict'] = np.mean(y_test_fit_list, axis=0).squeeze()\n",
    "\n",
    "corr_train = corr(y_test_fit['actual'], y_test_fit['predict'])\n",
    "corr_test = corr(y_test_fit['actual'], y_test_fit['predict'])\n",
    "mse_train = metrics.mean_squared_error(\n",
    "    y_test_fit['actual'], y_test_fit['predict'])\n",
    "mse_test = metrics.mean_squared_error(\n",
    "    y_test_fit['actual'], y_test_fit['predict'])\n",
    "print(\n",
    "    f' test | mse_test: {mse_train:.5f} | corr_test: {corr_train:0.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06e2c26d81da9559881f4d926ec79778e2b8c97ac068d329245981963e3d233b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
