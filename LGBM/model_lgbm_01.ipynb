{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_selection, metrics, model_selection\n",
    "\n",
    "import random\n",
    "from itertools import product\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "idx = pd.IndexSlice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = Path(r\"..\\\\..\\\\Data\\\\Input\")\n",
    "\n",
    "feature_directory = Path(r\"..\\\\..\\\\Data\\\\Feature\")\n",
    "\n",
    "model_name = \"model_lgbm_01\"\n",
    "model_directory = Path()/model_name\n",
    "model_directory.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 3141410 entries, (0, 1) to (1219, 3773)\n",
      "Columns: 302 entries, row_id to f_299\n",
      "dtypes: float32(301), object(1)\n",
      "memory usage: 3.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_parquet(input_directory/'train_low_mem.parquet', engine='pyarrow').set_index(['time_id','investment_id'])\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>-0.300875</td>\n",
       "      <td>0.932573</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>-0.402206</td>\n",
       "      <td>0.378386</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>-0.413469</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366028</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>0.200075</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.086764</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-1.044826</td>\n",
       "      <td>-0.287605</td>\n",
       "      <td>0.321566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.231040</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154193</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>0.615937</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>-0.607963</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>-1.083155</td>\n",
       "      <td>0.979656</td>\n",
       "      <td>-1.125681</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.551904</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.219097</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.612428</td>\n",
       "      <td>-0.113944</td>\n",
       "      <td>0.243608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.064780</td>\n",
       "      <td>-2.343535</td>\n",
       "      <td>-0.011870</td>\n",
       "      <td>1.874606</td>\n",
       "      <td>-0.606346</td>\n",
       "      <td>-0.586827</td>\n",
       "      <td>-0.815737</td>\n",
       "      <td>0.778096</td>\n",
       "      <td>0.298990</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382201</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.266359</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.609113</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>-0.783423</td>\n",
       "      <td>1.151730</td>\n",
       "      <td>-0.773309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.531940</td>\n",
       "      <td>0.842057</td>\n",
       "      <td>-0.262993</td>\n",
       "      <td>2.330030</td>\n",
       "      <td>-0.583422</td>\n",
       "      <td>-0.618392</td>\n",
       "      <td>-0.742814</td>\n",
       "      <td>-0.946789</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170365</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.741355</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.588445</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>0.753279</td>\n",
       "      <td>1.345611</td>\n",
       "      <td>-0.737624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         target       f_0       f_1       f_2       f_3  \\\n",
       "time_id investment_id                                                     \n",
       "0       1             -0.300875  0.932573  0.113691 -0.402206  0.378386   \n",
       "        2             -0.231040  0.810802 -0.514115  0.742368 -0.616673   \n",
       "        6              0.568807  0.393974  0.615937  0.567806 -0.607963   \n",
       "        7             -1.064780 -2.343535 -0.011870  1.874606 -0.606346   \n",
       "        8             -0.531940  0.842057 -0.262993  2.330030 -0.583422   \n",
       "\n",
       "                            f_4       f_5       f_6       f_7       f_8  ...  \\\n",
       "time_id investment_id                                                    ...   \n",
       "0       1             -0.203938 -0.413469  0.965623  1.230508  0.114809  ...   \n",
       "        2             -0.194255  1.771210  1.428127  1.134144  0.114809  ...   \n",
       "        6              0.068883 -1.083155  0.979656 -1.125681  0.114809  ...   \n",
       "        7             -0.586827 -0.815737  0.778096  0.298990  0.114809  ...   \n",
       "        8             -0.618392 -0.742814 -0.946789  1.230508  0.114809  ...   \n",
       "\n",
       "                          f_290     f_291     f_292     f_293     f_294  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.366028 -1.095620  0.200075  0.819155  0.941183   \n",
       "        2             -0.154193  0.912726 -0.734579  0.819155  0.941183   \n",
       "        6             -0.138020  0.912726 -0.551904 -1.220772 -1.060166   \n",
       "        7              0.382201  0.912726 -0.266359 -1.220772  0.941183   \n",
       "        8             -0.170365  0.912726 -0.741355 -1.220772  0.941183   \n",
       "\n",
       "                          f_295     f_296     f_297     f_298     f_299  \n",
       "time_id investment_id                                                    \n",
       "0       1             -0.086764 -1.087009 -1.044826 -0.287605  0.321566  \n",
       "        2             -0.387617 -1.087009 -0.929529 -0.974060 -0.343624  \n",
       "        6             -0.219097 -1.087009 -0.612428 -0.113944  0.243608  \n",
       "        7             -0.609113  0.104928 -0.783423  1.151730 -0.773309  \n",
       "        8             -0.588445  0.104928  0.753279  1.345611 -0.737624  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = df_data.drop('row_id', axis=1)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:41<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "df_data_norm = df_data.copy()\n",
    "\n",
    "normalization_dict = {}\n",
    "for i in tqdm(range(300)):\n",
    "    feature = f'f_{i}'\n",
    "\n",
    "    normalization = preprocessing.QuantileTransformer(\n",
    "        output_distribution='normal', random_state=0).fit(df_data[feature].values.reshape(-1, 1))\n",
    "    normalization_dict[feature] = normalization\n",
    "\n",
    "    df_data_norm[feature] = normalization.transform(\n",
    "        df_data[feature].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "f = open(model_directory/'pickle_normalization_dict', 'wb')\n",
    "pickle.dump(normalization_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_data_norm['f_0'].plot(kind='hist', bins=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:10<00:00, 28.86it/s]\n"
     ]
    }
   ],
   "source": [
    "df_feature_selection = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(300)):\n",
    "    feature = f'f_{i}'\n",
    "    df_feature_selection.loc[feature, 'corr'] = np.corrcoef(\n",
    "        df_data_norm['target'], df_data_norm[feature])[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_data_norm)\n",
    "rand_k_index = random.sample(range(n), n//50)\n",
    "rand_k_index.sort()\n",
    "\n",
    "mi = feature_selection.mutual_info_regression(\n",
    "    df_data_norm.iloc[rand_k_index, :].filter(like='f_'), df_data_norm.iloc[rand_k_index, :]['target'])\n",
    "df_feature_selection['mutual_information'] = mi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seleced feature by corr:  62\n",
      "seleced feature by mutual_information:  30\n",
      "seleced feature in final:  80\n"
     ]
    }
   ],
   "source": [
    "selected_feature1 = df_feature_selection[np.abs(\n",
    "    df_feature_selection['corr']) > 0.03].index\n",
    "selected_feature2 = df_feature_selection[(\n",
    "    df_feature_selection['mutual_information']) > 0.03].index\n",
    "print(\"seleced feature by corr: \", len(selected_feature1))\n",
    "print(\"seleced feature by mutual_information: \", len(selected_feature2))\n",
    "selected_feature = list(set(selected_feature1).union(set(selected_feature2)))\n",
    "print(\"seleced feature in final: \", len(selected_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(model_directory/'pickle_selected_feature', 'wb')\n",
    "pickle.dump(selected_feature, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Feature & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_id  investment_id\n",
       "0        1               -0.300875\n",
       "         2               -0.231040\n",
       "         6                0.568807\n",
       "         7               -1.064780\n",
       "         8               -0.531940\n",
       "                            ...   \n",
       "1219     3768             0.033600\n",
       "         3769            -0.223264\n",
       "         3770            -0.559415\n",
       "         3772             0.009599\n",
       "         3773             1.212112\n",
       "Name: target, Length: 3141410, dtype: float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = df_data_norm[selected_feature]\n",
    "target = df_data_norm['target']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f_193</th>\n",
       "      <th>f_181</th>\n",
       "      <th>f_232</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_172</th>\n",
       "      <th>f_100</th>\n",
       "      <th>f_159</th>\n",
       "      <th>f_65</th>\n",
       "      <th>f_106</th>\n",
       "      <th>f_207</th>\n",
       "      <th>...</th>\n",
       "      <th>f_203</th>\n",
       "      <th>f_252</th>\n",
       "      <th>f_41</th>\n",
       "      <th>f_93</th>\n",
       "      <th>f_137</th>\n",
       "      <th>f_51</th>\n",
       "      <th>f_158</th>\n",
       "      <th>f_237</th>\n",
       "      <th>f_83</th>\n",
       "      <th>f_127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.639075</td>\n",
       "      <td>-0.735823</td>\n",
       "      <td>1.118050</td>\n",
       "      <td>0.085528</td>\n",
       "      <td>0.906622</td>\n",
       "      <td>0.735255</td>\n",
       "      <td>0.652470</td>\n",
       "      <td>0.728373</td>\n",
       "      <td>0.115644</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>...</td>\n",
       "      <td>1.092563</td>\n",
       "      <td>1.128910</td>\n",
       "      <td>-0.383024</td>\n",
       "      <td>-0.214948</td>\n",
       "      <td>1.335818</td>\n",
       "      <td>0.245119</td>\n",
       "      <td>0.996062</td>\n",
       "      <td>-0.757689</td>\n",
       "      <td>1.057623</td>\n",
       "      <td>-0.383967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.930463</td>\n",
       "      <td>0.353929</td>\n",
       "      <td>0.227949</td>\n",
       "      <td>0.647818</td>\n",
       "      <td>0.945011</td>\n",
       "      <td>-0.980021</td>\n",
       "      <td>1.246285</td>\n",
       "      <td>-0.100110</td>\n",
       "      <td>-1.113229</td>\n",
       "      <td>0.051849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727053</td>\n",
       "      <td>0.106818</td>\n",
       "      <td>0.486480</td>\n",
       "      <td>0.889017</td>\n",
       "      <td>-1.191156</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>0.968755</td>\n",
       "      <td>-0.584711</td>\n",
       "      <td>-0.570547</td>\n",
       "      <td>-0.891040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.166376</td>\n",
       "      <td>-0.097530</td>\n",
       "      <td>-0.705530</td>\n",
       "      <td>-0.183767</td>\n",
       "      <td>-0.487557</td>\n",
       "      <td>1.005527</td>\n",
       "      <td>-0.103272</td>\n",
       "      <td>0.446216</td>\n",
       "      <td>0.160339</td>\n",
       "      <td>0.264848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812888</td>\n",
       "      <td>0.202081</td>\n",
       "      <td>-0.383024</td>\n",
       "      <td>-0.775608</td>\n",
       "      <td>0.681474</td>\n",
       "      <td>-0.081934</td>\n",
       "      <td>0.273517</td>\n",
       "      <td>-1.040022</td>\n",
       "      <td>0.030549</td>\n",
       "      <td>-0.316686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.120225</td>\n",
       "      <td>-1.528151</td>\n",
       "      <td>0.466435</td>\n",
       "      <td>-1.310882</td>\n",
       "      <td>-1.013628</td>\n",
       "      <td>-0.980021</td>\n",
       "      <td>-0.352753</td>\n",
       "      <td>0.570173</td>\n",
       "      <td>-1.504939</td>\n",
       "      <td>-1.597029</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.498077</td>\n",
       "      <td>-1.096637</td>\n",
       "      <td>-0.383024</td>\n",
       "      <td>0.564596</td>\n",
       "      <td>-1.162210</td>\n",
       "      <td>-0.371670</td>\n",
       "      <td>-0.758951</td>\n",
       "      <td>0.324927</td>\n",
       "      <td>0.728752</td>\n",
       "      <td>-1.431777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.262493</td>\n",
       "      <td>0.559995</td>\n",
       "      <td>0.093667</td>\n",
       "      <td>-0.594201</td>\n",
       "      <td>-0.872181</td>\n",
       "      <td>-0.980021</td>\n",
       "      <td>-1.362251</td>\n",
       "      <td>-1.395556</td>\n",
       "      <td>-2.394058</td>\n",
       "      <td>-1.307495</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.797494</td>\n",
       "      <td>-1.445431</td>\n",
       "      <td>-2.047396</td>\n",
       "      <td>0.690958</td>\n",
       "      <td>-0.684999</td>\n",
       "      <td>-0.928432</td>\n",
       "      <td>-1.837306</td>\n",
       "      <td>0.911607</td>\n",
       "      <td>1.195206</td>\n",
       "      <td>-0.136750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1219</th>\n",
       "      <th>3768</th>\n",
       "      <td>-0.370078</td>\n",
       "      <td>0.533908</td>\n",
       "      <td>-0.826903</td>\n",
       "      <td>-0.035808</td>\n",
       "      <td>-0.053930</td>\n",
       "      <td>-0.400635</td>\n",
       "      <td>-0.722744</td>\n",
       "      <td>-1.062437</td>\n",
       "      <td>-0.993829</td>\n",
       "      <td>-0.323678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.884996</td>\n",
       "      <td>-0.828514</td>\n",
       "      <td>-0.383024</td>\n",
       "      <td>0.252613</td>\n",
       "      <td>-0.458486</td>\n",
       "      <td>-0.148439</td>\n",
       "      <td>-0.778214</td>\n",
       "      <td>0.131032</td>\n",
       "      <td>0.324559</td>\n",
       "      <td>0.807970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>-0.955596</td>\n",
       "      <td>1.084681</td>\n",
       "      <td>-0.839842</td>\n",
       "      <td>-0.748431</td>\n",
       "      <td>-0.326347</td>\n",
       "      <td>-0.400635</td>\n",
       "      <td>-0.417822</td>\n",
       "      <td>-1.272297</td>\n",
       "      <td>0.735052</td>\n",
       "      <td>-0.512616</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.841771</td>\n",
       "      <td>-1.230674</td>\n",
       "      <td>-0.383024</td>\n",
       "      <td>0.284581</td>\n",
       "      <td>-0.488549</td>\n",
       "      <td>-0.821756</td>\n",
       "      <td>-1.510987</td>\n",
       "      <td>0.874547</td>\n",
       "      <td>1.577192</td>\n",
       "      <td>-0.604208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>0.125989</td>\n",
       "      <td>1.242220</td>\n",
       "      <td>-0.343938</td>\n",
       "      <td>-0.504301</td>\n",
       "      <td>0.053997</td>\n",
       "      <td>-0.400635</td>\n",
       "      <td>1.204961</td>\n",
       "      <td>1.168085</td>\n",
       "      <td>0.792703</td>\n",
       "      <td>0.515225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665096</td>\n",
       "      <td>1.074752</td>\n",
       "      <td>0.528548</td>\n",
       "      <td>0.218223</td>\n",
       "      <td>-0.492252</td>\n",
       "      <td>0.886925</td>\n",
       "      <td>-0.188015</td>\n",
       "      <td>0.495764</td>\n",
       "      <td>-1.065873</td>\n",
       "      <td>-1.663583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>-0.312623</td>\n",
       "      <td>-0.070828</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>1.177674</td>\n",
       "      <td>-0.360411</td>\n",
       "      <td>-0.400635</td>\n",
       "      <td>1.404446</td>\n",
       "      <td>-0.973981</td>\n",
       "      <td>-1.756942</td>\n",
       "      <td>-0.403521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159613</td>\n",
       "      <td>0.544791</td>\n",
       "      <td>-0.383024</td>\n",
       "      <td>0.310166</td>\n",
       "      <td>-0.410889</td>\n",
       "      <td>0.811605</td>\n",
       "      <td>-0.279343</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.390813</td>\n",
       "      <td>-2.423346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>1.475216</td>\n",
       "      <td>-0.222939</td>\n",
       "      <td>0.876177</td>\n",
       "      <td>1.612282</td>\n",
       "      <td>2.087166</td>\n",
       "      <td>0.690639</td>\n",
       "      <td>0.762170</td>\n",
       "      <td>1.270614</td>\n",
       "      <td>0.442923</td>\n",
       "      <td>0.994351</td>\n",
       "      <td>...</td>\n",
       "      <td>1.563242</td>\n",
       "      <td>1.281767</td>\n",
       "      <td>1.974495</td>\n",
       "      <td>-0.413186</td>\n",
       "      <td>0.360203</td>\n",
       "      <td>0.920803</td>\n",
       "      <td>1.295633</td>\n",
       "      <td>-1.128468</td>\n",
       "      <td>0.231411</td>\n",
       "      <td>1.268360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141410 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          f_193     f_181     f_232      f_28     f_172  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.639075 -0.735823  1.118050  0.085528  0.906622   \n",
       "        2              0.930463  0.353929  0.227949  0.647818  0.945011   \n",
       "        6             -0.166376 -0.097530 -0.705530 -0.183767 -0.487557   \n",
       "        7             -1.120225 -1.528151  0.466435 -1.310882 -1.013628   \n",
       "        8             -1.262493  0.559995  0.093667 -0.594201 -0.872181   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "1219    3768          -0.370078  0.533908 -0.826903 -0.035808 -0.053930   \n",
       "        3769          -0.955596  1.084681 -0.839842 -0.748431 -0.326347   \n",
       "        3770           0.125989  1.242220 -0.343938 -0.504301  0.053997   \n",
       "        3772          -0.312623 -0.070828  0.776263  1.177674 -0.360411   \n",
       "        3773           1.475216 -0.222939  0.876177  1.612282  2.087166   \n",
       "\n",
       "                          f_100     f_159      f_65     f_106     f_207  ...  \\\n",
       "time_id investment_id                                                    ...   \n",
       "0       1              0.735255  0.652470  0.728373  0.115644  0.005331  ...   \n",
       "        2             -0.980021  1.246285 -0.100110 -1.113229  0.051849  ...   \n",
       "        6              1.005527 -0.103272  0.446216  0.160339  0.264848  ...   \n",
       "        7             -0.980021 -0.352753  0.570173 -1.504939 -1.597029  ...   \n",
       "        8             -0.980021 -1.362251 -1.395556 -2.394058 -1.307495  ...   \n",
       "...                         ...       ...       ...       ...       ...  ...   \n",
       "1219    3768          -0.400635 -0.722744 -1.062437 -0.993829 -0.323678  ...   \n",
       "        3769          -0.400635 -0.417822 -1.272297  0.735052 -0.512616  ...   \n",
       "        3770          -0.400635  1.204961  1.168085  0.792703  0.515225  ...   \n",
       "        3772          -0.400635  1.404446 -0.973981 -1.756942 -0.403521  ...   \n",
       "        3773           0.690639  0.762170  1.270614  0.442923  0.994351  ...   \n",
       "\n",
       "                          f_203     f_252      f_41      f_93     f_137  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              1.092563  1.128910 -0.383024 -0.214948  1.335818   \n",
       "        2              0.727053  0.106818  0.486480  0.889017 -1.191156   \n",
       "        6              0.812888  0.202081 -0.383024 -0.775608  0.681474   \n",
       "        7             -1.498077 -1.096637 -0.383024  0.564596 -1.162210   \n",
       "        8             -1.797494 -1.445431 -2.047396  0.690958 -0.684999   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "1219    3768          -0.884996 -0.828514 -0.383024  0.252613 -0.458486   \n",
       "        3769          -1.841771 -1.230674 -0.383024  0.284581 -0.488549   \n",
       "        3770           0.665096  1.074752  0.528548  0.218223 -0.492252   \n",
       "        3772          -0.159613  0.544791 -0.383024  0.310166 -0.410889   \n",
       "        3773           1.563242  1.281767  1.974495 -0.413186  0.360203   \n",
       "\n",
       "                           f_51     f_158     f_237      f_83     f_127  \n",
       "time_id investment_id                                                    \n",
       "0       1              0.245119  0.996062 -0.757689  1.057623 -0.383967  \n",
       "        2              0.115538  0.968755 -0.584711 -0.570547 -0.891040  \n",
       "        6             -0.081934  0.273517 -1.040022  0.030549 -0.316686  \n",
       "        7             -0.371670 -0.758951  0.324927  0.728752 -1.431777  \n",
       "        8             -0.928432 -1.837306  0.911607  1.195206 -0.136750  \n",
       "...                         ...       ...       ...       ...       ...  \n",
       "1219    3768          -0.148439 -0.778214  0.131032  0.324559  0.807970  \n",
       "        3769          -0.821756 -1.510987  0.874547  1.577192 -0.604208  \n",
       "        3770           0.886925 -0.188015  0.495764 -1.065873 -1.663583  \n",
       "        3772           0.811605 -0.279343 -0.093023 -0.390813 -2.423346  \n",
       "        3773           0.920803  1.295633 -1.128468  0.231411  1.268360  \n",
       "\n",
       "[3141410 rows x 80 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 0, train_end: 1018, test_start: 1019, test_end: 1219\n"
     ]
    }
   ],
   "source": [
    "train_test_ratio = 5\n",
    "\n",
    "uniquedate = target.index.get_level_values(level='time_id').unique().tolist()\n",
    "train_start = 0\n",
    "train_end = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))]\n",
    "test_start = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))+1]\n",
    "test_end = 1219\n",
    "\n",
    "\n",
    "# dates of train, dates of test\n",
    "print(\n",
    "    f'train_start: {train_start}, train_end: {train_end}, test_start: {test_start}, test_end: {test_end}')\n",
    "\n",
    "feature_train = feature.loc[idx[train_start:train_end, :], :]\n",
    "feature_test = feature.loc[idx[test_start:test_end, :], :]\n",
    "\n",
    "target_train = target.loc[idx[train_start:train_end, :]]\n",
    "target_test = target.loc[idx[test_start:test_end, :]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval_mse_score(preds, lgbm_train):\n",
    "    labels = lgbm_train.get_label()\n",
    "    return 'mse', round(metrics.mean_squared_error(y_true=labels, y_pred=preds), 5), False\n",
    "\n",
    "\n",
    "def feval_mape_score(preds, lgbm_train):\n",
    "    labels = lgbm_train.get_label()\n",
    "    return 'mape', round(metrics.mean_absolute_percentage_error(y_true=labels, y_pred=preds), 5), False\n",
    "\n",
    "\n",
    "def feval_r2_score(preds, lgbm_train):\n",
    "    labels = lgbm_train.get_label()\n",
    "    return 'r2_score', round(metrics.r2_score(y_true=labels, y_pred=preds), 5), True\n",
    "\n",
    "\n",
    "def corr(y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0][1]\n",
    "\n",
    "\n",
    "def feval_corr_score(preds, lgbm_train):\n",
    "    labels = lgbm_train.get_label()\n",
    "    return 'corr_score', round(corr(y_true=labels, y_pred=preds), 5), True\n",
    "\n",
    "\n",
    "class MultipleTimeSeriesCV:\n",
    "    \"\"\"Generates tuples of train_idx, test_idx pairs\n",
    "    Assumes the MultiIndex contains levels 'symbol' and 'date'\n",
    "    purges overlapping outcomes\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_splits=3,\n",
    "                 train_period_length=126,\n",
    "                 test_period_length=21,\n",
    "                 lookahead=None,\n",
    "                 date_idx='date',\n",
    "                 shuffle=False):\n",
    "        self.n_splits = n_splits\n",
    "        self.lookahead = lookahead\n",
    "        self.test_length = test_period_length\n",
    "        self.train_length = train_period_length\n",
    "        self.shuffle = shuffle\n",
    "        self.date_idx = date_idx\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        unique_dates = X.index.get_level_values(self.date_idx).unique()\n",
    "        days = sorted(unique_dates, reverse=True)\n",
    "        split_idx = []\n",
    "        for i in range(self.n_splits):\n",
    "            test_end_idx = i * self.test_length\n",
    "            test_start_idx = test_end_idx + self.test_length\n",
    "            train_end_idx = test_start_idx + self.lookahead - 1\n",
    "            train_start_idx = min(\n",
    "                train_end_idx + self.train_length + self.lookahead - 1, len(days) - 1)\n",
    "            split_idx.append([train_start_idx, train_end_idx,\n",
    "                              test_start_idx, test_end_idx])\n",
    "\n",
    "        dates = X.reset_index()[[self.date_idx]]\n",
    "        for train_start, train_end, test_start, test_end in split_idx:\n",
    "            # print(train_start, train_end)\n",
    "            train_idx = dates[(dates[self.date_idx] > days[train_start])\n",
    "                              & (dates[self.date_idx] <= days[train_end])].index\n",
    "            test_idx = dates[(dates[self.date_idx] > days[test_start])\n",
    "                             & (dates[self.date_idx] <= days[test_end])].index\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(list(train_idx))\n",
    "            yield train_idx.to_numpy(), test_idx.to_numpy()\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "def get_train_valid_data(X, y, train_idx, valid_idx):\n",
    "    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    x_val, y_val = X.iloc[valid_idx, :], y.iloc[valid_idx]\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_val = scaler.transform(x_val)\n",
    "    return (x_train, y_train,\n",
    "            x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = ['objective', 'num_iterations', 'learning_rate', 'early_stopping_rounds', 'verbose',\n",
    "               'max_depth', 'num_leaves', 'lambda_l1', 'lambda_l2',\n",
    "               'feature_fraction',\n",
    "               'bagging_fraction',\n",
    "               'extra_trees']\n",
    "\n",
    "# For Better Accuracy\n",
    "objective = ['huber']\n",
    "num_iterations = [2000]\n",
    "learning_rate = [0.25]\n",
    "early_stopping_rounds = [25]\n",
    "verbose = [-1]\n",
    "# Deal with Over-fitting\n",
    "max_depth = [-1]\n",
    "num_leaves = [45]\n",
    "lambda_l1 = [0]\n",
    "lambda_l2 = [0]\n",
    "feature_fraction = [1]\n",
    "bagging_fraction = [1]\n",
    "extra_trees = [True]\n",
    "\n",
    "cv_params = list(product(objective,\n",
    "                         num_iterations,\n",
    "                         learning_rate,\n",
    "                         early_stopping_rounds,\n",
    "                         verbose,\n",
    "                         max_depth,\n",
    "                         num_leaves,\n",
    "                         lambda_l1,\n",
    "                         lambda_l2,\n",
    "                         feature_fraction,\n",
    "                         bagging_fraction,\n",
    "                         extra_trees\n",
    "                         ))\n",
    "\n",
    "n_splits = 5\n",
    "train_valid_ratio = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################################################################################################################\n",
      "Parmas:  {'objective': 'huber', 'num_iterations': 2000, 'learning_rate': 0.25, 'early_stopping_rounds': 25, 'verbose': -1, 'max_depth': -1, 'num_leaves': 45, 'lambda_l1': 0, 'lambda_l2': 0, 'feature_fraction': 1, 'bagging_fraction': 1, 'extra_trees': True} \n",
      "\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\ttraining's huber: 0.292824\ttraining's mse: 0.83365\ttraining's corr_score: 0.1636\tvalid_1's huber: 0.295916\tvalid_1's mse: 0.84464\tvalid_1's corr_score: 0.14466\n",
      "[200]\ttraining's huber: 0.290544\ttraining's mse: 0.82652\ttraining's corr_score: 0.1895\tvalid_1's huber: 0.295002\tvalid_1's mse: 0.84187\tvalid_1's corr_score: 0.15526\n",
      "[300]\ttraining's huber: 0.288676\ttraining's mse: 0.82068\ttraining's corr_score: 0.20935\tvalid_1's huber: 0.294496\tvalid_1's mse: 0.84042\tvalid_1's corr_score: 0.16047\n",
      "[400]\ttraining's huber: 0.286982\ttraining's mse: 0.81523\ttraining's corr_score: 0.22721\tvalid_1's huber: 0.294254\tvalid_1's mse: 0.83969\tvalid_1's corr_score: 0.163\n",
      "Early stopping, best iteration is:\n",
      "[417]\ttraining's huber: 0.286704\ttraining's mse: 0.81435\ttraining's corr_score: 0.22995\tvalid_1's huber: 0.294191\tvalid_1's mse: 0.8395\tvalid_1's corr_score: 0.16364\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "cv: 1 | mse_train: 0.81435 | corr_train: 0.22995 | mse_valid: 0.83950 | corr_valid: 0.16364\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\ttraining's huber: 0.293439\ttraining's mse: 0.83575\ttraining's corr_score: 0.16365\tvalid_1's huber: 0.293194\tvalid_1's mse: 0.83513\tvalid_1's corr_score: 0.14866\n",
      "[200]\ttraining's huber: 0.291227\ttraining's mse: 0.82883\ttraining's corr_score: 0.18888\tvalid_1's huber: 0.292377\tvalid_1's mse: 0.83273\tvalid_1's corr_score: 0.15771\n",
      "[300]\ttraining's huber: 0.289366\ttraining's mse: 0.82281\ttraining's corr_score: 0.20962\tvalid_1's huber: 0.291957\tvalid_1's mse: 0.8315\tvalid_1's corr_score: 0.16208\n",
      "[400]\ttraining's huber: 0.287642\ttraining's mse: 0.81733\ttraining's corr_score: 0.22751\tvalid_1's huber: 0.291635\tvalid_1's mse: 0.83058\tvalid_1's corr_score: 0.16529\n",
      "[500]\ttraining's huber: 0.285941\ttraining's mse: 0.81202\ttraining's corr_score: 0.24378\tvalid_1's huber: 0.291304\tvalid_1's mse: 0.82963\tvalid_1's corr_score: 0.16855\n",
      "[600]\ttraining's huber: 0.284378\ttraining's mse: 0.80701\ttraining's corr_score: 0.2587\tvalid_1's huber: 0.29115\tvalid_1's mse: 0.8291\tvalid_1's corr_score: 0.17029\n",
      "[700]\ttraining's huber: 0.282834\ttraining's mse: 0.8022\ttraining's corr_score: 0.27266\tvalid_1's huber: 0.290923\tvalid_1's mse: 0.82853\tvalid_1's corr_score: 0.17217\n",
      "[800]\ttraining's huber: 0.281349\ttraining's mse: 0.79748\ttraining's corr_score: 0.28569\tvalid_1's huber: 0.290787\tvalid_1's mse: 0.82816\tvalid_1's corr_score: 0.17338\n",
      "[900]\ttraining's huber: 0.279897\ttraining's mse: 0.7929\ttraining's corr_score: 0.29813\tvalid_1's huber: 0.290677\tvalid_1's mse: 0.82777\tvalid_1's corr_score: 0.17466\n",
      "[1000]\ttraining's huber: 0.278462\ttraining's mse: 0.78842\ttraining's corr_score: 0.30979\tvalid_1's huber: 0.290484\tvalid_1's mse: 0.82718\tvalid_1's corr_score: 0.17658\n",
      "[1100]\ttraining's huber: 0.2771\ttraining's mse: 0.78407\ttraining's corr_score: 0.32093\tvalid_1's huber: 0.290403\tvalid_1's mse: 0.82689\tvalid_1's corr_score: 0.17754\n",
      "[1200]\ttraining's huber: 0.275778\ttraining's mse: 0.77989\ttraining's corr_score: 0.33131\tvalid_1's huber: 0.290352\tvalid_1's mse: 0.82678\tvalid_1's corr_score: 0.17794\n",
      "[1300]\ttraining's huber: 0.274433\ttraining's mse: 0.77573\ttraining's corr_score: 0.34128\tvalid_1's huber: 0.290284\tvalid_1's mse: 0.82658\tvalid_1's corr_score: 0.17859\n",
      "Early stopping, best iteration is:\n",
      "[1323]\ttraining's huber: 0.274133\ttraining's mse: 0.77476\ttraining's corr_score: 0.34361\tvalid_1's huber: 0.290266\tvalid_1's mse: 0.82649\tvalid_1's corr_score: 0.17886\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "cv: 2 | mse_train: 0.77476 | corr_train: 0.34361 | mse_valid: 0.82649 | corr_valid: 0.17886\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\ttraining's huber: 0.293258\ttraining's mse: 0.83527\ttraining's corr_score: 0.1638\tvalid_1's huber: 0.294032\tvalid_1's mse: 0.83746\tvalid_1's corr_score: 0.14678\n",
      "[200]\ttraining's huber: 0.291007\ttraining's mse: 0.82822\ttraining's corr_score: 0.1895\tvalid_1's huber: 0.293215\tvalid_1's mse: 0.83505\tvalid_1's corr_score: 0.15598\n",
      "[300]\ttraining's huber: 0.289118\ttraining's mse: 0.82222\ttraining's corr_score: 0.20997\tvalid_1's huber: 0.292753\tvalid_1's mse: 0.83366\tvalid_1's corr_score: 0.16099\n",
      "[400]\ttraining's huber: 0.287389\ttraining's mse: 0.81681\ttraining's corr_score: 0.22731\tvalid_1's huber: 0.292403\tvalid_1's mse: 0.83261\tvalid_1's corr_score: 0.16467\n",
      "[500]\ttraining's huber: 0.285737\ttraining's mse: 0.81161\ttraining's corr_score: 0.24344\tvalid_1's huber: 0.292118\tvalid_1's mse: 0.83177\tvalid_1's corr_score: 0.16755\n",
      "[600]\ttraining's huber: 0.284162\ttraining's mse: 0.8065\ttraining's corr_score: 0.25873\tvalid_1's huber: 0.29194\tvalid_1's mse: 0.83116\tvalid_1's corr_score: 0.1696\n",
      "[700]\ttraining's huber: 0.282648\ttraining's mse: 0.80169\ttraining's corr_score: 0.27266\tvalid_1's huber: 0.291819\tvalid_1's mse: 0.83082\tvalid_1's corr_score: 0.17073\n",
      "[800]\ttraining's huber: 0.281192\ttraining's mse: 0.79708\ttraining's corr_score: 0.28549\tvalid_1's huber: 0.291703\tvalid_1's mse: 0.83045\tvalid_1's corr_score: 0.17197\n",
      "[900]\ttraining's huber: 0.279764\ttraining's mse: 0.79245\ttraining's corr_score: 0.29803\tvalid_1's huber: 0.291615\tvalid_1's mse: 0.83005\tvalid_1's corr_score: 0.17329\n",
      "Early stopping, best iteration is:\n",
      "[971]\ttraining's huber: 0.278736\ttraining's mse: 0.78926\ttraining's corr_score: 0.30651\tvalid_1's huber: 0.291515\tvalid_1's mse: 0.82975\tvalid_1's corr_score: 0.17426\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "cv: 3 | mse_train: 0.78926 | corr_train: 0.30651 | mse_valid: 0.82975 | corr_valid: 0.17426\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\ttraining's huber: 0.293152\ttraining's mse: 0.83511\ttraining's corr_score: 0.16334\tvalid_1's huber: 0.294355\tvalid_1's mse: 0.8382\tvalid_1's corr_score: 0.14809\n",
      "[200]\ttraining's huber: 0.290956\ttraining's mse: 0.82808\ttraining's corr_score: 0.18904\tvalid_1's huber: 0.293593\tvalid_1's mse: 0.83584\tvalid_1's corr_score: 0.15704\n",
      "[300]\ttraining's huber: 0.289059\ttraining's mse: 0.82205\ttraining's corr_score: 0.20967\tvalid_1's huber: 0.293128\tvalid_1's mse: 0.8344\tvalid_1's corr_score: 0.16216\n",
      "[400]\ttraining's huber: 0.287325\ttraining's mse: 0.81653\ttraining's corr_score: 0.22754\tvalid_1's huber: 0.292742\tvalid_1's mse: 0.83319\tvalid_1's corr_score: 0.16633\n",
      "[500]\ttraining's huber: 0.285707\ttraining's mse: 0.81149\ttraining's corr_score: 0.24308\tvalid_1's huber: 0.292486\tvalid_1's mse: 0.83244\tvalid_1's corr_score: 0.16886\n",
      "[600]\ttraining's huber: 0.284138\ttraining's mse: 0.80649\ttraining's corr_score: 0.25805\tvalid_1's huber: 0.292244\tvalid_1's mse: 0.83167\tvalid_1's corr_score: 0.17142\n",
      "Early stopping, best iteration is:\n",
      "[634]\ttraining's huber: 0.283627\ttraining's mse: 0.80485\ttraining's corr_score: 0.26283\tvalid_1's huber: 0.292199\tvalid_1's mse: 0.83154\tvalid_1's corr_score: 0.17186\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "cv: 4 | mse_train: 0.80485 | corr_train: 0.26283 | mse_valid: 0.83154 | corr_valid: 0.17186\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\ttraining's huber: 0.293034\ttraining's mse: 0.83472\ttraining's corr_score: 0.16376\tvalid_1's huber: 0.294635\tvalid_1's mse: 0.83906\tvalid_1's corr_score: 0.14911\n",
      "[200]\ttraining's huber: 0.290769\ttraining's mse: 0.82766\ttraining's corr_score: 0.18948\tvalid_1's huber: 0.293771\tvalid_1's mse: 0.83644\tvalid_1's corr_score: 0.15893\n",
      "[300]\ttraining's huber: 0.288913\ttraining's mse: 0.82186\ttraining's corr_score: 0.20931\tvalid_1's huber: 0.293344\tvalid_1's mse: 0.83524\tvalid_1's corr_score: 0.16315\n",
      "[400]\ttraining's huber: 0.287248\ttraining's mse: 0.81657\ttraining's corr_score: 0.22661\tvalid_1's huber: 0.293096\tvalid_1's mse: 0.83454\tvalid_1's corr_score: 0.16549\n",
      "[500]\ttraining's huber: 0.285561\ttraining's mse: 0.81126\ttraining's corr_score: 0.24315\tvalid_1's huber: 0.292816\tvalid_1's mse: 0.83369\tvalid_1's corr_score: 0.16834\n",
      "[600]\ttraining's huber: 0.283951\ttraining's mse: 0.80608\ttraining's corr_score: 0.25879\tvalid_1's huber: 0.292623\tvalid_1's mse: 0.83306\tvalid_1's corr_score: 0.17048\n",
      "[700]\ttraining's huber: 0.282424\ttraining's mse: 0.80127\ttraining's corr_score: 0.27257\tvalid_1's huber: 0.292447\tvalid_1's mse: 0.83256\tvalid_1's corr_score: 0.17208\n",
      "[800]\ttraining's huber: 0.28095\ttraining's mse: 0.79669\ttraining's corr_score: 0.28548\tvalid_1's huber: 0.292317\tvalid_1's mse: 0.83216\tvalid_1's corr_score: 0.17335\n",
      "[900]\ttraining's huber: 0.279462\ttraining's mse: 0.79198\ttraining's corr_score: 0.29806\tvalid_1's huber: 0.292169\tvalid_1's mse: 0.83178\tvalid_1's corr_score: 0.17463\n",
      "[1000]\ttraining's huber: 0.278062\ttraining's mse: 0.78757\ttraining's corr_score: 0.30974\tvalid_1's huber: 0.292108\tvalid_1's mse: 0.83158\tvalid_1's corr_score: 0.17528\n",
      "Early stopping, best iteration is:\n",
      "[1008]\ttraining's huber: 0.277953\ttraining's mse: 0.78722\ttraining's corr_score: 0.31069\tvalid_1's huber: 0.2921\tvalid_1's mse: 0.83153\tvalid_1's corr_score: 0.17541\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "cv: 5 | mse_train: 0.78722 | corr_train: 0.31069 | mse_valid: 0.83153 | corr_valid: 0.17541\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      " mean | mse_train: 0.79409 | corr_train: 0.29072 | mse_valid: 0.83176 | corr_valid: 0.17281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_metrics = np.inf\n",
    "best_models_list = []\n",
    "best_params = {}\n",
    "\n",
    "for params_tuple in cv_params:\n",
    "\n",
    "    params = {param_names[i]: x for (i, x) in enumerate(params_tuple)}\n",
    "    print('#############################################################################################################################################')\n",
    "    print('Parmas: ', params, '\\n')\n",
    "\n",
    "    # models_list, metrics_dict = model_train(\n",
    "    #     model_type, feature_train, target_train, n_splits, train_test_ratio, params)\n",
    "    date_length = len(feature_train.index.get_level_values(\n",
    "        level='time_id').unique().tolist())\n",
    "\n",
    "    train_length = int(date_length/(1+n_splits*train_test_ratio))\n",
    "    valid_length = int(train_length * train_test_ratio)\n",
    "\n",
    "    # print(\n",
    "    #     f'train_date_length: {train_length}, test_date_length: {test_length}')\n",
    "    # cv = MultipleTimeSeriesCV(n_splits=n_splits,\n",
    "    #                           lookahead=1,\n",
    "    #                           test_period_length=valid_length,\n",
    "    #                           train_period_length=train_length,\n",
    "    #                           date_idx='time_id'\n",
    "    #                           )\n",
    "    cv = model_selection.KFold(\n",
    "        n_splits=n_splits, random_state=2021, shuffle=True)\n",
    "\n",
    "    models_list = []\n",
    "    metrics_dict = pd.DataFrame()\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(cv.split(X=feature_train)):\n",
    "        X_train, y_train, X_valid, y_valid = get_train_valid_data(\n",
    "            feature_train, target_train, train_idx, valid_idx)\n",
    "\n",
    "        lgbm_train = lgbm.Dataset(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        lgbm_valid = lgbm.Dataset(\n",
    "            X_valid, y_valid, reference=lgbm_train\n",
    "        )\n",
    "\n",
    "        model = lgbm.train(params=params,\n",
    "                           train_set=lgbm_train,\n",
    "                           valid_sets=[lgbm_train, lgbm_valid],\n",
    "                           #    num_boost_round=1000,\n",
    "                           feval=[feval_mse_score, feval_corr_score],\n",
    "                           verbose_eval=100,\n",
    "                           )\n",
    "\n",
    "        # y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "        y_train_fit = model.predict(\n",
    "            X_train, num_iteration=model.best_iteration)\n",
    "        y_valid_fit = model.predict(\n",
    "            X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "        r2_train = metrics.r2_score(y_train, y_train_fit)\n",
    "        r2_valid = metrics.r2_score(y_valid, y_valid_fit)\n",
    "        corr_train = corr(y_train, y_train_fit)\n",
    "        corr_valid = corr(y_valid, y_valid_fit)\n",
    "        mse_train = metrics.mean_squared_error(\n",
    "            y_train, y_train_fit)\n",
    "        mse_valid = metrics.mean_squared_error(y_valid, y_valid_fit)\n",
    "        print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        print(f'cv: {i+1:01} | mse_train: {mse_train:.5f} | corr_train: {corr_train:0.5f} | mse_valid: {mse_valid:.5f} | corr_valid: {corr_valid:0.5f}')\n",
    "        print('---------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        models_list.append(model)\n",
    "        metrics_dict.loc[i, 'corr_train'] = corr_train\n",
    "        metrics_dict.loc[i, 'corr_valid'] = corr_valid\n",
    "        metrics_dict.loc[i, 'mse_train'] = mse_train\n",
    "        metrics_dict.loc[i, 'mse_valid'] = mse_valid\n",
    "\n",
    "    metrics_dict = metrics_dict.mean(axis=0)\n",
    "    corr_train, corr_valid, mse_train, mse_valid = tuple(\n",
    "        metrics_dict)\n",
    "    print(\n",
    "        f' mean | mse_train: {mse_train:.5f} | corr_train: {corr_train:0.5f} | mse_valid: {mse_valid:.5f} | corr_valid: {corr_valid:0.5f}')\n",
    "    print()\n",
    "\n",
    "    if metrics_dict['mse_train'] < best_metrics:\n",
    "        best_metrics = metrics_dict['mse_train']\n",
    "        best_models_list = models_list.copy()\n",
    "        best_params = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test | mse_test: 0.80947 | corr_test: 12.71994%\n"
     ]
    }
   ],
   "source": [
    "y_test_fit_list = []\n",
    "for (i, model) in enumerate(best_models_list):\n",
    "    y_test_fit = model.predict(feature_test)\n",
    "    y_test_fit_list.append(y_test_fit)\n",
    "\n",
    "y_test_fit = pd.DataFrame(index=feature_test.index)\n",
    "y_test_fit['actual'] = target_test\n",
    "y_test_fit['predict'] = np.mean(y_test_fit_list, axis=0).squeeze()\n",
    "\n",
    "corr_train = corr(y_test_fit['actual'], y_test_fit['predict'])\n",
    "corr_test = corr(y_test_fit['actual'], y_test_fit['predict'])\n",
    "mse_train = metrics.mean_squared_error(y_test_fit['actual'], y_test_fit['predict'])\n",
    "mse_test = metrics.mean_squared_error(y_test_fit['actual'], y_test_fit['predict'])\n",
    "print(\n",
    "    f' test | mse_test: {mse_train:.5f} | corr_test: {corr_train:0.5%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "for (i, model) in enumerate(best_models_list):\n",
    "    ax = plt.subplot(1, 5, i+1)\n",
    "    lgbm.plot_importance(model, max_num_features=100, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(model_directory/'pickle_best_models_list', 'wb')\n",
    "pickle.dump(best_models_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(model_directory/'pickle_best_models_list', 'rb')\n",
    "models_list = pickle.load(f1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06e2c26d81da9559881f4d926ec79778e2b8c97ac068d329245981963e3d233b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
