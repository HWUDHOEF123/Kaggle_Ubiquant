{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_selection, metrics, model_selection, decomposition\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "import random\n",
    "from itertools import product\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = Path(r\"..\\\\..\\\\Data\\\\Input\")\n",
    "\n",
    "feature_directory = Path(r\"..\\\\..\\\\Data\\\\Feature\")\n",
    "\n",
    "model_name = \"model_nn(tf)_01\"\n",
    "model_directory = Path()/model_name\n",
    "model_directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dic, save_path):\n",
    "    with open(save_path, 'wb') as f:\n",
    "    # with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(dic, f)\n",
    "\n",
    "def load_pickle(load_path):\n",
    "    with open(load_path, 'rb') as f:\n",
    "    # with gzip.open(load_path, 'rb') as f:\n",
    "        message_dict = pickle.load(f)\n",
    "    return message_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = pd.read_parquet(input_directory/'train_low_mem.parquet', engine='pyarrow').set_index(['time_id','investment_id'])\n",
    "# df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = df_data.drop('row_id', axis=1)\n",
    "# df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data_norm = df_data.copy()\n",
    "\n",
    "# for i in tqdm(range(300)):\n",
    "#     feature = f'f_{i}'\n",
    "\n",
    "#     df_data_norm[feature] = df_data[feature].groupby(level='time_id').apply(\n",
    "#         lambda x: pd.DataFrame(preprocessing.RobustScaler(quantile_range=(1., 99.), with_scaling=True, with_centering=True).fit_transform(x.values.reshape(-1, 1)), index=x.index, columns=[f'f_{i}']))\n",
    "\n",
    "\n",
    "# df_data_norm.to_parquet(input_directory/'train_norm2.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_norm = pd.read_parquet(input_directory/'train_norm2.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'histgram of normalized feature'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAEICAYAAAAKmB3fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeDklEQVR4nO3de/RldV3/8efLQRFMkMtwmwEHdbLAlRdGwq4amqNYUD/IqYypyEnkl1mtXw7+WukqKVirAqmgMPxx0QQkRRJJESKzH4IDatxkMT9BGGeE4RLgBXD0/fvjfE6e+fK9nO+Z7/me7+X5WGuvs/dn789nf/aePd/zPp/PZ++dqkKSJGkQTxt1BSRJ0vxlICFJkgZmICFJkgZmICFJkgZmICFJkgZmICFJkgZmICH1IcndSV49wbqfTHLHbNdptiXZN8lnkjyW5C9HXZ/pSHJtkt9q87+a5FMzXP6KJJVkpwnWvzDJF9q5e9tM7lsatXEvekn9q6p/B1441XZJ3g28oKreNPRKDcc64AFgt5rHD6Cpqg8CH5zl3f4hcG1VvXRHC0pyLfCBqvqHHa6VNANskZDmiYl+7c6i5wK3DTuISMdC+9v0XODWUVcC5sR1pAVmof1nlYbpJUn+M8kjSS5O8kyAJK9Msqm7UZJ3JPlaa8a+I8mRSVYD7wTemOQbSb7Utj24p7vg00n+NskH2rpuc/kJSe4BrmnpH07y9VaPzyQ5tGff5yU5K8mVbT//kWS/JGckeTjJl5NM+Ks4yY8l+Xwr+/NJfqxbLrAW+MNW7lO6edq+/zbJFe14rk/y/KnKbuuuTXJKkv8AvgU8rx37W5Pc2cr70yTPT3JdkkeTXJLkGS3/Hkk+nmRrO86PJ1k+wTH+epLPtvnu8XSn77RjJcnuSc5NsqX9e74nyZK2bkmSv0jyQJKvAEdNck6vAV4F/E3bxw8m2bnlvyfJfUn+LskuUx1LklOAn+wp628yTrdKtu/K+fV2HZye5CHg3ZPtX5q2qnJycppiAu4GbgAOAPYEbgfe0ta9EtjU5l8I3Asc0JZXAM9v8++m0yTdW+51wF8AzwB+Ani0u03LW8AFwLOAXVr6bwLPBnYGzgC+2FPeeXS6Hw4Dnkkn+LgLOB5YArwH+NcJjnFP4GHg1+h0e/5yW96rp+z3THKOzgMeAg5v+T8IXNRn2dcC9wCHtvVPb8d+ObBbS38CuBp4HrA7cBuwtuXfC/gfwK7t3HwYuKynbtcCv9Xmfx347Dj1PxDYDLy+LV8G/H079/u0f//fbuveAny55dkT+NdW350mODf/vf+2fEY7tj1bff8Z+PPpHsuY62SnSY53G/A77dzuMtn+nZymO9kiIfXvzKraXFUP0fnD+5JxtvkunS/4Q5I8varurqr/N15hSQ4CXg78cVU9WVWfpfPHfax3V9U3q+rbAFX1/qp6rKqeoBOcvDjJ7j3bf7Sqbqyqx4GPAo9X1QVV9V3gYmCiFomjgDur6sKq2lZVH6LzZflzk52UMT5SVTdU1TY6gcRLplH2eVV1a1v/nZZ2WlU9WlW3ArcAn6qqr1TVI8CV3WOpqger6p+q6ltV9RhwCvDT/Va6/Rq/DHhvVX0iyb7A64C3t3N/P3A6sKZl+SXgjKq6t10Pfz6NfQV4M/B7VfVQq++fdcve0WOZwOaq+uv27/L4ZPuXpsu+Mql/X++Z/xad1ontVNXGJG+n8wV/aJJPAr9fVZvHKe8A4KGq+lZP2r10fuUyJg3oNKnT+WI5DlgKfK+t2ht4pM3f15P32+Ms/8A4denW56tj0r4KLJtg+/GMPUfdffVT9r081VTHsh9Akl3pfNGvBvZo65+dZEkLoKZyLnBHVZ3Wlp9Lp1VkS+d7H+h0BXfreMCY+o49tskspdPacGNP2aHTYjQTxzKe3rpOun9pumyRkGZYVf1jVf0EnS+jArpfTmMHKW4B9mxfHF1jg4ix+X4FOBp4NZ3m/RUtPey4zXTq3Osg4GuzVPaODOL8AzrdSj9aVbsBP9XSpzwvSda3vCf0JN9Lpytl76p6Tpt2q6rueJQtbP9vddA06voAnSDo0J6yd6+qbtA11bGMPU/fbJ+919F+Y7bpzTPV/qVpMZCQZlA6zwv4mSQ702lC/jad7g7o/JpekXZHQlV9FdhAZ/DbM5K8gqm7EZ5N5wvuQTpfHH82g9X/BPCDSX4lyU5J3ggcAnx8jpcNnfPybeC/kuwJvKufTEleB7wNOKbbdQRQVVuATwF/mWS3JE9rAz27XQyXAG9LsjzJHsD6fitaVd8D3gecnmSfVo9lSV7b57HcR2ecSLe8rXQCsje1QaC/CTyfCfSxf2laDCSkmbUzcCqdX31fpzNI751t3Yfb54NJbmrzvwq8gk5g8B46YxiemKT8C+g0o3+NzmDDz81UxavqQeANdH4RP0jn2QdvqKoH5nLZzRl0BhE+QOec/Euf+d5Ip6n/9p47N/6urTueziDY2+gMDL0U2L+tex/wSeBLwE3AR6ZZ33cAG4HPJXkU+DTffxbJVMfyXuDYdkfHmS3tzcD/onNuDwX+7w7sX5qWVM3b58pIC06Si4EvV1Vfv6gladRskZBGKMnLW5P509J51sTRdO4ekKR5wbs2pNHaj06z+F7AJuDEqvrCaKskSf2za0OSJA3Mrg1JkjQwuzaavffeu1asWDHqakiSNOfceOOND1TV0vHWGUg0K1asYMOGDaOuhiRJc06SCZ/eateGJEkamIGEJEkamIGEJEkamIGEJEkamIGEJEkamIGEJEka2NACiSTvT3J/klt60vZMclWSO9vnHj3rTk6yMckdva+zTXJYkpvbujOTpKXvnOTiln59khU9eda2fdyZZO2wjlGSpMVumC0S5wGrx6StB66uqpXA1W2ZJIcAa+i8/nY1cFaSJS3P2cA6YGWbumWeADxcVS8ATgdOa2XtCbwL+FHgcOBdvQGLJEmaOUMLJKrqM8BDY5KPBs5v8+cDx/SkX1RVT1TVXcBG4PAk+wO7VdV11XkpyAVj8nTLuhQ4srVWvBa4qqoeqqqHgat4akAjSZJmwGyPkdi3qrYAtM99Wvoy4N6e7Ta1tGVtfmz6dnmqahvwCJ03KE5UlqR5bMX6K1ix/opRV0PSGHNlsGXGSatJ0gfNs/1Ok3VJNiTZsHXr1r4qKkmSvm+2A4n7WncF7fP+lr4JOLBnu+XA5pa+fJz07fIk2QnYnU5XykRlPUVVnVNVq6pq1dKl476LRJIkTWK2A4nLge5dFGuBj/Wkr2l3YhxMZ1DlDa3747EkR7TxD8ePydMt61jgmjaO4pPAzybZow2y/NmWJkmSZtjQ3v6Z5EPAK4G9k2yicyfFqcAlSU4A7gGOA6iqW5NcAtwGbANOqqrvtqJOpHMHyC7AlW0COBe4MMlGOi0Ra1pZDyX5U+Dzbbs/qaqxgz4lSdIMSOdHvFatWlW+Rlyae8YOsLz71KNGVBNp8UpyY1WtGm/dXBlsKUmS5iEDCUmSNDADCUmSNDADCUnzig+mkuYWAwlJkjQwAwlJc4otDtL8YiAhSZIGNrQHUknSjpiqVaK73udKSKNli4QkSRqYgYQkSRqYgYQkSRqYgYQkSRqYgYQkSRqYgYQkSRqYgYQkSRqYgYSkec0nYUqjZSAhSZIGZiAhSZIGZiAhSZIGZiAhSZIGZiAhSZIGZiAhSZIGZiAhSZIGttOoKyBJgM+CkOYpWyQkSdLADCQkLQg+4VIaDQMJSZI0MAMJSZI0MAMJSZI0MAMJSZI0MAMJSZI0MAMJSZI0MAMJSZI0sJEEEkl+L8mtSW5J8qEkz0yyZ5KrktzZPvfo2f7kJBuT3JHktT3phyW5ua07M0la+s5JLm7p1ydZMYLDlCRpwZv1QCLJMuBtwKqqehGwBFgDrAeurqqVwNVtmSSHtPWHAquBs5IsacWdDawDVrZpdUs/AXi4ql4AnA6cNguHJknSojOqro2dgF2S7ATsCmwGjgbOb+vPB45p80cDF1XVE1V1F7ARODzJ/sBuVXVdVRVwwZg83bIuBY7stlZIkqSZM+uBRFV9DfgL4B5gC/BIVX0K2LeqtrRttgD7tCzLgHt7itjU0pa1+bHp2+Wpqm3AI8BeY+uSZF2SDUk2bN26dWYOUNJI+ahsaXaNomtjDzotBgcDBwDPSvKmybKMk1aTpE+WZ/uEqnOqalVVrVq6dOnkFZckSU8xiq6NVwN3VdXWqvoO8BHgx4D7WncF7fP+tv0m4MCe/MvpdIVsavNj07fL07pPdgceGsrRSJK0iI0ikLgHOCLJrm3cwpHA7cDlwNq2zVrgY23+cmBNuxPjYDqDKm9o3R+PJTmilXP8mDzdso4FrmnjKCRJ0gzaabZ3WFXXJ7kUuAnYBnwBOAf4AeCSJCfQCTaOa9vfmuQS4La2/UlV9d1W3InAecAuwJVtAjgXuDDJRjotEWtm4dAkTZNjGaT5b9YDCYCqehfwrjHJT9BpnRhv+1OAU8ZJ3wC8aJz0x2mBiCRJGh6fbClJkgZmICFpQfI2UGl2GEhIkqSBGUhIkqSBGUhIkqSBGUhIkqSBGUhIkqSBGUhIkqSBGUhIkqSBGUhIWtB8noQ0XAYSkiRpYAYSkiRpYCN5aZekxc2uBmnhsEVCkiQNzEBCkiQNzEBCkiQNzEBCkiQNzEBCkiQNzLs2JM0a79aQFh4DCUmLQm8Qc/epR42wJtLCYteGJEkamIGEJEkamIGEJEkamIGEJEkamIMtJQ2dd2tIC5ctEpIkaWAGEpIWnRXrr7CVRJohfQUSSV407IpIkqT5p98Wib9LckOStyZ5zjArJEmS5o++Aomq+gngV4EDgQ1J/jHJa4ZaM0mSNOf1PUaiqu4E/gh4B/DTwJlJvpzkF4dVOUmSNLf1dftnkh8BfgM4CrgK+LmquinJAcB1wEeGV0VJ85UDGqWFr98Wib8BbgJeXFUnVdVNAFW1mU4rxbQkeU6SS1uLxu1JXpFkzyRXJbmzfe7Rs/3JSTYmuSPJa3vSD0tyc1t3ZpK09J2TXNzSr0+yYrp1lCRJU+s3kHg98I9V9W2AJE9LsitAVV04wH7fC/xLVf0Q8GLgdmA9cHVVrQSubsskOQRYAxwKrAbOSrKklXM2sA5Y2abVLf0E4OGqegFwOnDaAHWUJElT6DeQ+DSwS8/yri1t2pLsBvwUcC5AVT1ZVf8FHA2c3zY7HzimzR8NXFRVT1TVXcBG4PAk+wO7VdV1VVXABWPydMu6FDiy21ohSZJmTr+PyH5mVX2ju1BV3+i2SAzgecBW4P8keTFwI/C7wL5VtaWVvyXJPm37ZcDnevJvamnfafNj07t57m1lbUvyCLAX8EBvRZKso9OiwUEHHTTg4Ugay7ER0uLRb4vEN5O8rLuQ5DDg2wPucyfgZcDZVfVS4Ju0bowJjNeSUJOkT5Zn+4Sqc6pqVVWtWrp06eS1liRJT9Fvi8TbgQ8n2dyW9wfeOOA+NwGbqur6tnwpnUDiviT7t9aI/YH7e7Y/sCf/cmBzS18+Tnpvnk1JdgJ2Bx4asL6SFqhuy8ndpx414ppI81e/D6T6PPBDwInAW4EfrqobB9lhVX0duDfJC1vSkcBtwOXA2pa2FvhYm78cWNPuxDiYzqDKG1o3yGNJjmjjH44fk6db1rHANW0chSRJmkHTeY34y4EVLc9Lk1BVFwy4398BPpjkGcBX6Dyj4mnAJUlOAO4BjgOoqluTXEIn2NgGnFRV323lnAicR2cg6JVtgs5AzguTbKTTErFmwHpKkqRJ9PtAqguB5wNfBLpf4t07Jaatqr4IrBpn1ZETbH8KcMo46RuAp7xQrKoepwUikiRpePptkVgFHGL3gCRJ6tXvXRu3APsNsyKSJGn+6bdFYm/gtiQ3AE90E6vq54dSK0mSNC/0G0i8e5iVkKRR8jZQaXB9BRJV9W9JngusrKpPt6daLpkqnyRJWtj6GiOR5M10Hhz19y1pGXDZkOokSZLmiX67Nk4CDgeuB6iqO3vehSFJgO/YkBajfu/aeKKqnuwutMdOeyuoJEmLXL+BxL8leSewS5LXAB8G/nl41ZIkSfNBv4HEejqv/r4Z+G3gE8AfDatSkiRpfuj3ro3vAe9rkyRJEtD/uzbuYpwxEVX1vBmvkSRJmjem866NrmfSeSHWnjNfHUmSNJ/0NUaiqh7smb5WVWcAPzPcqknS7Fqx/gpvYZWmqd+ujZf1LD6NTgvFs4dSI0nzjl++0uLVb9fGX/bMbwPuBn5pxmsjSZLmlX7v2njVsCsiSZLmn367Nn5/svVV9VczUx1JkjSfTOeujZcDl7flnwM+A9w7jEpJkqT5od9AYm/gZVX1GECSdwMfrqrfGlbFJEnS3NdvIHEQ8GTP8pPAihmvjaR5xbs1JPUbSFwI3JDko3SecPkLwAVDq5UkjVA3QLr71KNGXBNp7uv3ro1TklwJ/GRL+o2q+sLwqiVJkuaDft/+CbAr8GhVvRfYlOTgIdVJkiTNE30FEkneBbwDOLklPR34wLAqJUmS5od+WyR+Afh54JsAVbUZH5EtSdKi128g8WRVFe1V4kmeNbwqSZKk+aLfuzYuSfL3wHOSvBn4TeB9w6uWpLnM2z4ldU0ZSCQJcDHwQ8CjwAuBP66qq4ZcN0mSNMdNGUhUVSW5rKoOAwweJEnSf+t3jMTnkrx8qDWRpDlmxfor7MaRptDvGIlXAW9JcjedOzdCp7HiR4ZVMUmSNPdN2iKR5KA2+zrgecDP0Hnz5xva58CSLEnyhSQfb8t7JrkqyZ3tc4+ebU9OsjHJHUle25N+WJKb27oz23gOkuyc5OKWfn2SFTtSV0mSNL6pujYuA6iqrwJ/VVVf7Z12cN+/C9zes7weuLqqVgJXt2WSHAKsAQ4FVgNnJVnS8pwNrANWtml1Sz8BeLiqXgCcDpy2g3WVhE39kp5qqkAiPfPPm6mdJlkOHAX8Q0/y0cD5bf584Jie9Iuq6omqugvYCByeZH9gt6q6rj3j4oIxebplXQoc2W2tkCRJM2eqQKImmN9RZwB/CHyvJ23fqtoC0D73aenLgHt7ttvU0pa1+bHp2+Wpqm3AI8BeYyuRZF2SDUk2bN26dQcPSZKkxWeqQOLFSR5N8hjwI23+0SSPJXl0kB0meQNwf1Xd2G+WcdJqkvTJ8myfUHVOVa2qqlVLly7tszqSFhu7dKSJTXrXRlUtmWz9gH4c+PkkrweeCeyW5APAfUn2r6otrdvi/rb9JuDAnvzLgc0tffk46b15NiXZCdgdeGgIxyJJ0qI2ndeIz4iqOrmqllfVCjqDKK+pqjcBlwNr22ZrgY+1+cuBNe1OjIPpDKq8oXV/PJbkiDb+4fgxebplHdv2MZNdM9Ki4i9ySRPp9zkSs+FUOu/0OAG4BzgOoKpuTXIJcBuwDTipqr7b8pwInAfsAlzZJoBzgQuTbKTTErFmtg5CkqTFZKSBRFVdC1zb5h8Ejpxgu1OAU8ZJ3wC8aJz0x2mBiCRJGp5Z79qQJEkLh4GEJEka2FwaIyFpjnGA5fa65+PuU48acU2kucMWCUmSNDBbJCQ9hS0RkvplICEJMHiQNBi7NiRJ0sAMJCRJ0sAMJCRpmnxkuPR9jpGQFjm/ECXtCFskJEnSwAwkJEnSwAwkJEnSwAwkJEnSwBxsKS1SDrKUNBNskZCkAXkbqGQgIUmSdoCBhCRJGphjJKRFxqZ4STPJFglJkjQwAwlJ2kEOutRiZteGtEj4RSdpGAwkpAXOAELSMNm1IUkzxC4OLUYGEpIkaWAGEpIkaWCOkZCkGdbbvXH3qUeNsCbS8BlISAuUffWSZoNdG5IkaWAGEpIkaWAGEpI0RN4SqoXOMRLSAuOXlqTZNOstEkkOTPKvSW5PcmuS323peya5Ksmd7XOPnjwnJ9mY5I4kr+1JPyzJzW3dmUnS0ndOcnFLvz7Jitk+TkmSFoNRdG1sA/6gqn4YOAI4KckhwHrg6qpaCVzdlmnr1gCHAquBs5IsaWWdDawDVrZpdUs/AXi4ql4AnA6cNhsHJo2STehzm/8+WqhmPZCoqi1VdVObfwy4HVgGHA2c3zY7HzimzR8NXFRVT1TVXcBG4PAk+wO7VdV1VVXABWPydMu6FDiy21ohSZJmzkjHSLQuh5cC1wP7VtUW6AQbSfZpmy0DPteTbVNL+06bH5vezXNvK2tbkkeAvYAHhnMk0uj4K1fSKI3sro0kPwD8E/D2qnp0sk3HSatJ0ifLM7YO65JsSLJh69atU1VZknaYXRxaaEYSSCR5Op0g4oNV9ZGWfF/rrqB93t/SNwEH9mRfDmxu6cvHSd8uT5KdgN2Bh8bWo6rOqapVVbVq6dKlM3FokiQtKqO4ayPAucDtVfVXPasuB9a2+bXAx3rS17Q7MQ6mM6jyhtYN8liSI1qZx4/J0y3rWOCaNo5CkiTNoFGMkfhx4NeAm5N8saW9EzgVuCTJCcA9wHEAVXVrkkuA2+jc8XFSVX235TsROA/YBbiyTdAJVC5MspFOS8SaIR+TNGtsFl8Yuv+OvtRL892sBxJV9VnGH8MAcOQEeU4BThknfQPwonHSH6cFIpIkaXh8RLYkSRqYgYQkSRqYgYQkjZC3g2q+M5CQJEkDM5CQpDnAlgnNV75GXJon/JKRNBfZIiFJkgZmICFJc4hdHJpvDCQkSdLAHCMhzWH+MpU019kiIUlzkF0cmi8MJCRpDjOg0Fxn14Y0B/nFIWm+sEVCkuYBWyY0V9kiIc0hflFImm8MJKQ5wABC0nxlICGNkAGEpqt7zdx96lEjronU4RgJSZqHHDOhucIWCWkE/AKQtFAYSEizyABCM633mrK7Q6NgICHNAgMIzQbHT2gUHCMhSZIGZouENAS2QGiUbJnQbLJFQpIWKO/s0GywRUKaQf7R1lxkC4WGyRYJSVokbKHQMNgiIUmLzNhgwpYK7QgDCWkH+OtOC4GBhXaEgYQ0TQYPWugcU6HpMJCQ+mQAocXGgEL9MJCQJmDgIHXY9aHJGEhIkqbF93uol4GENIYtEVL/bK3Qgg4kkqwG3gssAf6hqk4dcZUkaUGbKBA3wFi4FmwgkWQJ8LfAa4BNwOeTXF5Vt422Zho1Wxyk2TfV/zsDjflrwQYSwOHAxqr6CkCSi4CjAQOJBcSgQFoYZvL/skHJ7FrIgcQy4N6e5U3Aj/ZukGQdsK4tfiPJHT2r9wYeGGoNFxbP1/R4vqbPczY9i/Z85bSBsy7ac9aH5060YiEHEhknrbZbqDoHOGfczMmGqlo1jIotRJ6v6fF8TZ/nbHo8X9PnORvMQn5p1ybgwJ7l5cDmEdVFkqQFaSEHEp8HViY5OMkzgDXA5SOukyRJC8qC7dqoqm1J/ifwSTq3f76/qm6dRhHjdnloQp6v6fF8TZ/nbHo8X9PnORtAqmrqrSRJksaxkLs2JEnSkBlISJKkgRlIAEmOS3Jrku8lmfDWnyR3J7k5yReTbJjNOs410zhnq5PckWRjkvWzWce5JMmeSa5Kcmf73GOC7Rb1NTbV9ZKOM9v6/0zyslHUcy7p45y9Mskj7Zr6YpI/HkU954ok709yf5JbJljvNTZNBhIdtwC/CHymj21fVVUv8V7jqc9Zz2PKXwccAvxykkNmp3pzznrg6qpaCVzdlieyKK+xPq+X1wEr27QOOHtWKznHTOP/2L+3a+olVfUns1rJuec8YPUk673GpslAAqiq26vqjqm3VFef5+y/H1NeVU8C3ceUL0ZHA+e3+fOBY0ZXlTmrn+vlaOCC6vgc8Jwk+892RecQ/49NU1V9Bnhokk28xqbJQGJ6CvhUkhvb47U1ufEeU75sRHUZtX2ragtA+9xngu0W8zXWz/XiNbW9fs/HK5J8KcmVSQ6dnarNW15j07RgnyMxVpJPA/uNs+p/V9XH+izmx6tqc5J9gKuSfLlFtwvSDJyzKR9TvpBMdr6mUcyiusbG6Od6WVTXVB/6OR83Ac+tqm8keT1wGZ1me43Pa2yaFk0gUVWvnoEyNrfP+5N8lE6z4oL9Iz8D52xRPaZ8svOV5L4k+1fVltZMev8EZSyqa2yMfq6XRXVN9WHK81FVj/bMfyLJWUn2ripfTjU+r7FpsmujT0meleTZ3XngZ+kMONTEfEz5910OrG3za4GntOh4jfV1vVwOHN9G1h8BPNLtMlqkpjxnSfZLkjZ/OJ2/+w/Oek3nD6+xaTKQAJL8QpJNwCuAK5J8sqUfkOQTbbN9gc8m+RJwA3BFVf3LaGo8ev2cs6raBnQfU347cMk0H1O+kJwKvCbJncBr2rLXWI+Jrpckb0nylrbZJ4CvABuB9wFvHUll54g+z9mxwC3tujoTWFOL+JHGST4EXAe8MMmmJCd4je0YH5EtSZIGZouEJEkamIGEJEkamIGEJEkamIGEJEkamIGEJEkamIGEJEkamIGEJEka2P8HpgFzKsK+I5QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1296x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(18, 4))\n",
    "# ax = plt.subplot(121)\n",
    "# df_data['f_1'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of raw feature\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "df_data_norm['f_2'].plot(kind='hist', bins=200, ax=ax, title=\"histgram of normalized feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Feature & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_id  investment_id\n",
       "0        1               -0.300875\n",
       "         2               -0.231040\n",
       "         6                0.568807\n",
       "         7               -1.064780\n",
       "         8               -0.531940\n",
       "                            ...   \n",
       "1219     3768             0.033600\n",
       "         3769            -0.223264\n",
       "         3770            -0.559415\n",
       "         3772             0.009599\n",
       "         3773             1.212112\n",
       "Name: target, Length: 3141410, dtype: float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target = df_data_norm.pop('target')\n",
    "feature = df_data_norm\n",
    "# target = target.reset_index(drop=True)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.133483</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>-0.107040</td>\n",
       "      <td>0.232203</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>-0.084325</td>\n",
       "      <td>0.295687</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.565489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.097949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049540</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.239130</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>0.119334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108289</td>\n",
       "      <td>-0.102564</td>\n",
       "      <td>0.160293</td>\n",
       "      <td>-0.011403</td>\n",
       "      <td>0.029314</td>\n",
       "      <td>0.390865</td>\n",
       "      <td>0.426703</td>\n",
       "      <td>0.295676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.076149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.097370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.209301</td>\n",
       "      <td>-0.285416</td>\n",
       "      <td>-0.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.119521</td>\n",
       "      <td>-0.009270</td>\n",
       "      <td>0.091237</td>\n",
       "      <td>-0.229989</td>\n",
       "      <td>0.299662</td>\n",
       "      <td>-0.347619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.298825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.059196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.127263</td>\n",
       "      <td>-0.063854</td>\n",
       "      <td>0.104602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.544325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424744</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.063067</td>\n",
       "      <td>-0.171823</td>\n",
       "      <td>0.242565</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.337304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.080431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.171502</td>\n",
       "      <td>0.262178</td>\n",
       "      <td>-0.087566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.114755</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>0.531115</td>\n",
       "      <td>-0.003263</td>\n",
       "      <td>-0.070495</td>\n",
       "      <td>-0.155961</td>\n",
       "      <td>-0.246051</td>\n",
       "      <td>0.323107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.098786</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.075289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226062</td>\n",
       "      <td>0.312121</td>\n",
       "      <td>-0.080823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1219</th>\n",
       "      <th>3768</th>\n",
       "      <td>-0.047359</td>\n",
       "      <td>-0.148677</td>\n",
       "      <td>-0.083344</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.017038</td>\n",
       "      <td>-0.092924</td>\n",
       "      <td>0.378928</td>\n",
       "      <td>0.318737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.387626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080894</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.105754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063816</td>\n",
       "      <td>-0.096651</td>\n",
       "      <td>-0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>-0.374504</td>\n",
       "      <td>-0.029735</td>\n",
       "      <td>-0.035861</td>\n",
       "      <td>-0.006363</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.029534</td>\n",
       "      <td>-0.402315</td>\n",
       "      <td>0.339488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.329040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.107742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.107359</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.039689</td>\n",
       "      <td>-0.294157</td>\n",
       "      <td>-0.070620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>0.154132</td>\n",
       "      <td>-0.237883</td>\n",
       "      <td>0.186720</td>\n",
       "      <td>-0.009407</td>\n",
       "      <td>-0.012827</td>\n",
       "      <td>0.291013</td>\n",
       "      <td>0.030878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.191191</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015572</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.022208</td>\n",
       "      <td>-0.489768</td>\n",
       "      <td>-0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>-0.652056</td>\n",
       "      <td>0.089206</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.435851</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>-0.146850</td>\n",
       "      <td>0.091365</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.302704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.053337</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018566</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.147317</td>\n",
       "      <td>0.130867</td>\n",
       "      <td>-0.020435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>-0.088998</td>\n",
       "      <td>0.059471</td>\n",
       "      <td>-0.123831</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>0.122948</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>0.257889</td>\n",
       "      <td>-0.286822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.524376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032652</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.069543</td>\n",
       "      <td>0.148146</td>\n",
       "      <td>0.170569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141410 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            f_0       f_1       f_2       f_3       f_4  \\\n",
       "time_id investment_id                                                     \n",
       "0       1              0.133483  0.025641 -0.107040  0.232203  0.027035   \n",
       "        2              0.108289 -0.102564  0.160293 -0.011403  0.029314   \n",
       "        6              0.022050  0.128205  0.119521 -0.009270  0.091237   \n",
       "        7             -0.544325  0.000000  0.424744 -0.008875 -0.063067   \n",
       "        8              0.114755 -0.051282  0.531115 -0.003263 -0.070495   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "1219    3768          -0.047359 -0.148677 -0.083344 -0.002532 -0.017038   \n",
       "        3769          -0.374504 -0.029735 -0.035861 -0.006363 -0.048209   \n",
       "        3770           0.154132 -0.237883  0.186720 -0.009407 -0.012827   \n",
       "        3772          -0.652056  0.089206  0.000941  0.435851 -0.010454   \n",
       "        3773          -0.088998  0.059471 -0.123831  0.139659  0.122948   \n",
       "\n",
       "                            f_5       f_6       f_7  f_8       f_9  ...  \\\n",
       "time_id investment_id                                               ...   \n",
       "0       1             -0.084325  0.295687  0.323107  0.0 -0.565489  ...   \n",
       "        2              0.390865  0.426703  0.295676  0.0 -0.076149  ...   \n",
       "        6             -0.229989  0.299662 -0.347619  0.0 -0.298825  ...   \n",
       "        7             -0.171823  0.242565  0.057936  0.0 -0.337304  ...   \n",
       "        8             -0.155961 -0.246051  0.323107  0.0 -0.017943  ...   \n",
       "...                         ...       ...       ...  ...       ...  ...   \n",
       "1219    3768          -0.092924  0.378928  0.318737  0.0 -0.387626  ...   \n",
       "        3769          -0.029534 -0.402315  0.339488  0.0 -0.329040  ...   \n",
       "        3770           0.291013  0.030878  0.000000  0.0  0.253357  ...   \n",
       "        3772          -0.146850  0.091365 -0.286822  0.0 -0.302704  ...   \n",
       "        3773           0.096628  0.257889 -0.286822  0.0 -0.524376  ...   \n",
       "\n",
       "                          f_290  f_291     f_292  f_293  f_294     f_295  \\\n",
       "time_id investment_id                                                      \n",
       "0       1              0.102298   -1.0  0.097949    0.0    0.0  0.049540   \n",
       "        2             -0.003282    0.0 -0.097370    0.0    0.0 -0.025318   \n",
       "        6              0.000000    0.0 -0.059196   -1.0   -1.0  0.016613   \n",
       "        7              0.105580    0.0  0.000476   -1.0    0.0 -0.080431   \n",
       "        8             -0.006564    0.0 -0.098786   -1.0    0.0 -0.075289   \n",
       "...                         ...    ...       ...    ...    ...       ...   \n",
       "1219    3768          -0.080894   -1.0 -0.105754    0.0    0.0 -0.031788   \n",
       "        3769           0.008933   -1.0 -0.107742    0.0    0.0 -0.107359   \n",
       "        3770          -0.170720   -1.0  0.191191   -1.0    0.0 -0.015572   \n",
       "        3772          -0.170720   -1.0  0.053337   -1.0    0.0 -0.018566   \n",
       "        3773          -0.086849    0.0  0.682466    0.0    0.0  0.032652   \n",
       "\n",
       "                       f_296     f_297     f_298     f_299  \n",
       "time_id investment_id                                       \n",
       "0       1               -0.5 -0.239130 -0.108588  0.119334  \n",
       "        2               -0.5 -0.209301 -0.285416 -0.006368  \n",
       "        6               -0.5 -0.127263 -0.063854  0.104602  \n",
       "        7                0.0 -0.171502  0.262178 -0.087566  \n",
       "        8                0.0  0.226062  0.312121 -0.080823  \n",
       "...                      ...       ...       ...       ...  \n",
       "1219    3768             0.0 -0.063816 -0.096651 -0.068704  \n",
       "        3769            -0.5  0.039689 -0.294157 -0.070620  \n",
       "        3770             0.5  0.022208 -0.489768 -0.003278  \n",
       "        3772            -0.5 -0.147317  0.130867 -0.020435  \n",
       "        3773             0.5 -0.069543  0.148146  0.170569  \n",
       "\n",
       "[3141410 rows x 300 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature = feature.reset_index(drop=True)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_corr = {}\n",
    "for col in feature:\n",
    "    col_corr[col] = np.abs(np.corrcoef(feature[col].values, target.values)[0][1])\n",
    "\n",
    "\n",
    "num_selected_col = 128\n",
    "col_corr = pd.DataFrame(col_corr,index=['corr']).T.sort_values('corr',ascending=False).iloc[:num_selected_col,:]\n",
    "\n",
    "features_A = col_corr.index.to_list()\n",
    "features_A.sort()\n",
    "# col_corr.to_csv(checkpoint_path/'col_corr.csv')\n",
    "# col_corr.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = selected_features.sort()\n",
    "features_B = list(set(feature.columns) - set(features_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 172)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_A), len(features_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 0, train_end: 1018, test_start: 1019, test_end: 1219\n"
     ]
    }
   ],
   "source": [
    "train_test_ratio = 5\n",
    "\n",
    "uniquedate = target.index.get_level_values(level='time_id').unique().tolist()\n",
    "train_start = 0\n",
    "train_end = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))]\n",
    "test_start = uniquedate[int(\n",
    "    len(uniquedate)*train_test_ratio/(train_test_ratio+1))+1]\n",
    "test_end = 1219\n",
    "\n",
    "\n",
    "# dates of train, dates of test\n",
    "print(\n",
    "    f'train_start: {train_start}, train_end: {train_end}, test_start: {test_start}, test_end: {test_end}')\n",
    "\n",
    "feature_train = feature.loc[idx[train_start:train_end, :], :]\n",
    "feature_test = feature.loc[idx[test_start:test_end, :], :]\n",
    "\n",
    "target_train = target.loc[idx[train_start:train_end, :]]\n",
    "target_test = target.loc[idx[test_start:test_end, :]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2490893, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_valid_data(X, y, train_idx, test_idx):\n",
    "    # x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    # x_val, y_val = X.iloc[test_idx, :], y.iloc[test_idx]\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # x_train = scaler.fit_transform(x_train)\n",
    "    # x_val = scaler.transform(x_val)\n",
    "    return (X.iloc[train_idx, :], y.iloc[train_idx], \n",
    "            X.iloc[test_idx, :], y.iloc[test_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_corr(y_actual, y_pred):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    \n",
    "    y_actual = (y_actual - tf.reduce_mean(y_actual))/tf.sqrt(tf.reduce_sum(tf.square(y_actual - tf.reduce_mean(y_actual))))\n",
    "    y_pred = (y_pred - tf.reduce_mean(y_pred))/tf.sqrt(tf.reduce_sum(tf.square(y_pred - tf.reduce_mean(y_pred))))\n",
    "\n",
    "    return tf.reduce_sum(y_actual*y_pred)\n",
    "\n",
    "def corr(y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_B (InputLayer)           [(None, 172)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_90 (Dense)               (None, 128)          22144       ['input_B[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 128)          0           ['dense_90[0][0]']               \n",
      "                                                                                                  \n",
      " dense_91 (Dense)               (None, 64)           8256        ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dense_92 (Dense)               (None, 16)           1040        ['dense_91[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 16)           0           ['dense_92[0][0]']               \n",
      "                                                                                                  \n",
      " dense_94 (Dense)               (None, 128)          2176        ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 144)          0           ['dense_94[0][0]',               \n",
      "                                                                  'dense_92[0][0]']               \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 256)          37120       ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 256)          0           ['dense_95[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 256)         1024        ['dropout_17[0][0]']             \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 128)          32896       ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 128)          0           ['dense_96[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dropout_18[0][0]']             \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_97 (Dense)               (None, 64)           8256        ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_98 (Dense)               (None, 32)           2080        ['dense_97[0][0]']               \n",
      "                                                                                                  \n",
      " input_A (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_99 (Dense)               (None, 1)            33          ['dense_98[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 115,537\n",
      "Trainable params: 114,769\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input_A = layers.Input(shape=(num_selected_col,), name='input_A')\n",
    "    input_B = layers.Input(\n",
    "        shape=(300-num_selected_col,), name='input_B')\n",
    "\n",
    "    input_B_ = layers.Dense(\n",
    "        128, activation='selu', kernel_initializer='lecun_normal')(input_B)\n",
    "    input_B_ = keras.layers.Dropout(0.2)(input_B_)\n",
    "    # input_B = keras.layers.BatchNormalization()(input_B)\n",
    "    input_B_ = layers.Dense(\n",
    "        64, activation='selu', kernel_initializer='lecun_normal')(input_B_)\n",
    "    # hidden = keras.layers.Dropout(0.1)(hidden)\n",
    "    # input_B = keras.layers.BatchNormalization()(input_B)\n",
    "    input_B_ = layers.Dense(\n",
    "        16, activation='selu', kernel_initializer='lecun_normal')(input_B_)\n",
    "    # hidden = keras.layers.Dropout(0.1)(hidden)\n",
    "    # input_B = keras.layers.BatchNormalization()(input_B)\n",
    "\n",
    "\n",
    "    input_A_ = layers.Dense(\n",
    "        128, activation='selu', kernel_initializer='lecun_normal')(input_A)\n",
    "    input_A_ = keras.layers.Dropout(0.2)(input_B_)\n",
    "    input_A_ = layers.Dense(\n",
    "        128, activation='selu', kernel_initializer='lecun_normal')(input_A_)\n",
    "\n",
    "    # hidden = keras.layers.Dropout(0.1)(hidden)\n",
    "    # input_B = keras.layers.BatchNormalization()(input_B)\n",
    "\n",
    "\n",
    "    concat = layers.concatenate([input_A_, input_B_])\n",
    "    concat = layers.Dense(\n",
    "        256, activation='selu', kernel_initializer='he_normal')(concat)\n",
    "    concat = keras.layers.Dropout(0.1)(concat)\n",
    "    concat = keras.layers.BatchNormalization()(concat)\n",
    "\n",
    "    concat = layers.Dense(\n",
    "        128, activation='selu', kernel_initializer='he_normal')(concat)\n",
    "    concat = keras.layers.Dropout(0.1)(concat)\n",
    "    concat = keras.layers.BatchNormalization()(concat)\n",
    "\n",
    "    concat = layers.Dense(\n",
    "        64, activation='selu', kernel_initializer='he_normal')(concat)\n",
    "    concat = layers.Dense(\n",
    "        32, activation='selu', kernel_initializer='he_normal')(concat)\n",
    "    # concat = keras.layers.Dropout(0.1)(concat)\n",
    "    # concat = keras.layers.BatchNormalization()(concat)\n",
    "\n",
    "    output = layers.Dense(1)(concat)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.005),\n",
    "                  loss='huber', metrics=['mse', custom_corr])\n",
    "    return model\n",
    "\n",
    "\n",
    "get_model().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 12\n",
    "\n",
    "n_splits = 2\n",
    "\n",
    "cv = model_selection.KFold(\n",
    "    n_splits=n_splits, random_state=2021, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, memory: 49.40%\n",
      "Epoch 1/12\n",
      "4866/4866 [==============================] - 27s 5ms/step - loss: 0.3178 - mse: 0.8660 - custom_corr: 0.0740 - val_loss: 0.3133 - val_mse: 0.8660 - val_custom_corr: 0.0970\n",
      "Epoch 2/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3125 - mse: 0.8479 - custom_corr: 0.1034 - val_loss: 0.3132 - val_mse: 0.8511 - val_custom_corr: 0.1083\n",
      "Epoch 3/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3114 - mse: 0.8450 - custom_corr: 0.1178 - val_loss: 0.3137 - val_mse: 0.8548 - val_custom_corr: 0.1135\n",
      "Epoch 4/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3108 - mse: 0.8433 - custom_corr: 0.1269 - val_loss: 0.3877 - val_mse: 756.0128 - val_custom_corr: 0.1127\n",
      "Epoch 5/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3102 - mse: 0.8416 - custom_corr: 0.1344 - val_loss: 0.3135 - val_mse: 0.9700 - val_custom_corr: 0.1193\n",
      "Epoch 6/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3097 - mse: 0.8404 - custom_corr: 0.1406 - val_loss: 0.3120 - val_mse: 0.8596 - val_custom_corr: 0.1220\n",
      "Epoch 7/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3091 - mse: 0.8387 - custom_corr: 0.1467 - val_loss: 0.3132 - val_mse: 1.2669 - val_custom_corr: 0.1233\n",
      "Epoch 8/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3088 - mse: 0.8379 - custom_corr: 0.1510 - val_loss: 0.3132 - val_mse: 0.8572 - val_custom_corr: 0.1226\n",
      "Epoch 9/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3083 - mse: 0.8364 - custom_corr: 0.1567 - val_loss: 0.3134 - val_mse: 0.8699 - val_custom_corr: 0.1299\n",
      "Epoch 10/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3079 - mse: 0.8351 - custom_corr: 0.1612 - val_loss: 0.3106 - val_mse: 1.0636 - val_custom_corr: 0.1464\n",
      "Epoch 11/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3075 - mse: 0.8341 - custom_corr: 0.1649 - val_loss: 0.3131 - val_mse: 0.8546 - val_custom_corr: 0.1277\n",
      "Epoch 12/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3071 - mse: 0.8331 - custom_corr: 0.1685 - val_loss: 0.3132 - val_mse: 0.8448 - val_custom_corr: 0.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [05:00, 300.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1, memory: 52.00%\n",
      "Epoch 1/12\n",
      "4866/4866 [==============================] - 26s 5ms/step - loss: 0.3190 - mse: 0.8691 - custom_corr: 0.0696 - val_loss: 0.3127 - val_mse: 0.8517 - val_custom_corr: 0.1019\n",
      "Epoch 2/12\n",
      "4866/4866 [==============================] - 23s 5ms/step - loss: 0.3125 - mse: 0.8490 - custom_corr: 0.1058 - val_loss: 0.3134 - val_mse: 0.9536 - val_custom_corr: 0.1069\n",
      "Epoch 3/12\n",
      "4866/4866 [==============================] - 23s 5ms/step - loss: 0.3115 - mse: 0.8466 - custom_corr: 0.1186 - val_loss: 0.3134 - val_mse: 0.8463 - val_custom_corr: 0.1179\n",
      "Epoch 4/12\n",
      "4866/4866 [==============================] - 23s 5ms/step - loss: 0.3108 - mse: 0.8445 - custom_corr: 0.1286 - val_loss: 0.3110 - val_mse: 0.8420 - val_custom_corr: 0.1256\n",
      "Epoch 5/12\n",
      "4866/4866 [==============================] - 23s 5ms/step - loss: 0.3101 - mse: 0.8429 - custom_corr: 0.1360 - val_loss: 0.3115 - val_mse: 0.8490 - val_custom_corr: 0.1332\n",
      "Epoch 6/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3096 - mse: 0.8414 - custom_corr: 0.1426 - val_loss: 0.3113 - val_mse: 0.8403 - val_custom_corr: 0.1342\n",
      "Epoch 7/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3091 - mse: 0.8400 - custom_corr: 0.1483 - val_loss: 0.3113 - val_mse: 0.8443 - val_custom_corr: 0.1378\n",
      "Epoch 8/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3087 - mse: 0.8387 - custom_corr: 0.1535 - val_loss: 0.3137 - val_mse: 0.8465 - val_custom_corr: 0.1141\n",
      "Epoch 9/12\n",
      "4866/4866 [==============================] - 24s 5ms/step - loss: 0.3084 - mse: 0.8380 - custom_corr: 0.1565 - val_loss: 0.3125 - val_mse: 0.8454 - val_custom_corr: 0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [08:42, 261.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, valid_idx) in tqdm(enumerate(cv.split(X=feature_train))):\n",
    "    print(f'fold: {fold}, memory: {psutil.virtual_memory().percent/100:.2%}')\n",
    "\n",
    "    model = get_model()\n",
    "    \n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = get_train_valid_data(\n",
    "        feature_train, target_train, train_idx, valid_idx)\n",
    "\n",
    "    # id_train = X_train.index.get_level_values(\n",
    "    #     'investment_id').to_series().reset_index(drop=True)\n",
    "    # id_valid = X_valid.index.get_level_values(\n",
    "    #     'investment_id').to_series().reset_index(drop=True)\n",
    "\n",
    "    \n",
    "\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        model_directory/f\"model_{fold}.tf\", monitor=\"val_loss\", mode=\"min\", save_best_only=True, save_weights_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "    model.fit((X_train[features_A], X_train[features_B]), y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=((X_valid[features_A], X_valid[features_B]), y_valid),\n",
    "                        epochs=epochs,\n",
    "                        #   verbose=0,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[checkpoint_cb, early_stop_cb]\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_list = []\n",
    "\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    model = get_model()\n",
    "    model.load_weights(model_directory/f\"model_{fold}.tf\")\n",
    "    best_models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test | mse_test: 0.81305 | corr_test: 0.11232\n"
     ]
    }
   ],
   "source": [
    "y_test_fit_list = []\n",
    "for (i, model) in enumerate(best_models_list):\n",
    "    # id_test = feature_test.index.get_level_values(\n",
    "    #     'investment_id').to_series().reset_index(drop=True)\n",
    "    y_test_fit = model.predict((feature_test[features_A],feature_test[features_B]))\n",
    "    y_test_fit_list.append(y_test_fit)\n",
    "\n",
    "y_test_fit = pd.DataFrame(index=feature_test.index)\n",
    "y_test_fit['actual'] = target_test\n",
    "y_test_fit['predict'] = np.mean(y_test_fit_list, axis=0).squeeze()\n",
    "\n",
    "corr_train = corr(y_test_fit['actual'], y_test_fit['predict'])\n",
    "corr_test = corr(y_test_fit['actual'], y_test_fit['predict'])\n",
    "mse_train = metrics.mean_squared_error(\n",
    "    y_test_fit['actual'], y_test_fit['predict'])\n",
    "mse_test = metrics.mean_squared_error(\n",
    "    y_test_fit['actual'], y_test_fit['predict'])\n",
    "print(\n",
    "    f' test | mse_test: {mse_train:.5f} | corr_test: {corr_train:0.5}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(best_models_list, model_directory/'best_model_list')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06e2c26d81da9559881f4d926ec79778e2b8c97ac068d329245981963e3d233b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
